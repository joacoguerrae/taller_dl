{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a RNN: Clasificación de Logs de HDFS con PyTorch\n",
    "\n",
    "En esta clase, nos enfocaremos en el uso de Redes Neuronales Recurrentes (RNN) para la clasificación de secuencias de logs del sistema HDFS (Hadoop Distributed File System). Los sistemas distribuidos generan una gran cantidad de logs, y es crucial poder detectar anomalías en esos registros para prevenir fallos en el sistema. El objetivo será clasificar secuencias de logs como **normales** o **anormales**, utilizando PyTorch para entrenar y evaluar el modelo.\n",
    "\n",
    "### Contexto\n",
    "\n",
    "El dataset de HDFS está formado por secuencias de logs generados por sistemas distribuidos. Dado que los eventos anómalos son mucho menos frecuentes que los eventos normales, enfrentaremos el desafío de un **dataset desbalanceado**, lo cual introduce complejidades adicionales en el entrenamiento del modelo. Además, como trabajamos con datos secuenciales, las **Redes Neuronales Recurrentes (RNNs)** son una arquitectura adecuada para modelar la dependencia temporal entre eventos de la secuencia.\n",
    "\n",
    "### Introducción a la Capa de Embedding\n",
    "\n",
    "Una parte clave del pipeline será convertir los logs de texto en una representación numérica que el modelo pueda procesar. En este punto, introduciremos la **capa de embedding** de PyTorch. Las capas de embedding son una herramienta poderosa que permite mapear tokens (en este caso, logs) a vectores densos de tamaño fijo. Estas representaciones densas son más informativas y eficientes que las simples representaciones one-hot, ya que capturan relaciones semánticas entre los distintos tokens.\n",
    "\n",
    "### Objetivos de la Clase\n",
    "\n",
    "1. **Preprocesar secuencias de logs del sistema HDFS** para convertirlas en un formato adecuado para un modelo de RNN.\n",
    "2. **Implementar una capa de embedding en PyTorch** para transformar los logs en representaciones numéricas densas.\n",
    "3. **Entrenar un modelo RNN para la clasificación de secuencias**, abordando el reto del desbalance en los datos.\n",
    "4. **Evaluar el desempeño del modelo en la detección de anomalías** en el sistema HDFS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from utils import (\n",
    "    train,\n",
    "    model_classification_report,\n",
    "    plot_training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la semilla para que los resultados sean reproducibles\n",
    "SEED = 23\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "elif torch.xpu.is_available():\n",
    "    DEVICE = \"xpu\"  # si no hay GPU, pero hay XPU, usamos XPU\n",
    "\n",
    "print(f\"Usando {DEVICE}\")\n",
    "\n",
    "NUM_WORKERS = 0 # Win y MacOS pueden tener problemas con múltiples workers\n",
    "if sys.platform == 'linux':\n",
    "    NUM_WORKERS = 4  # numero de workers para cargar los datos (depende de cada caso)\n",
    "\n",
    "print(f\"Usando {NUM_WORKERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256  # tamaño del batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Datos\n",
    "\n",
    "En primer lugar, cargaremos el dataset de logs de HDFS. Este dataset contiene secuencias de logs generados por un sistema distribuido. Cada secuencia de logs está etiquetada como **normal** o **anormal**. El objetivo es entrenar un modelo que pueda clasificar automáticamente si una secuencia de logs es normal o anormal.\n",
    "\n",
    "Pero nosotros no vamos a usar el dataset original, ejemplo:\n",
    "\n",
    "```plaintext\n",
    "2025-10-12 11:00:00,123 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* NameSystem.addStoredBlock: block blk_1073741825 added to datanode DatanodeInfoWithStorage[127.0.0.1:50010]\n",
    "2025-10-12 11:00:00,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder for block blk_1073741825 terminating\n",
    "2025-10-12 11:00:01,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received block blk_1073741826 from /127.0.0.1\n",
    "2025-10-12 11:00:02,104 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/joaquin/file1.txt. BP-123-... blk_1073741827\n",
    "2025-10-12 11:00:03,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder for block blk_1073741827 terminating\n",
    "2025-10-12 11:00:05,521 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Could not replicate blk_1073741827 to 192.168.1.3 because of disk failure\n",
    "```\n",
    "Sino que vamos a usar un dataset ya preprocesado, que contiene secuencias de logs tokenizadas y convertidas en índices numéricos. Este dataset está dividido en conjuntos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = str(Path(\"data\") / \"log_classification_train.csv\")\n",
    "TEST_PATH = str(Path(\"data\") / \"log_classification_test.csv\")\n",
    "\n",
    "# Cargamos los datos\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis exploratorio de datos\n",
    "\n",
    "Primero, realizaremos un análisis exploratorio de los datos para comprender mejor la distribución de las clases y la longitud de las secuencias de logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribución de las clases\n",
    "class_distribution = train_df[\"class\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.title('Distribución de Clases en el Conjunto de Entrenamiento')\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largo de secuencias\n",
    "train_df['sequence_length'] = train_df['sequence'].apply(lambda x: len(eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(train_df['sequence_length'], bins=50, alpha=0.5, label='Train')\n",
    "plt.hist(train_df['sequence_length'], bins=50, alpha=0.5, label='Test')\n",
    "plt.title('Distribución del Largo de las Secuencias')\n",
    "plt.xlabel('Largo de la Secuencia')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Secuencia más larga: {train_df['sequence_length'].max()}\")\n",
    "print(f\"Secuencia más corta: {train_df['sequence_length'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Histograma para la clase \"normal\"\n",
    "plt.hist(train_df[train_df['class'] == 'normal']['sequence_length'], bins=30, alpha=0.5, label='Normal')\n",
    "\n",
    "plt.title('Distribución del Largo de las Secuencias por Clase')\n",
    "plt.xlabel('Largo de la Secuencia')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Secuencia más larga: {train_df[train_df['class'] == 'normal']['sequence_length'].max()}\")\n",
    "print(f\"Secuencia más corta: {train_df[train_df['class'] == 'normal']['sequence_length'].min()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Histograma para la clase \"abnormal\"\n",
    "plt.hist(train_df[train_df['class'] == 'abnormal']['sequence_length'], bins=30, alpha=0.5, label='Abnormal')\n",
    "\n",
    "plt.title('Distribución del Largo de las Secuencias por Clase')\n",
    "plt.xlabel('Largo de la Secuencia')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Secuencia más larga: {train_df[train_df['class'] == 'abnormal']['sequence_length'].max()}\")\n",
    "print(f\"Secuencia más corta: {train_df[train_df['class'] == 'abnormal']['sequence_length'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué valores tienen los datos? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las secuencias de string a listas y obtener una lista de listas\n",
    "all_sequences = train_df['sequence'].apply(lambda x: list(eval(x)))\n",
    "\n",
    "# Aplanar la lista de listas para tener una lista de todos los elementos de todas las secuencias\n",
    "all_elements = [item for sublist in all_sequences for item in sublist]\n",
    "\n",
    "# Encontrar el valor mínimo y máximo\n",
    "min_value = min(all_elements)\n",
    "max_value = max(all_elements)\n",
    "\n",
    "print(f\"Valor mínimo de las secuencias: {min_value}\")\n",
    "print(f\"Valor máximo de las secuencias: {max_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de Datos\n",
    "\n",
    "Dado que vamos a necesitar una sequencia de logs de longitud fija para entrenar el modelo, realizaremos un preprocesamiento de los datos para convertir las secuencias de logs en tensores de longitud fija. Además, como el valor minimos de la secuencia es 0 vamos a dejar lo en base 1 (sumando 1 a cada valor). De esa manera reservamos el 0 para hacer padding.\n",
    "\n",
    "Preguntas relevantes:\n",
    "- ¿Qué es más importante, el principio o el final de la secuencia?\n",
    "- ¿Qué longitud de secuencia es adecuada para capturar la información relevante?\n",
    "- ¿Hacer left-padding o right-padding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50  # Longitud de las secuencias con las que vamos a trabajar\n",
    "PADDING_VALUE = 0  # Valor usado para hacer padding\n",
    "\n",
    "def preprocess_sequence(sequence, max_len=SEQ_LEN, truncating='post', padding_value=PADDING_VALUE, padding='left'):\n",
    "    # Convertir la secuencia de string a lista de enteros (en base 1)\n",
    "    sequence = [x + 1 for x in eval(sequence)]\n",
    "    \n",
    "    # Si la secuencia es más larga que max_len, la truncamos \n",
    "    if len(sequence) > max_len:\n",
    "        if truncating == 'post':\n",
    "            sequence = sequence[:max_len]\n",
    "        else:\n",
    "            sequence = sequence[-max_len:]\n",
    "    \n",
    "    # Si la secuencia es más corta que max_len, añadimos padding\n",
    "    if len(sequence) < max_len:\n",
    "        padding_size = max_len - len(sequence)\n",
    "        \n",
    "        if padding == 'right':\n",
    "            sequence = sequence + [padding_value] * padding_size\n",
    "        else:\n",
    "            sequence = [padding_value] * padding_size + sequence\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Aplicamos el preprocesamiento a todas las secuencias en el DataFrame\n",
    "train_df['padded_sequence'] = train_df['sequence'].apply(lambda x: preprocess_sequence(x, truncating='pre', padding='left'))\n",
    "test_df['padded_sequence'] = test_df['sequence'].apply(lambda x: preprocess_sequence(x, truncating='pre', padding='left'))\n",
    "\n",
    "# Ver el resultado\n",
    "print(train_df[['sequence', 'padded_sequence']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences (list of lists): Secuencias preprocesadas.\n",
    "            labels (list): Etiquetas correspondientes (e.g., 0 para normal, 1 para abnormal).\n",
    "        \"\"\"\n",
    "        self.sequences = sequences  # Las secuencias preprocesadas (padded)\n",
    "        self.labels = labels  # Las etiquetas (normal, abnormal)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Devuelve la longitud del dataset (número de secuencias)\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Devuelve una secuencia y su etiqueta correspondiente\n",
    "        sequence = torch.tensor(self.sequences[idx], dtype=torch.long)  # Convertimos la secuencia a tensor\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)  # Convertimos la etiqueta a tensor\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar `sklearn.model_selection.train_test_split` para dividir el dataset en conjuntos de entrenamiento y validación ya que el dataset esta desbalanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = train_df['padded_sequence'].tolist()  # Lista de secuencias preprocesadas\n",
    "labels = [0 if cls == 'normal' else 1 for cls in train_df['class']]  # Etiquetas convertidas a 0 y 1\n",
    "\n",
    "# Dividir las secuencias y las etiquetas en entrenamiento y prueba\n",
    "# manteniendo la proporción de clases (stratify=labels)\n",
    "train_seq, val_seq, train_labels, val_labels = train_test_split(sequences, labels, test_size=0.2, random_state=SEED, stratify=labels)\n",
    "\n",
    "train_dataset = LogDataset(train_seq, train_labels)\n",
    "val_dataset = LogDataset(val_seq, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = test_df['padded_sequence'].tolist()\n",
    "test_labels = [0 if cls == 'normal' else 1 for cls in test_df['class']]\n",
    "test_dataset = LogDataset(test_seq, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size):\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data_loaders(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que nuestro dataset está desbalanceado, usaremos ponderación de clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = Counter(train_labels)\n",
    "total = sum(class_counts.values())\n",
    "class_weights = {cls: total/count for cls, count in class_counts.items()}\n",
    "class_weights = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float).to(DEVICE)\n",
    "\n",
    "print(f\"Class Weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITERION = nn.CrossEntropyLoss(weight=class_weights).to(DEVICE)  # función de pérdida con pesos para cada clase\n",
    "LR = 0.0005\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Embedding\n",
    "\n",
    "La [capa de embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) en PyTorch es una capa lineal que mapea un índice a un vector de características. Por ejemplo, si nuestro vocabulario tiene 100 palabras/simbolos y estamos utilizando un embedding de tamaño 20, la capa de embedding tendrá una matriz de pesos de tamaño 100 x 20. Dado un índice de palabra, la capa de embedding devuelve la fila correspondiente de la matriz de pesos, que es el vector de características de la palabra.\n",
    "\n",
    "Pregunta relevante:\n",
    "- ¿Qué tamaño de embedding es adecuado para nuestros logs? Regla general: usasr la raiz cuadrada del vocabulario o el logaritmo (base 2) del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = max_value + 2 # Tamaño del vocabulario (+1 por el padding, +1 porque los índices empiezan en 1)\n",
    "EMBEDDING_DIM = 8  # Dimensión del embedding\n",
    "\n",
    "embedding_layer = nn.Embedding(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, padding_idx=PADDING_VALUE # Los pesos correspondientes al padding no se actualizan\n",
    ")\n",
    "word_indices = torch.tensor([0, 1, 2, 3, 4, 5], dtype=torch.int32) # <- IMPORTANTE: la capa de embedding espera tensores de tipo long/enteros\n",
    "embedded = embedding_layer(word_indices)\n",
    "\n",
    "print(f\"Word indices: {word_indices}\")\n",
    "print(f\"Embedded: {embedded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = torch.tensor([2, 10, 50], dtype=torch.int32)\n",
    "# embedding_layer(word_indices) # por qué falla?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.RNN\n",
    "\n",
    "La capa [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) en PyTorch es una capa recurrente que procesa una secuencia de entrada paso a paso, manteniendo un estado oculto que captura la información de pasos anteriores. Dado un tensor de entrada de tamaño `(batch, secuencia, características)`, la capa RNN procesa la secuencia paso a paso y devuelve el estado oculto final para cada secuencia en el lote.\n",
    "\n",
    "<!-- ![rnn](https://d2l.ai/_images/rnn.svg) -->\n",
    "<img src=\"https://d2l.ai/_images/rnn.svg\" width=\"500\" style=\"background:white; display: block; margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fórmula Matemática de la RNN**\n",
    "\n",
    "Según la [documentación oficial de PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), para cada paso temporal $t$, una RNN calcula:\n",
    "\n",
    "$$h_t = \\tanh( x_t W_{ih}^T + b_{ih} +  h_{t-1} W_{hh}^T + b_{hh})$$\n",
    "\n",
    "Donde:\n",
    "- $x_t$ es la entrada en el tiempo $t$ (shape: `input_size`)\n",
    "- $h_{t-1}$ es el estado oculto del paso anterior (shape: `hidden_size`)\n",
    "- $h_t$ es el nuevo estado oculto (shape: `hidden_size`)\n",
    "- $W_{ih}$ son los pesos input-to-hidden (shape: `hidden_size × input_size`)\n",
    "- $W_{hh}$ son los pesos hidden-to-hidden (shape: `hidden_size × hidden_size`)\n",
    "- $b_{ih}$ y $b_{hh}$ son los biases (shape: `hidden_size`)\n",
    "\n",
    "> Notar que tiene dos términos de bias pero podría ser uno solo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parámetros de la RNN**\n",
    "\n",
    "Vamos a crear una RNN muy pequeña para hacer las cuentas a mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1   # un solo ejemplo\n",
    "seq_len = 2      # secuencia de solo 2 pasos\n",
    "input_size = 3   # cantidad de características de entrada\n",
    "hidden_size = 4  # dimensión del hidden state\n",
    "\n",
    "# Crear la RNN\n",
    "rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True) # por defecto batch_first=False\n",
    "\n",
    "print(\"=== PARÁMETROS DE LA RNN ===\\n\")\n",
    "for name, param in rnn.named_parameters():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Shape: {param.shape}\")\n",
    "    print(f\"  Valores:\\n{param.data}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    [[1.0, 2.0, 3.0],   # paso temporal t=0\n",
    "     [0.5, 1.5, 2.5]]   # paso temporal t=1\n",
    "])  # (batch_size, seq_len, input_size)\n",
    "\n",
    "# Extraer los parámetros\n",
    "W_ih = rnn.weight_ih_l0.data  # (4, 3)\n",
    "W_hh = rnn.weight_hh_l0.data  # (4, 4)\n",
    "b_ih = rnn.bias_ih_l0.data    # (4,)\n",
    "b_hh = rnn.bias_hh_l0.data    # (4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estado inicial (por defecto es cero)\n",
    "x_0 = x[:, 0, :]  # (1, 3) - primera entrada\n",
    "h_0 = torch.zeros(batch_size, hidden_size)  # (1, 4)\n",
    "\n",
    "print(\"=== PASO TEMPORAL t=0 ===\\n\")\n",
    "print(f\"x_0: {x_0}\")\n",
    "print(f\"h_0 (inicial): {h_0}\\n\")\n",
    "\n",
    "# Paso 1: input contribution\n",
    "input_contrib = x_0 @ W_ih.T + b_ih  # (1, 4)\n",
    "print(f\"Input contribution (x_0 @ W_ih.T + b_ih):\\n{input_contrib}\\n\")\n",
    "\n",
    "# Paso 2: hidden contribution\n",
    "hidden_contrib = h_0 @ W_hh.T + b_hh  # (1, 4)\n",
    "print(f\"Hidden contribution (h_0 @ W_hh.T + b_hh):\\n{hidden_contrib}\\n\")\n",
    "\n",
    "# Paso 3: sumar y aplicar tanh\n",
    "h_1 = torch.tanh(input_contrib + hidden_contrib)\n",
    "print(f\"h_1 = tanh(input_contrib + hidden_contrib):\\n{h_1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = x[:, 1, :]  # (1, 3) - segunda entrada\n",
    "h_1 = h_1  # (1, 4) - estado oculto del paso anterior\n",
    "\n",
    "print(\"=== PASO TEMPORAL t=1 ===\\n\")\n",
    "print(f\"x_1: {x_1}\")\n",
    "print(f\"h_1 (anterior): {h_1}\\n\")\n",
    "\n",
    "# Paso 1: input contribution\n",
    "input_contrib = x_1 @ W_ih.T + b_ih  # (1, 4)\n",
    "print(f\"Input contribution (x_1 @ W_ih.T + b_ih):\\n{input_contrib}\\n\")\n",
    "\n",
    "# Paso 2: hidden contribution\n",
    "hidden_contrib = h_1 @ W_hh.T + b_hh  # (1, 4)\n",
    "print(f\"Hidden contribution (h_1 @ W_hh.T + b_hh):\\n{hidden_contrib}\\n\")\n",
    "\n",
    "# Paso 3: sumar y aplicar tanh\n",
    "h_2 = torch.tanh(input_contrib + hidden_contrib)\n",
    "print(f\"h_2 = tanh(input_contrib + hidden_contrib):\\n{h_2}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos si coincide con la implementación de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass usando PyTorch\n",
    "output, hidden = rnn(x)\n",
    "\n",
    "print(\"=== OUTPUT DE PYTORCH ===\")\n",
    "print(f\"Output shape: {output.shape}\")  # (batch_size, seq_len, hidden_size)\n",
    "print(f\"Output:\\n{output}\\n\")\n",
    "\n",
    "print(f\"Hidden shape: {hidden.shape}\")  # (num_layers, batch_size, hidden_size)\n",
    "print(f\"Hidden:\\n{hidden}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=EMBEDDING_DIM, hidden_size=64, batch_first=True)\n",
    "tensor = torch.randn(BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM)\n",
    "\n",
    "output, hidden = rnn(tensor)\n",
    "print(f\"Output shape: {output.shape} (batch_size, seq_len, hidden_size)\")\n",
    "print(f\"Hidden shape: {hidden.shape} (num_layers, batch_size, hidden_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDFSClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes=2):\n",
    "        super(HDFSClassifier, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "summary(\n",
    "    HDFSClassifier(VOCAB_SIZE, EMBEDDING_DIM, hidden_dim=128, num_classes=2),\n",
    "    input_size=(BATCH_SIZE, SEQ_LEN),\n",
    "    dtypes=[torch.long],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HDFSClassifier(VOCAB_SIZE, EMBEDDING_DIM, hidden_dim=128, num_classes=2).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors, val_errors = train(\n",
    "    model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=CRITERION,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    do_early_stopping=False,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(train_errors, val_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification_report(model, test_loader, DEVICE, 2, digits=4, do_confusion_matrix=True, do_balanced_accuracy=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taller-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
