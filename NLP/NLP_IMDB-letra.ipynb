{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a NLP: Preprocesamiento de Texto con PyTorch\n",
    "\n",
    "En esta notebook, nos enfocaremos en el preprocesamiento de texto, un paso fundamental en cualquier proyecto de Procesamiento de Lenguaje Natural (NLP). Antes de sumergirnos en arquitecturas complejas como RNNs o Transformers, es crucial dominar cómo preparar los datos de texto para que sean utilizados eficazmente por los modelos de aprendizaje profundo.\n",
    "\n",
    "## Intro\n",
    "\n",
    "### Objetivos\n",
    "\n",
    "1. **Explorar técnicas clave de preprocesamiento de texto**, incluyendo tokenización, limpieza, y manejo de vocabulario.\n",
    "2. **Comprender la importancia del padding y el truncamiento** en el manejo de secuencias de texto de longitud variable.\n",
    "3. **Convertir texto en representaciones numéricas** adecuadas para ser ingresadas en modelos de NLP.\n",
    "4. **Implementar un pipeline de preprocesamiento** en PyTorch que prepare el texto para futuras etapas de modelado.\n",
    "\n",
    "### Contenido\n",
    "\n",
    "1. Introducción al concepto de preprocesamiento en NLP y su relevancia.\n",
    "2. Limpieza y tokenización del texto utilizando bibliotecas estándar.\n",
    "3. Construcción de un vocabulario a partir de datos textuales.\n",
    "4. Conversión de texto en índices numéricos para su uso en modelos.\n",
    "5. Implementación de técnicas de padding y truncamiento.\n",
    "6. Preparación del dataset para ser utilizado en modelos de aprendizaje profundo en PyTorch.\n",
    "\n",
    "### Sobre el Dataset IMDB\n",
    "\n",
    "En esta notebook utilizaremos el dataset de reseñas de películas de IMDB, un conjunto de datos ampliamente utilizado en la investigación de NLP. El dataset contiene 50,000 reseñas de películas en inglés, etiquetadas como **positivas** o **negativas**. Se divide equitativamente en un conjunto de entrenamiento y un conjunto de prueba, con 25,000 reseñas en cada uno. Las reseñas positivas y negativas están equilibradas, lo que lo convierte en un excelente recurso para entrenar y evaluar modelos de análisis de sentimiento.\n",
    "\n",
    "El dataset fue creado por Andrew Maas y sus colegas en la Universidad de Stanford, y está disponible públicamente [aquí](https://ai.stanford.edu/~amaas/data/sentiment/). El propósito principal de este conjunto de datos es facilitar la investigación en tareas de clasificación de texto, como el análisis de sentimientos, donde el objetivo es predecir si una reseña es positiva o negativa basándose en su contenido textual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from utils import (\n",
    "    train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la semilla para que los resultados sean reproducibles\n",
    "SEED = 23\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando cpu\n",
      "Usando 4\n"
     ]
    }
   ],
   "source": [
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "elif torch.xpu.is_available():\n",
    "    DEVICE = \"xpu\"  # si no hay GPU, pero hay XPU, usamos XPU\n",
    "\n",
    "print(f\"Usando {DEVICE}\")\n",
    "\n",
    "NUM_WORKERS = 0 # Win y MacOS pueden tener problemas con múltiples workers\n",
    "if sys.platform == 'linux':\n",
    "    NUM_WORKERS = 4  # numero de workers para cargar los datos (depende de cada caso)\n",
    "\n",
    "print(f\"Usando {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512  # tamaño del batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Datos\n",
    "\n",
    "Primero, descargaremos el [dataset IMDB](https://ai.stanford.edu/~amaas/data/sentiment/) y lo cargaremos en un DataFrame de Pandas para su fácil manipulación.\n",
    "\n",
    "Se nos presentan dos carpetas: `train` y `test`, cada una con subcarpetas `pos` y `neg` que contienen reseñas positivas y negativas, respectivamente. Cada reseña se almacena en un archivo de texto separado. Utilizaremos la biblioteca `os` para navegar por las carpetas y cargar las reseñas en un DataFrame de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Download complete.\n",
      "Extracting files...\n",
      "Extraction complete.\n",
      "Dataset extracted to: data/aclImdb\n",
      "Contents: ['train', 'imdbEr.txt', 'README', 'imdb.vocab', 'test']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"data\"\n",
    "\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "tar_path = os.path.join(DATA_PATH, \"aclImdb_v1.tar.gz\")\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "# Download the file if not already downloaded\n",
    "if not os.path.exists(tar_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(url, tar_path)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"File already downloaded.\")\n",
    "\n",
    "# Extract the tar.gz file\n",
    "print(\"Extracting files...\")\n",
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=DATA_PATH)\n",
    "print(\"Extraction complete.\")\n",
    "\n",
    "# Optional: print extracted folder contents\n",
    "extracted_path = os.path.join(DATA_PATH, \"aclImdb\")\n",
    "print(f\"Dataset extracted to: {extracted_path}\")\n",
    "print(\"Contents:\", os.listdir(extracted_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = str(Path(extracted_path) / \"train\")\n",
    "TEST_PATH = str(Path(extracted_path) / \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de IMDB desde archivos locales\n",
    "def load_imdb_data(base_directory):\n",
    "    data = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        folder = os.path.join(base_directory, label)\n",
    "        for file in os.listdir(folder):\n",
    "            with open(os.path.join(folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                data.append((f.read(), 1 if label == \"pos\" else 0)) # 1 para positivo, 0 para negativo\n",
    "    return pd.DataFrame(data, columns=[\"review\", \"sentiment\"])\n",
    "\n",
    "train_df = load_imdb_data(TRAIN_PATH)\n",
    "test_df = load_imdb_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar más caracteres por columna\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Mostrar más columnas y filas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 0)  # 0 = autoajuste según la terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "d261ef62-49e7-431f-bfd5-874e3d8632bd",
       "rows": [
        [
         "0",
         "YES, the plot is hardly plausible and very thin. YES, the acting does range from average to laughable. YES, it has been done so many times before. However what we are dealing with is a film that does not shy away from these facts and pretends to be nothing more than it is. There are indeed some original death scenes and the tension does increase throughout the movie. In addition you are never more than a few minutes away from a gory killing. I urge everyone to watch this film with an unprejudiced eye and see it for what it set out to be; a scary, funny slasher flick with a theme tune second to none.",
         "1"
        ],
        [
         "1",
         "This movie was great! It was an excellent rendition of an ancient myth. The animation was somewhat odd, but nothing new from Disney. It was definitely better than expected for a Disney movie with no singing.<br /><br />The background animation was magical. It was a different level of work for the Disney people. Some of the characters were a little boxy, but it was more than made up for with the beauty and lushness of the scenery. The music was largely instrumental but that was perfect for the movie. This was definitely not a film that needed the characters to bust into song.<br /><br />Perfect. 10 out of 10.",
         "1"
        ],
        [
         "2",
         "One of my favourite films first saw it when I was about 10, which probably tells you a lot about the type of humour. Although dated the humour definitely has a charm about it. Expect to see the usual Askey & Murdoch banter so popular in its day, with lots of interesting, quirky co-characters. The lady with the parrot, the couple due to get married and are in trouble from 'her', and my favourite, the stationmaster, \"Nobody knows where it comes from ... nobody knows where it goes..\" Interestingly the ghost train was written by Arnold Ridley of Dads Army fame (Private Godfrey the medic) Watch it on a rainy Sunday afternoon after your lunch and smile.",
         "1"
        ],
        [
         "3",
         "I am surprised than many viewers hold more respect for the sequel to this brilliant movie... I have seen all the guinea pigs and this one is easily the best.<br /><br />Even though ive seen the \"making of\", i still have doubts when watching those 35mins of pure torture : its that powerful.<br /><br />A 10 out of 10 because this movie achieved perfectly what it set out to do : be the best fake snuff film ever made.",
         "1"
        ],
        [
         "4",
         "Part Two picks up... not where the last film left off. As part of the quasi-conventionality of Steven Soderbergh's epic 4+ hour event, Che's two stories are told as classic \"Rise\" and \"Fall\" scenarios. In Part Two, Che Guevara, leaving his post as a bureaucrat in Cuba and after a failed attempt in the Congo (only in passing mentioned in the film), goes down to Bolivia to try and start up another through-the-jungle style revolution. Things don't go quite as well planned, at all, probably because of Che's then notorious stature as a Communist and revolutionary, and in part because of America's involvement on the side of the Bolivian Government, and, of course, that Castro wasn't really around as a back-up for Che.<br /><br />As it goes, the second part of Che is sadder, but in some ways wiser than the first part. Which makes sense, as Guevara has to endure low morale from his men, betrayals from those around him, constant mistakes by grunts and nearby peasants, and by ultimately the enclosing, larger military force. But what's sadder still is that Guevara, no matter what, won't give in. One may see this as an incredible strength or a fatal flaw- maybe both- but it's also clear how one starts to see Che, if not totally more fully rounded, then as something of a more sympathetic character. True, he did kill, and executed, and felt justified all the way. And yet it starts to work on the viewer in the sense of a primal level of pity; the sequence where Guevara's health worsens without medicine, leading up to the shocking stabbing of a horse, marks as one of the most memorable and satisfying of any film this year.<br /><br />Again, Soderbergh's command of narrative is strong, if, on occasion, slightly sluggish (understandable due to the big running time), and one or two scenes just feel totally odd (Matt Damon?), but these are minor liabilities. Going this time for the straight color camera approach, this is almost like a pure militia-style war picture, told with a great deal of care for the men in the group, as well as Guevara as the Lord-over this group, and how things dwindle down the final scene. And as always, Del-Toro is at the top of his game, in every scene, every beat knowing this guy so well- for better and for worse- that he comes about as close to embodiment as possible. Overall, the two parts of Che make up an impressive package: history as drama in compelling style, good for an audience even if they don't know Che or, better, if they don't think highly of him. It's that special. 8.5/10",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YES, the plot is hardly plausible and very thin. YES, the acting does range from average to laughable. YES, it has been done so many times before. However what we are dealing with is a film that does not shy away from these facts and pretends to be nothing more than it is. There are indeed some original death scenes and the tension does increase throughout the movie. In addition you are never more than a few minutes away from a gory killing. I urge everyone to watch this film with an unprejudiced eye and see it for what it set out to be; a scary, funny slasher flick with a theme tune second to none.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie was great! It was an excellent rendition of an ancient myth. The animation was somewhat odd, but nothing new from Disney. It was definitely better than expected for a Disney movie with no singing.&lt;br /&gt;&lt;br /&gt;The background animation was magical. It was a different level of work for the Disney people. Some of the characters were a little boxy, but it was more than made up for with the beauty and lushness of the scenery. The music was largely instrumental but that was perfect for the movie. This was definitely not a film that needed the characters to bust into song.&lt;br /&gt;&lt;br /&gt;Perfect. 10 out of 10.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of my favourite films first saw it when I was about 10, which probably tells you a lot about the type of humour. Although dated the humour definitely has a charm about it. Expect to see the usual Askey &amp; Murdoch banter so popular in its day, with lots of interesting, quirky co-characters. The lady with the parrot, the couple due to get married and are in trouble from 'her', and my favourite, the stationmaster, \"Nobody knows where it comes from ... nobody knows where it goes..\" Interestingly the ghost train was written by Arnold Ridley of Dads Army fame (Private Godfrey the medic) Watch it on a rainy Sunday afternoon after your lunch and smile.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am surprised than many viewers hold more respect for the sequel to this brilliant movie... I have seen all the guinea pigs and this one is easily the best.&lt;br /&gt;&lt;br /&gt;Even though ive seen the \"making of\", i still have doubts when watching those 35mins of pure torture : its that powerful.&lt;br /&gt;&lt;br /&gt;A 10 out of 10 because this movie achieved perfectly what it set out to do : be the best fake snuff film ever made.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Part Two picks up... not where the last film left off. As part of the quasi-conventionality of Steven Soderbergh's epic 4+ hour event, Che's two stories are told as classic \"Rise\" and \"Fall\" scenarios. In Part Two, Che Guevara, leaving his post as a bureaucrat in Cuba and after a failed attempt in the Congo (only in passing mentioned in the film), goes down to Bolivia to try and start up another through-the-jungle style revolution. Things don't go quite as well planned, at all, probably because of Che's then notorious stature as a Communist and revolutionary, and in part because of America's involvement on the side of the Bolivian Government, and, of course, that Castro wasn't really around as a back-up for Che.&lt;br /&gt;&lt;br /&gt;As it goes, the second part of Che is sadder, but in some ways wiser than the first part. Which makes sense, as Guevara has to endure low morale from his men, betrayals from those around him, constant mistakes by grunts and nearby peasants, and by ultimately the enclosing, larger military force. But what's sadder still is that Guevara, no matter what, won't give in. One may see this as an incredible strength or a fatal flaw- maybe both- but it's also clear how one starts to see Che, if not totally more fully rounded, then as something of a more sympathetic character. True, he did kill, and executed, and felt justified all the way. And yet it starts to work on the viewer in the sense of a primal level of pity; the sequence where Guevara's health worsens without medicine, leading up to the shocking stabbing of a horse, marks as one of the most memorable and satisfying of any film this year.&lt;br /&gt;&lt;br /&gt;Again, Soderbergh's command of narrative is strong, if, on occasion, slightly sluggish (understandable due to the big running time), and one or two scenes just feel totally odd (Matt Damon?), but these are minor liabilities. Going this time for the straight color camera approach, this is almost like a pure militia-style war picture, told with a great deal of care for the men in the group, as well as Guevara as the Lord-over this group, and how things dwindle down the final scene. And as always, Del-Toro is at the top of his game, in every scene, every beat knowing this guy so well- for better and for worse- that he comes about as close to embodiment as possible. Overall, the two parts of Che make up an impressive package: history as drama in compelling style, good for an audience even if they don't know Che or, better, if they don't think highly of him. It's that special. 8.5/10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      review  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             YES, the plot is hardly plausible and very thin. YES, the acting does range from average to laughable. YES, it has been done so many times before. However what we are dealing with is a film that does not shy away from these facts and pretends to be nothing more than it is. There are indeed some original death scenes and the tension does increase throughout the movie. In addition you are never more than a few minutes away from a gory killing. I urge everyone to watch this film with an unprejudiced eye and see it for what it set out to be; a scary, funny slasher flick with a theme tune second to none.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    This movie was great! It was an excellent rendition of an ancient myth. The animation was somewhat odd, but nothing new from Disney. It was definitely better than expected for a Disney movie with no singing.<br /><br />The background animation was magical. It was a different level of work for the Disney people. Some of the characters were a little boxy, but it was more than made up for with the beauty and lushness of the scenery. The music was largely instrumental but that was perfect for the movie. This was definitely not a film that needed the characters to bust into song.<br /><br />Perfect. 10 out of 10.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            One of my favourite films first saw it when I was about 10, which probably tells you a lot about the type of humour. Although dated the humour definitely has a charm about it. Expect to see the usual Askey & Murdoch banter so popular in its day, with lots of interesting, quirky co-characters. The lady with the parrot, the couple due to get married and are in trouble from 'her', and my favourite, the stationmaster, \"Nobody knows where it comes from ... nobody knows where it goes..\" Interestingly the ghost train was written by Arnold Ridley of Dads Army fame (Private Godfrey the medic) Watch it on a rainy Sunday afternoon after your lunch and smile.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          I am surprised than many viewers hold more respect for the sequel to this brilliant movie... I have seen all the guinea pigs and this one is easily the best.<br /><br />Even though ive seen the \"making of\", i still have doubts when watching those 35mins of pure torture : its that powerful.<br /><br />A 10 out of 10 because this movie achieved perfectly what it set out to do : be the best fake snuff film ever made.   \n",
       "4  Part Two picks up... not where the last film left off. As part of the quasi-conventionality of Steven Soderbergh's epic 4+ hour event, Che's two stories are told as classic \"Rise\" and \"Fall\" scenarios. In Part Two, Che Guevara, leaving his post as a bureaucrat in Cuba and after a failed attempt in the Congo (only in passing mentioned in the film), goes down to Bolivia to try and start up another through-the-jungle style revolution. Things don't go quite as well planned, at all, probably because of Che's then notorious stature as a Communist and revolutionary, and in part because of America's involvement on the side of the Bolivian Government, and, of course, that Castro wasn't really around as a back-up for Che.<br /><br />As it goes, the second part of Che is sadder, but in some ways wiser than the first part. Which makes sense, as Guevara has to endure low morale from his men, betrayals from those around him, constant mistakes by grunts and nearby peasants, and by ultimately the enclosing, larger military force. But what's sadder still is that Guevara, no matter what, won't give in. One may see this as an incredible strength or a fatal flaw- maybe both- but it's also clear how one starts to see Che, if not totally more fully rounded, then as something of a more sympathetic character. True, he did kill, and executed, and felt justified all the way. And yet it starts to work on the viewer in the sense of a primal level of pity; the sequence where Guevara's health worsens without medicine, leading up to the shocking stabbing of a horse, marks as one of the most memorable and satisfying of any film this year.<br /><br />Again, Soderbergh's command of narrative is strong, if, on occasion, slightly sluggish (understandable due to the big running time), and one or two scenes just feel totally odd (Matt Damon?), but these are minor liabilities. Going this time for the straight color camera approach, this is almost like a pure militia-style war picture, told with a great deal of care for the men in the group, as well as Guevara as the Lord-over this group, and how things dwindle down the final scene. And as always, Del-Toro is at the top of his game, in every scene, every beat knowing this guy so well- for better and for worse- that he comes about as close to embodiment as possible. Overall, the two parts of Che make up an impressive package: history as drama in compelling style, good for an audience even if they don't know Che or, better, if they don't think highly of him. It's that special. 8.5/10   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "17433965-5798-409e-be6e-e1205df49585",
       "rows": [
        [
         "24995",
         "Spoilers ahead -- proceed at your own caution.<br /><br />My main problem with this movie is that once Harry learns the identities of the three blackmailers -- with relative ease -- he continues to cave into their demands. And then the whole scene with his wife being kidnapped, he decides to wire his classic car up to explode (with the money in it), which makes us take a pretty tall leap of logic.<br /><br />Okay, so he wanted to keep his affair with Cini out of the public eye due to his wife's involvement with the DA campaign. This I can see, but why not hire someone to slap these turds around a bit, or even kill them once he'd determined there was no actual blackmail evidence (e.g, Cini's body?) This was a pretty interesting movie for the first 2/3 of it. After that, it sort of falls apart.",
         "0"
        ],
        [
         "24996",
         "I read the book Celestine Prophecy and was looking forward to seeing the movie. Be advised that the movie is loosely based on the book. Many of the book's most interesting points do not even come out in the movie. It is a \"B\" movie at best. Many events, characters, how the character interact and meet in the book are simply changed or do not occur. The flow of events that in the book are very smooth, are choppy and fed to the view as though you a child. The character development is very poor. Personnallities of the characters differ from those in the book. The direction is similar to a \"B\" horror flick. I understand that it would take six hours in film to present all that is in the book, but they screen play base missed many points. The casting was very good.",
         "0"
        ],
        [
         "24997",
         "This is a pretty bad movie. But not so bad as it's reputation suggests. The production values aren't too bad and there is the odd effective scene. And it does have an 80's cheezoid veneer that means that it is always kind of fun. Watch out, too, for Jimmy Nail's brief appearance - his attempt at an American accent is so astoundingly rubbish it's fantastic. Fantastic too are Sybil Danning's breasts - they make a brief appearance in the movie but the scene is repeated umpteen times in the end credits in what can only be described as the 12\" remix of Sybil Danning's boobs. Has to be seen to be believed. As a horror movie it isn't scary, the effects are silly and Christopher Lee turns up to sleepwalk through his performance. I guess he was buying a new house and needed some cash for the deposit. The two central characters - the man and the woman - were so negligible that I have forgotten almost everything about them and I just watched this movie earlier tonight. The werewolves are noticeably less impressive than in the original movie, in fact, bizarrely, they sometimes look more like badly burned apes. The eastern European setting is quite good and the music provided by the new wave band Babel, while being pretty terrible, does at least give the film some added cheese.<br /><br />Overall? Good for a laugh. Not good quality but did you seriously expect it to be? And, at the very least, you've always got Sybil's knockers.",
         "0"
        ],
        [
         "24998",
         "Would someone tell shaq to stick to what he is good at basketball. This movie was not even entertaining on a stupid level. In this movie shaq plays a genie who lives in a boom box is that not orginal a genie in a boom box instead of a lamp. He is supposed to help a little boy played by the equally annoying francais cappra. This movie had the most flimsy storyline since water world, the acting was awful and I think that anyone who likes this flim would be afraid to admit it.",
         "0"
        ],
        [
         "24999",
         "I was really hoping that this would be a funny show, given all the hype and the clever preview clips. And talk about hype, I even heard an interview with the show's creator on the BBC World Today - a show that is broadcast all over the world.<br /><br />Unfortunately, this show doesn't even come close to delivering. All of the jokes are obvious - the kind that sound kind of funny the first time you hear them but after that seem lame - and they are not given any new treatment or twist. All of the characters are one-dimensional. The acting is - well - mediocre (I'm being nice). It's the classic CBC recipe - one that always fails.<br /><br />If you're Muslim I think you would have to be stupid to believe any of the white characters, and if you're white you'd probably be offended a little by the fact that almost all of the white characters are portrayed as either bigoted, ignorant, or both. Not that making fun of white people is a problem - most of the better comedies are rooted in that. It's only a problem when it isn't funny - as in this show.<br /><br />Canada is bursting with funny people - so many that we export them to Hollywood on a regular basis. So how come the producers of this show couldn't find any?",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>Spoilers ahead -- proceed at your own caution.&lt;br /&gt;&lt;br /&gt;My main problem with this movie is that once Harry learns the identities of the three blackmailers -- with relative ease -- he continues to cave into their demands. And then the whole scene with his wife being kidnapped, he decides to wire his classic car up to explode (with the money in it), which makes us take a pretty tall leap of logic.&lt;br /&gt;&lt;br /&gt;Okay, so he wanted to keep his affair with Cini out of the public eye due to his wife's involvement with the DA campaign. This I can see, but why not hire someone to slap these turds around a bit, or even kill them once he'd determined there was no actual blackmail evidence (e.g, Cini's body?) This was a pretty interesting movie for the first 2/3 of it. After that, it sort of falls apart.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I read the book Celestine Prophecy and was looking forward to seeing the movie. Be advised that the movie is loosely based on the book. Many of the book's most interesting points do not even come out in the movie. It is a \"B\" movie at best. Many events, characters, how the character interact and meet in the book are simply changed or do not occur. The flow of events that in the book are very smooth, are choppy and fed to the view as though you a child. The character development is very poor. Personnallities of the characters differ from those in the book. The direction is similar to a \"B\" horror flick. I understand that it would take six hours in film to present all that is in the book, but they screen play base missed many points. The casting was very good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>This is a pretty bad movie. But not so bad as it's reputation suggests. The production values aren't too bad and there is the odd effective scene. And it does have an 80's cheezoid veneer that means that it is always kind of fun. Watch out, too, for Jimmy Nail's brief appearance - his attempt at an American accent is so astoundingly rubbish it's fantastic. Fantastic too are Sybil Danning's breasts - they make a brief appearance in the movie but the scene is repeated umpteen times in the end credits in what can only be described as the 12\" remix of Sybil Danning's boobs. Has to be seen to be believed. As a horror movie it isn't scary, the effects are silly and Christopher Lee turns up to sleepwalk through his performance. I guess he was buying a new house and needed some cash for the deposit. The two central characters - the man and the woman - were so negligible that I have forgotten almost everything about them and I just watched this movie earlier tonight. The werewolves are noticeably less impressive than in the original movie, in fact, bizarrely, they sometimes look more like badly burned apes. The eastern European setting is quite good and the music provided by the new wave band Babel, while being pretty terrible, does at least give the film some added cheese.&lt;br /&gt;&lt;br /&gt;Overall? Good for a laugh. Not good quality but did you seriously expect it to be? And, at the very least, you've always got Sybil's knockers.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>Would someone tell shaq to stick to what he is good at basketball. This movie was not even entertaining on a stupid level. In this movie shaq plays a genie who lives in a boom box is that not orginal a genie in a boom box instead of a lamp. He is supposed to help a little boy played by the equally annoying francais cappra. This movie had the most flimsy storyline since water world, the acting was awful and I think that anyone who likes this flim would be afraid to admit it.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>I was really hoping that this would be a funny show, given all the hype and the clever preview clips. And talk about hype, I even heard an interview with the show's creator on the BBC World Today - a show that is broadcast all over the world.&lt;br /&gt;&lt;br /&gt;Unfortunately, this show doesn't even come close to delivering. All of the jokes are obvious - the kind that sound kind of funny the first time you hear them but after that seem lame - and they are not given any new treatment or twist. All of the characters are one-dimensional. The acting is - well - mediocre (I'm being nice). It's the classic CBC recipe - one that always fails.&lt;br /&gt;&lt;br /&gt;If you're Muslim I think you would have to be stupid to believe any of the white characters, and if you're white you'd probably be offended a little by the fact that almost all of the white characters are portrayed as either bigoted, ignorant, or both. Not that making fun of white people is a problem - most of the better comedies are rooted in that. It's only a problem when it isn't funny - as in this show.&lt;br /&gt;&lt;br /&gt;Canada is bursting with funny people - so many that we export them to Hollywood on a regular basis. So how come the producers of this show couldn't find any?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                review  \\\n",
       "24995                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Spoilers ahead -- proceed at your own caution.<br /><br />My main problem with this movie is that once Harry learns the identities of the three blackmailers -- with relative ease -- he continues to cave into their demands. And then the whole scene with his wife being kidnapped, he decides to wire his classic car up to explode (with the money in it), which makes us take a pretty tall leap of logic.<br /><br />Okay, so he wanted to keep his affair with Cini out of the public eye due to his wife's involvement with the DA campaign. This I can see, but why not hire someone to slap these turds around a bit, or even kill them once he'd determined there was no actual blackmail evidence (e.g, Cini's body?) This was a pretty interesting movie for the first 2/3 of it. After that, it sort of falls apart.   \n",
       "24996                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 I read the book Celestine Prophecy and was looking forward to seeing the movie. Be advised that the movie is loosely based on the book. Many of the book's most interesting points do not even come out in the movie. It is a \"B\" movie at best. Many events, characters, how the character interact and meet in the book are simply changed or do not occur. The flow of events that in the book are very smooth, are choppy and fed to the view as though you a child. The character development is very poor. Personnallities of the characters differ from those in the book. The direction is similar to a \"B\" horror flick. I understand that it would take six hours in film to present all that is in the book, but they screen play base missed many points. The casting was very good.   \n",
       "24997  This is a pretty bad movie. But not so bad as it's reputation suggests. The production values aren't too bad and there is the odd effective scene. And it does have an 80's cheezoid veneer that means that it is always kind of fun. Watch out, too, for Jimmy Nail's brief appearance - his attempt at an American accent is so astoundingly rubbish it's fantastic. Fantastic too are Sybil Danning's breasts - they make a brief appearance in the movie but the scene is repeated umpteen times in the end credits in what can only be described as the 12\" remix of Sybil Danning's boobs. Has to be seen to be believed. As a horror movie it isn't scary, the effects are silly and Christopher Lee turns up to sleepwalk through his performance. I guess he was buying a new house and needed some cash for the deposit. The two central characters - the man and the woman - were so negligible that I have forgotten almost everything about them and I just watched this movie earlier tonight. The werewolves are noticeably less impressive than in the original movie, in fact, bizarrely, they sometimes look more like badly burned apes. The eastern European setting is quite good and the music provided by the new wave band Babel, while being pretty terrible, does at least give the film some added cheese.<br /><br />Overall? Good for a laugh. Not good quality but did you seriously expect it to be? And, at the very least, you've always got Sybil's knockers.   \n",
       "24998                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Would someone tell shaq to stick to what he is good at basketball. This movie was not even entertaining on a stupid level. In this movie shaq plays a genie who lives in a boom box is that not orginal a genie in a boom box instead of a lamp. He is supposed to help a little boy played by the equally annoying francais cappra. This movie had the most flimsy storyline since water world, the acting was awful and I think that anyone who likes this flim would be afraid to admit it.   \n",
       "24999                                                                                                                                                                                                                       I was really hoping that this would be a funny show, given all the hype and the clever preview clips. And talk about hype, I even heard an interview with the show's creator on the BBC World Today - a show that is broadcast all over the world.<br /><br />Unfortunately, this show doesn't even come close to delivering. All of the jokes are obvious - the kind that sound kind of funny the first time you hear them but after that seem lame - and they are not given any new treatment or twist. All of the characters are one-dimensional. The acting is - well - mediocre (I'm being nice). It's the classic CBC recipe - one that always fails.<br /><br />If you're Muslim I think you would have to be stupid to believe any of the white characters, and if you're white you'd probably be offended a little by the fact that almost all of the white characters are portrayed as either bigoted, ignorant, or both. Not that making fun of white people is a problem - most of the better comedies are rooted in that. It's only a problem when it isn't funny - as in this show.<br /><br />Canada is bursting with funny people - so many that we export them to Hollywood on a regular basis. So how come the producers of this show couldn't find any?   \n",
       "\n",
       "       sentiment  \n",
       "24995          0  \n",
       "24996          0  \n",
       "24997          0  \n",
       "24998          0  \n",
       "24999          0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La distribución de clases en el conjunto de entrenamiento es: sentiment\n",
      "1    12500\n",
      "0    12500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_class_distribution = train_df[\"sentiment\"].value_counts()\n",
    "\n",
    "print(\n",
    "    \"La distribución de clases en el conjunto de entrenamiento es:\",\n",
    "    train_class_distribution,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de Texto\n",
    "\n",
    "El preprocesamiento de texto es esencial para preparar los datos de texto antes de ingresarlos en un modelo de aprendizaje profundo. A continuación se describen los pasos clave que realizaremos:\n",
    "\n",
    "1. **Limpieza del Texto de Bajo Nivel**:\n",
    "   - **Eliminación de HTML**: Remover etiquetas HTML u otros elementos de markup.\n",
    "   - **Eliminación de Texto entre Corchetes**: Eliminar texto entre corchetes, como [imagen], [audio], etc.\n",
    "   - **Eliminación de Caracteres No AlfaNuméricos**: Remover caracteres especiales, como puntuación y otros símbolos.\n",
    "   - **Eliminación de Espacios en Blanco Adicionales**: Remover espacios en blanco adicionales y espacios al principio y al final del texto.\n",
    "\n",
    "2. **Limpieza del Texto de Alto Nivel**:\n",
    "   - **Transformación a Minúsculas**: Convertir todo el texto a minúsculas para evitar duplicados.\n",
    "   - **Eliminación de Stop Words**: Remover palabras comunes que no aportan información.\n",
    "   - **Lematización**: Convertir las palabras a su forma base utilizando técnicas de lematización.\n",
    "\n",
    "3. **Construcción del Vocabulario**:\n",
    "   - **Creación de un Diccionario**: Asignar un índice único a cada palabra en el corpus.\n",
    "   - **Filtro por Frecuencia**: Eliminar palabras demasiado raras o comunes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza del Texto de Bajo Nivel\n",
    "\n",
    "En esta etapa, eliminaremos las etiquetas HTML, la puntuación, los caracteres especiales y los espacios en blanco innecesarios de las reseñas. Nos ayudaremos con expresiones regulares de la [biblioteca `re`](https://docs.python.org/3/library/re.html) para realizar estas tareas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: <p>Hello    World! [Spoiler] The villain is the butler! [End of spoiler] <br> See you .</p>\n",
      "Cleaned: Hello World! The villain is the butler! See you.\n"
     ]
    }
   ],
   "source": [
    "def strip_html_tags(text):\n",
    "    \"\"\"Elimina etiquetas HTML\"\"\"\n",
    "    pattern = r\"<.*?>\"\n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    \"\"\"Elimina texto entre corchetes cuadrados (ej: [Spoiler], [Citation needed])\"\"\"\n",
    "    pattern = r\"\\[[^\\]]*\\]\"\n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "def remove_special_characters(text, keep_punctuation=True):\n",
    "    \"\"\"\n",
    "    Elimina caracteres especiales\n",
    "    \n",
    "    Args:\n",
    "        text: texto a limpiar\n",
    "        keep_punctuation: si True, conserva puntuación básica (.,!?;:)\n",
    "    \"\"\"\n",
    "    if keep_punctuation:\n",
    "        # Conservamos letras, números, espacios y puntuación común\n",
    "        pattern = r\"[^a-zA-Z0-9\\s.,!?;:\\-']\"\n",
    "    else:\n",
    "        # Solo letras, números y espacios\n",
    "        pattern = r\"[^a-zA-Z0-9\\s]\"\n",
    "    \n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "def remove_additional_whitespace(text):\n",
    "    \"\"\"Elimina espacios en blanco múltiples y espacios alrededor de puntuación\"\"\"\n",
    "    # Primero eliminamos espacios múltiples\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    # Eliminamos espacios antes de puntuación\n",
    "    text = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", text)\n",
    "    # Aseguramos un espacio después de puntuación (si no hay ya)\n",
    "    text = re.sub(r\"([.,!?;:])([^\\s])\", r\"\\1 \\2\", text)\n",
    "    # Eliminamos espacios al inicio y final\n",
    "    return text.strip()\n",
    "\n",
    "def low_level_text_cleaning(text, keep_punctuation=True):\n",
    "    \"\"\"\n",
    "    Limpieza completa de texto\n",
    "    \n",
    "    Args:\n",
    "        text: texto a limpiar\n",
    "        keep_punctuation: si True, conserva puntuación (recomendado para NLP)\n",
    "    \"\"\"\n",
    "    text = strip_html_tags(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_special_characters(text, keep_punctuation)\n",
    "    text = remove_additional_whitespace(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Texto de prueba\n",
    "testing_text = \"<p>Hello    World! [Spoiler] The villain is the butler! [End of spoiler] <br> See you .</p>\"\n",
    "print(f\"Original: {testing_text}\")\n",
    "print(f\"Cleaned: {low_level_text_cleaning(testing_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la limpieza a los conjuntos de entrenamiento y prueba\n",
    "train_df[\"review_low_level_cleaned\"] = train_df[\"review\"].apply(low_level_text_cleaning)\n",
    "test_df[\"review_low_level_cleaned\"] = test_df[\"review\"].apply(low_level_text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza del Texto de Alto Nivel\n",
    "\n",
    "En esta etapa, convertiremos el texto a minúsculas y eliminaremos las palabras vacías (stop words) del texto. Las palabras vacías son palabras comunes que no aportan información significativa al texto, como \"a\", \"the\", \"is\", etc. Utilizaremos la biblioteca `nltk` para descargar la lista de palabras vacías y eliminarlas del texto.\n",
    "\n",
    "> **Nota**: Dependiendo de la tarea y el dominio, es posible que desee personalizar la lista de palabras vacías para adaptarla a sus necesidades.\n",
    "\n",
    "> **Nota 2**: A veces descargar el paquete de stopwords puede fallar debido a `429: Too Many Requests`. Si esto ocurre, hay que descargar el paquete manualmente `python -m nltk.downloader stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords: {'d', \"she'd\", 'our', 'here', 'below', 'this', \"i've\", 'those', \"wouldn't\", 'from', 'some', \"we're\", \"we've\", 'do', \"won't\", 'only', \"you'd\", 'an', 'the', \"haven't\", 'during', 'themselves', 'haven', 'for', 'to', 'all', 'does', \"i'd\", \"shouldn't\", 'than', 'ours', 'having', \"that'll\", 'yourself', 'ain', \"shan't\", 'now', \"mightn't\", 'was', 'aren', 'be', 'out', 'are', 'has', 'his', 'theirs', 'were', 'how', 'into', 't', \"he'll\", 's', 'ma', 'their', 'off', \"he's\", 'against', 'mightn', 'mustn', 'on', 'myself', 'most', 'when', \"weren't\", 'between', 'again', \"we'd\", 'needn', 'no', 'too', \"it'd\", 've', 'll', 'there', 'further', 'up', 'himself', 'if', 'same', 'yours', 'both', 'itself', 'about', 'over', 'her', \"you'll\", 'should', 'a', 'its', 'by', \"they'll\", 'ourselves', 'own', 'hers', 'been', 'being', 'she', 'whom', 'shan', 'where', 'which', 'and', 'weren', \"you're\", 'at', 'you', \"hasn't\", 'why', 'while', \"i'm\", 'them', 'had', 'down', 'me', 're', \"you've\", \"should've\", \"couldn't\", 'did', 'won', 'more', 'hasn', 'nor', 'he', \"it's\", 'other', 'y', 'wasn', 'until', 'i', 'because', 'am', 'after', \"mustn't\", \"i'll\", 'isn', \"doesn't\", 'didn', 'it', 'not', \"we'll\", 'these', 'can', 'before', 'couldn', 'don', 'yourselves', 'each', 'have', 'shouldn', \"she's\", 'm', 'of', 'any', 'hadn', 'through', \"wasn't\", 'doing', 'few', 'so', 'they', 'is', 'in', 'him', 'with', 'your', 'wouldn', 'or', \"hadn't\", 'but', 'herself', \"they're\", 'under', \"don't\", 'such', 'my', 'will', 'who', \"needn't\", 'very', \"he'd\", \"they've\", 'once', 'as', \"aren't\", 'doesn', \"she'll\", 'we', 'what', 'above', \"they'd\", 'then', 'that', \"didn't\", 'just', \"isn't\", \"it'll\", 'o'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rami/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Descargamos stopwords de nltk\n",
    "nltk.download(\"stopwords\")\n",
    "all_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(f\"Stopwords: {all_stopwords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:                          The quick brown fox don't jumps over the lazy dog\n",
      "Lower Case and Stop Words Removed: quick brown fox jumps lazy dog\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_words(full_text_line):\n",
    "    # Eliminamos stopwords\n",
    "    tokens = full_text_line.split()\n",
    "    tokens = [token for token in tokens if token not in all_stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def to_lower_case(full_text_line):\n",
    "    # Convertimos a minúsculas\n",
    "    return full_text_line.lower()\n",
    "\n",
    "# Texto de prueba\n",
    "testing_text = \"The quick brown fox don't jumps over the lazy dog\"\n",
    "print(f\"Original:                          {testing_text}\")\n",
    "print(f\"Lower Case and Stop Words Removed: {remove_stop_words(to_lower_case(testing_text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lematización / Stemming\n",
    "\n",
    "- La lematización es el proceso de convertir las palabras a su forma base o lema. Por ejemplo, las palabras \"corriendo\", \"corre\" y \"corrió\" se convertirían a \"correr\". La lematización ayuda a reducir la variabilidad de las palabras y a agrupar palabras similares juntas.\n",
    "\n",
    "- El stemming es un proceso similar a la lematización, pero más simple. Consiste en eliminar los sufijos de las palabras para obtener su raíz. Por ejemplo, las palabras \"corriendo\", \"corre\" y \"corrió\" se convertirían a \"corr\". Aunque el stemming es más rápido que la lematización, a menudo produce resultados menos precisos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rami/nltk_data...\n",
      "[nltk_data] Error downloading 'punkt' from\n",
      "[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-\n",
      "[nltk_data]     pages/packages/tokenizers/punkt.zip>:   HTTP Error\n",
      "[nltk_data]     429: Too Many Requests\n",
      "[nltk_data] Downloading package punkt_tab to /home/rami/nltk_data...\n",
      "[nltk_data] Error downloading 'punkt_tab' from\n",
      "[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-\n",
      "[nltk_data]     pages/packages/tokenizers/punkt_tab.zip>:   HTTP Error\n",
      "[nltk_data]     429: Too Many Requests\n",
      "[nltk_data] Downloading package wordnet to /home/rami/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/rami/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/rami/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/rami/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# necesitamos esto para que funcione el tokenizador (nltk.word_tokenize()), de esta forma maneja mejor la puntuación\n",
    "nltk.download('punkt') \n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# necesitamos esto para que funcione el lematizador (WordNetLemmatizer)\n",
    "nltk.download('wordnet')\n",
    "# para que la salida de pos_tag sea compatible con WordNetLemmatizer\n",
    "nltk.download('universal_tagset') \n",
    "\n",
    "# necesitamos esto para que funcione el etiquetador POS (nltk.pos_tag)\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando vamos a separar el texto en tokens, no es tan sencillo cómo hacer un `split(\" \")`, ya que hay que tener en cuenta la puntuación, los signos de interrogación, etc. Para esto, utilizamos el tokenizador de NLTK `nltk.word_tokenize()`, que maneja estos casos de manera adecuada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/rami/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/share/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m test_texts = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHello, world! This is a test.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIt\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms a beautiful day, isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt it?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDr. Smith went to Washington D.C. on Jan. 5th.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe quick brown fox jumps over the lazy dog.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m ]\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m testing_text \u001b[38;5;129;01min\u001b[39;00m test_texts:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtesting_text\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# separamos en tokens\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtesting_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/rami/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/share/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"Hello, world! This is a test.\",\n",
    "    \"It's a beautiful day, isn't it?\",\n",
    "    \"Dr. Smith went to Washington D.C. on Jan. 5th.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]\n",
    "\n",
    "for testing_text in test_texts:\n",
    "    tokens = nltk.word_tokenize(testing_text) # separamos en tokens\n",
    "    print(f\"Original:   {testing_text}\")\n",
    "    print(f\"tokens: {tokens}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEXTO ORIGINAL:\n",
      "================================================================================\n",
      "The children were running faster than their friends.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/rami/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/share/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Tokenizar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# lowercase para mejor procesamiento\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# POS tagging\u001b[39;00m\n\u001b[32m     25\u001b[39m pos_tags = nltk.pos_tag(tokens, tagset=\u001b[33m'\u001b[39m\u001b[33muniversal\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/rami/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/share/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Función para convertir POS (Part of Speech) tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'): # adjetivo\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'): # verbo\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'): # sustantivo\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'): # adverbio\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # por defecto, sustantivo\n",
    "\n",
    "text = \"The children were running faster than their friends.\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEXTO ORIGINAL:\")\n",
    "print(\"=\"*80)\n",
    "print(text)\n",
    "\n",
    "# Tokenizar\n",
    "tokens = nltk.word_tokenize(text.lower())  # lowercase para mejor procesamiento\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS DETALLADO:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Original':<15} {'POS':<8} {'Lemma':<15} {'Stem':<15}\")\n",
    "print(\"-\"*80)\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "lemmatized_tokens = []\n",
    "stemmed_tokens = []\n",
    "for token, pos in pos_tags:\n",
    "    if token.isalpha():  # Solo palabras, ignorar puntuación\n",
    "        wordnet_pos = get_wordnet_pos(pos)\n",
    "        lemma = lemmatizer.lemmatize(token, wordnet_pos)\n",
    "        stem = stemmer.stem(token)\n",
    "        \n",
    "        # Mostrar solo si hay cambio\n",
    "        if token != lemma or token != stem:\n",
    "            print(f\"{token:<15} {pos:<8} {lemma:<15} {stem:<15}\")\n",
    "        \n",
    "        lemmatized_tokens.append(lemma)\n",
    "        stemmed_tokens.append(stem)\n",
    "    else:\n",
    "        lemmatized_tokens.append(token)\n",
    "        stemmed_tokens.append(token)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXTO LEMATIZADO:\")\n",
    "print(\"=\"*80)\n",
    "lemmatized_text = \" \".join(lemmatized_tokens).replace(\".\", \".\\n\")\n",
    "print(lemmatized_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXTO STEMMED:\")\n",
    "print(\"=\"*80)\n",
    "stemmed_text = \" \".join(stemmed_tokens).replace(\" .\", \".\\n\")\n",
    "print(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The children were running faster than their friends.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/rami/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/share/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLemma: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mhigh_level_text_cleaning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSteam: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhigh_level_text_cleaning_v2(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mhigh_level_text_cleaning\u001b[39m\u001b[34m(text, remove_stop_words)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remove_stop_words:\n\u001b[32m     19\u001b[39m     text = remove_stop_words(text)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m text = \u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mlemmatize\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlemmatize\u001b[39m(text):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Tokenizar el texto\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Obtener etiquetas POS\u001b[39;00m\n\u001b[32m      5\u001b[39m     pos_tags = nltk.pos_tag(tokens, tagset=\u001b[33m'\u001b[39m\u001b[33muniversal\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/rami/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/share/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(text):\n",
    "    # Tokenizar el texto\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Obtener etiquetas POS\n",
    "    pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "    # Lematizar cada token con su etiqueta POS correspondiente\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos_tag)) for token, pos_tag in pos_tags]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "def stem_words(text):\n",
    "    # Aplicamos stemming a cada palabra\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(stemmed_tokens)\n",
    "\n",
    "def high_level_text_cleaning(text, remove_stop_words=False):\n",
    "    text = to_lower_case(text)\n",
    "    if remove_stop_words:\n",
    "        text = remove_stop_words(text)\n",
    "    text = lemmatize(text)\n",
    "    return text\n",
    "\n",
    "def high_level_text_cleaning_v2(text, remove_stop_words=False):\n",
    "    text = to_lower_case(text)\n",
    "    if remove_stop_words:\n",
    "        text = remove_stop_words(text)\n",
    "    text = stem_words(text)\n",
    "    return text\n",
    "\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Lemma: {high_level_text_cleaning(text)}\")\n",
    "print(f\"Steam: {high_level_text_cleaning_v2(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_text = (\n",
    "    \"Why waste time saying a lot of words when a few words do the trick?\"\n",
    ")\n",
    "print(f\"Lemma: {high_level_text_cleaning(testing_text)}\")\n",
    "print(f\"Steam: {high_level_text_cleaning_v2(testing_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media1.tenor.com/m/IsYdPRq7bjcAAAAC/why-waste-time-when-few-word-do-trick.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kevin's Small Talk - The Office US](https://www.youtube.com/watch?v=_K-L9uhsBLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'high_level_text_cleaning_v2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Aplicamos la limpieza de alto nivel a los conjuntos de entrenamiento y prueba\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_df[\u001b[33m\"\u001b[39m\u001b[33mreview_clean\u001b[39m\u001b[33m\"\u001b[39m] = train_df[\u001b[33m\"\u001b[39m\u001b[33mreview_low_level_cleaned\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[43mhigh_level_text_cleaning_v2\u001b[49m) \u001b[38;5;66;03m# podemos usar lemmatization o stemming\u001b[39;00m\n\u001b[32m      3\u001b[39m test_df[\u001b[33m\"\u001b[39m\u001b[33mreview_clean\u001b[39m\u001b[33m\"\u001b[39m] = test_df[\u001b[33m\"\u001b[39m\u001b[33mreview_low_level_cleaned\u001b[39m\u001b[33m\"\u001b[39m].apply(high_level_text_cleaning_v2)\n",
      "\u001b[31mNameError\u001b[39m: name 'high_level_text_cleaning_v2' is not defined"
     ]
    }
   ],
   "source": [
    "# Aplicamos la limpieza de alto nivel a los conjuntos de entrenamiento y prueba\n",
    "train_df[\"review_clean\"] = train_df[\"review_low_level_cleaned\"].apply(high_level_text_cleaning_v2) # podemos usar lemmatization o stemming\n",
    "test_df[\"review_clean\"] = test_df[\"review_low_level_cleaned\"].apply(high_level_text_cleaning_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción del Vocabulario\n",
    "\n",
    "Finalmente, construiremos un vocabulario a partir de las reseñas limpias. Un vocabulario es un conjunto de todas las palabras únicas en el corpus. Cada palabra en el vocabulario se asigna a un índice único, que se utilizará para convertir el texto en una secuencia de índices numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_TOKEN = \"<OOV>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "MAX_VOCAB_SIZE = 20_000\n",
    "SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: {'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8, 'The': 9, 'Then': 10, '<OOV>': 11, '<PAD>': 0}\n"
     ]
    }
   ],
   "source": [
    "def make_vocab(all_texts, max_vocab_size, min_freq=5):\n",
    "    # Contamos la frecuencia de cada palabra\n",
    "    counts = Counter(chain(*(all_texts.str.split())))\n",
    "    counts = {word: freq for word, freq in counts.items() if freq >= min_freq}\n",
    "\n",
    "    # Ordenamos las palabras por frecuencia y nos quedamos con las max_vocab_size palabras más frecuentes\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)[:max_vocab_size]\n",
    "    vocab.append(OOV_TOKEN)  # Añadimos el token OOV al final\n",
    "\n",
    "    # Mapa de palabras a índices\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "    vocab_to_int[PAD_TOKEN] = 0  # Añadimos el token PAD al principio\n",
    "\n",
    "    return vocab_to_int\n",
    "\n",
    "# Texto de prueba\n",
    "testing_text1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "testing_text2 = \"Then the quick brown fox jumps over the lazy dog\"\n",
    "print(f\"Vocab: {make_vocab(pd.Series([testing_text1, testing_text2]), 100, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'review_clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'review_clean'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m word_to_index = make_vocab(\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreview_clean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, MAX_VOCAB_SIZE)\n\u001b[32m      2\u001b[39m VOCAB_SIZE = \u001b[38;5;28mlen\u001b[39m(word_to_index)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTamaño del vocabulario: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVOCAB_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'review_clean'"
     ]
    }
   ],
   "source": [
    "word_to_index = make_vocab(train_df[\"review_clean\"], MAX_VOCAB_SIZE)\n",
    "VOCAB_SIZE = len(word_to_index)\n",
    "print(f\"Tamaño del vocabulario: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a implementar una funcion que transforma un string con la review en una lista de enteros con la posición de las palabras en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The quick brown fox jumps <br> over the lazy dog ! ThisWordIsOOV.\n",
      "Limpieza bajo nivel: The quick brown fox jumps over the lazy dog! ThisWordIsOOV.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/rami/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/share/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m testing_text = low_level_text_cleaning(testing_text)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLimpieza bajo nivel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtesting_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m testing_text = \u001b[43mhigh_level_text_cleaning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtesting_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLimpieza alto nivel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtesting_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m testing_text = get_review_representation(testing_text, word_to_index, \u001b[32m15\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mhigh_level_text_cleaning\u001b[39m\u001b[34m(text, remove_stop_words)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remove_stop_words:\n\u001b[32m     19\u001b[39m     text = remove_stop_words(text)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m text = \u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mlemmatize\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlemmatize\u001b[39m(text):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Tokenizar el texto\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Obtener etiquetas POS\u001b[39;00m\n\u001b[32m      5\u001b[39m     pos_tags = nltk.pos_tag(tokens, tagset=\u001b[33m'\u001b[39m\u001b[33muniversal\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/rami/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/share/nltk_data'\n    - '/home/rami/.conda/envs/Taller_DL/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def get_review_features(review_text, word_to_idx):\n",
    "    \"\"\"\n",
    "    Convierte un texto en una lista de índices basados en el vocabulario.\n",
    "    \"\"\"\n",
    "    # Tokenizar el texto y convertir cada palabra a su índice correspondiente\n",
    "    return [\n",
    "        word_to_idx.get(word, word_to_idx[OOV_TOKEN]) for word in review_text.split()\n",
    "    ]\n",
    "    \n",
    "def truncate_sequence(sequence, max_length, keep='last'):\n",
    "    \"\"\"\n",
    "    Trunca la secuencia manteniendo las primeras o últimas palabras\n",
    "    \n",
    "    Args:\n",
    "        sequence: lista de tokens/índices\n",
    "        max_length: longitud máxima\n",
    "        keep: 'first' o 'last'\n",
    "    \"\"\"\n",
    "    if len(sequence) <= max_length:\n",
    "        return sequence\n",
    "    \n",
    "    if keep == 'last':\n",
    "        return sequence[-max_length:]  # Últimas palabras\n",
    "    else:\n",
    "        return sequence[:max_length]   # Primeras palabras\n",
    "\n",
    "def left_pad_features(review_ints, seq_length, pad_value=0, truncate_keep='last'):\n",
    "    \"\"\"\n",
    "    Aplica padding a la izquierda a una secuencia de índices para que todas las secuencias tengan la misma longitud.\n",
    "    \"\"\"\n",
    "    # Truncar si es más largo que seq_length\n",
    "    review_ints = truncate_sequence(review_ints, seq_length, keep=truncate_keep)\n",
    "    \n",
    "    # Padding a la izquierda\n",
    "    if len(review_ints) < seq_length:\n",
    "        padding = [pad_value] * (seq_length - len(review_ints))\n",
    "        return padding + review_ints\n",
    "    else:\n",
    "        return review_ints\n",
    "\n",
    "def get_review_representation(review_text, word_to_idx, max_sequence_length):\n",
    "    \"\"\"\n",
    "    Convierte el texto de entrada en una representación de secuencia con padding a la izquierda.\n",
    "    \"\"\"\n",
    "    review_ints = get_review_features(review_text, word_to_idx)\n",
    "    return left_pad_features(review_ints, max_sequence_length)\n",
    "\n",
    "# Texto de prueba\n",
    "testing_text = \"The quick brown fox jumps <br> over the lazy dog ! ThisWordIsOOV.\"\n",
    "print(f\"Original: {testing_text}\")\n",
    "testing_text = low_level_text_cleaning(testing_text)\n",
    "print(f\"Limpieza bajo nivel: {testing_text}\")\n",
    "testing_text = high_level_text_cleaning(testing_text)\n",
    "print(f\"Limpieza alto nivel: {testing_text}\")\n",
    "testing_text = get_review_representation(testing_text, word_to_index, 15)\n",
    "print(f\"Features: {testing_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y DataLoader\n",
    "\n",
    "Nuestro dataset `IMDBDataset` tomará el DataFrame de Pandas con las **reseñas preprocesadas** y el vocabulario, y devolverá una secuencia de índices numéricos para cada reseña. Luego, utilizaremos un DataLoader para cargar los datos en lotes y alimentarlos a nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, vocab, max_sequence_length):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Obtener texto y etiqueta\n",
    "        text = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convertir texto a representación con padding\n",
    "        indices = get_review_representation(text, self.vocab, self.max_sequence_length)\n",
    "\n",
    "        return torch.tensor(indices, dtype=torch.int32), torch.tensor(\n",
    "            [label], dtype=torch.float32\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'review_clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'review_clean'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Crear el dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m train_dataset = IMDBDataset(\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreview_clean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[32m      4\u001b[39m     train_df[\u001b[33m\"\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      5\u001b[39m     word_to_index,\n\u001b[32m      6\u001b[39m     max_sequence_length=SEQUENCE_LENGTH,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m test_dataset = IMDBDataset(\n\u001b[32m      9\u001b[39m     test_df[\u001b[33m\"\u001b[39m\u001b[33mreview_clean\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     10\u001b[39m     test_df[\u001b[33m\"\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     11\u001b[39m     word_to_index,\n\u001b[32m     12\u001b[39m     max_sequence_length=SEQUENCE_LENGTH,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Separar el conjunto de entrenamiento en subconjuntos de entrenamiento y validación\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Taller_DL/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'review_clean'"
     ]
    }
   ],
   "source": [
    "# Crear el dataset\n",
    "train_dataset = IMDBDataset(\n",
    "    train_df[\"review_clean\"],\n",
    "    train_df[\"sentiment\"],\n",
    "    word_to_index,\n",
    "    max_sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "test_dataset = IMDBDataset(\n",
    "    test_df[\"review_clean\"],\n",
    "    test_df[\"sentiment\"],\n",
    "    word_to_index,\n",
    "    max_sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "# Separar el conjunto de entrenamiento en subconjuntos de entrenamiento y validación\n",
    "train_len = len(train_dataset)\n",
    "val_len = int(0.2 * train_len)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_len - val_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Modelos sin RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo base\n",
    "\n",
    "Para capturar la semántica de las reseñas necesitamos tomar nuestro texto (ya convertido a índices) y convertirlo en un vector de características. Para esto utilizamos una capa de embedding que mapea cada índice a un vector de características. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Embedding\n",
    "\n",
    "La [capa de embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) en PyTorch es una capa lineal que mapea un índice a un vector de características. Por ejemplo, si nuestro vocabulario tiene 10,000 palabras y estamos utilizando un embedding de tamaño 100, la capa de embedding tendrá una matriz de pesos de tamaño 10,000 x 100. Dado un índice de palabra, la capa de embedding devuelve la fila correspondiente de la matriz de pesos, que es el vector de características de la palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, padding_idx=word_to_index[PAD_TOKEN]\n",
    ")\n",
    "word_indices = torch.tensor([0, 1, 2, 3, 4, 5])\n",
    "embedding_layer(word_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con otras capas en PyTorch, la capa de embedding se inicializa con pesos aleatorios y se ajusta durante el entrenamiento.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Parámetros} = \\text{Tamaño del Vocabulario} \\times \\text{Tamaño del Embedding} + \\text{Tamaño del Embedding}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura\n",
    "\n",
    "Para clasificar las reseñas de IMDB, utilizaremos una arquitectura de modelo simple con las siguientes capas:\n",
    "\n",
    "1. **Capa de Embedding**: Mapea cada índice de palabra a un vector de características.\n",
    "2. **Capa de Promedio**: Calcula el promedio de los vectores de características de todas las palabras en una reseña.\n",
    "3. **Capa Lineal Oculta**: Transforma el vector de características promedio en un vector de características de tamaño oculto.\n",
    "4. **Capa de Salida**: Produce la salida final, que es la probabilidad de que la reseña sea positiva o negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VOCAB_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     22\u001b[39m         x = F.sigmoid(\u001b[38;5;28mself\u001b[39m.out(x))\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m     25\u001b[39m summary(\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     SentimentModel(\u001b[43mVOCAB_SIZE\u001b[49m, EMBEDDING_DIM, \u001b[32m512\u001b[39m),\n\u001b[32m     27\u001b[39m     input_size=(BATCH_SIZE, SEQUENCE_LENGTH),\n\u001b[32m     28\u001b[39m     dtypes=[torch.int32],\n\u001b[32m     29\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'VOCAB_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=word_to_index[PAD_TOKEN]\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [BATCH_SIZE, SEQUENCE_LENGTH]\n",
    "        x = self.embed(x)\n",
    "        # x: [BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_DIM]\n",
    "        x = x.mean(dim=1)\n",
    "        # x: [BATCH_SIZE, EMBEDDING_DIM]\n",
    "        x = self.relu(x)\n",
    "        # x: [BATCH_SIZE, HIDDEN_DIM]\n",
    "        x = self.fc(x)\n",
    "        #\n",
    "        x = self.relu(x)\n",
    "        x = F.sigmoid(self.out(x))\n",
    "        return x\n",
    "\n",
    "summary(\n",
    "    SentimentModel(VOCAB_SIZE, EMBEDDING_DIM, 512),\n",
    "    input_size=(BATCH_SIZE, SEQUENCE_LENGTH),\n",
    "    dtypes=[torch.int32],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITERION = nn.BCELoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = SentimentModel(\n",
    "    vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, hidden_dim=512\n",
    ").to(DEVICE)\n",
    "base_optimizer = optim.Adam(base_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = train(\n",
    "    base_model,\n",
    "    optimizer=base_optimizer,\n",
    "    criterion=CRITERION,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    do_early_stopping=True,\n",
    "    patience=3,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = torch.where(model(x) > 0.5, 1, 0)\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(out.cpu().numpy())\n",
    "        print(f\"Accuracy: {np.mean(np.array(y_true) == np.array(y_pred)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy(base_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "- Explorar otras técnicas de preprocesamiento de texto, como stemming, eliminación de números, etc.\n",
    "- Explorar con los hiperparámetros del modelo, como el tamaño del embedding, el tamaño de la capa oculta, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: Modelos con RNNs\n",
    "\n",
    "Las [redes neuronales recurrentes (RNNs)](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html) son una clase de redes neuronales diseñadas para manejar datos secuenciales. A diferencia de las redes neuronales convolucionales (CNNs), que son eficaces para procesar datos espaciales, como imágenes, las RNNs son ideales para modelar datos secuenciales, como texto, audio y series temporales.\n",
    "\n",
    "En este caso tendremos una arquitectura many-to-one, donde la entrada es una secuencia de palabras y la salida es una sola etiqueta de clase (positiva o negativa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs\n",
    "\n",
    "### nn.RNN\n",
    "\n",
    "La capa [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) en PyTorch es una capa recurrente que procesa una secuencia de entrada paso a paso, manteniendo un estado oculto que captura la información de pasos anteriores. Dado un tensor de entrada de tamaño `(batch, secuencia, características)`, la capa RNN procesa la secuencia paso a paso y devuelve el estado oculto final para cada secuencia en el lote.\n",
    "\n",
    "<!-- ![rnn](https://d2l.ai/_images/rnn.svg) -->\n",
    "<img src=\"https://d2l.ai/_images/rnn.svg\" width=\"500\" style=\"background:white; display: block; margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=EMBEDDING_DIM, hidden_size=32, num_layers=1, batch_first=True)\n",
    "tensor = torch.randn(BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "output, hidden = rnn(tensor)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Hidden shape: {hidden.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=2, dropout=0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=word_to_index[PAD_TOKEN]\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim*2)\n",
    "        self.out = nn.Linear(hidden_dim*2, 1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [BATCH_SIZE, SEQUENCE_LENGTH]\n",
    "        x = self.embed(x)\n",
    "        # x: [BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_DIM]\n",
    "        x = self.drop(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        # x: [BATCH_SIZE, SEQUENCE_LENGTH, HIDDEN_DIM]\n",
    "        x = x[:, -1, :]\n",
    "        # x: [BATCH_SIZE, HIDDEN_DIM]\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        # x: [BATCH_SIZE, HIDDEN_DIM*2]\n",
    "        x = F.sigmoid(self.out(x))\n",
    "        return x\n",
    "\n",
    "summary(\n",
    "    SentimentRNN(VOCAB_SIZE, EMBEDDING_DIM, 128),\n",
    "    input_size=(BATCH_SIZE, SEQUENCE_LENGTH),\n",
    "    dtypes=[torch.int32],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = SentimentRNN(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=128,\n",
    "    n_layers=2,\n",
    "    dropout=0.5,\n",
    ").to(DEVICE)\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = train(\n",
    "    rnn_model,\n",
    "    optimizer=rnn_optimizer,\n",
    "    criterion=CRITERION,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    do_early_stopping=False,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy(rnn_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs\n",
    "\n",
    "Las [redes LSTM (Long Short-Term Memory)](https://d2l.ai/chapter_recurrent-modern/lstm.html) son una variante de las RNNs que están diseñadas para manejar el problema del desvanecimiento del gradiente. Las LSTMs utilizan una estructura de celda más compleja que permite que el gradiente fluya sin desvanecerse o explotar, lo que las hace más efectivas para modelar secuencias a largo plazo.\n",
    "\n",
    "<img src=\"https://d2l.ai/_images/lstm-3.svg\" width=\"500\" style=\"background:white; display: block; margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=EMBEDDING_DIM, hidden_size=32, num_layers=1, batch_first=True)\n",
    "tensor = torch.randn(BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "output, (hidden, cell) = lstm(tensor)\n",
    "print(f\"Output shape: {output.shape} (batch_size, seq_length, hidden_size)\")\n",
    "print(f\"Hidden shape: {hidden.shape} (num_layers, batch_size, hidden_size)\")\n",
    "print(f\"Cell shape: {cell.shape} (num_layers, batch_size, hidden_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim, hidden_dim, n_layers=2, dropout=0.5\n",
    "    ):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=word_to_index[PAD_TOKEN]\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text: [BATCH_SIZE, SEQUENCE_LENGTH]\n",
    "        pass # use sigmoid activation for the output\n",
    "\n",
    "summary(\n",
    "    SentimentLSTM(VOCAB_SIZE, EMBEDDING_DIM, 128),\n",
    "    input_size=(BATCH_SIZE, SEQUENCE_LENGTH),\n",
    "    dtypes=[torch.int32],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = SentimentLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=128,\n",
    "    n_layers=4,\n",
    "    dropout=0.5,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "_, _ = train(\n",
    "    lstm_model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=CRITERION,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    do_early_stopping=False,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy(lstm_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejericios\n",
    "\n",
    "- Explorar otras arquitecturas de RNNs, como [GRUs](https://d2l.ai/chapter_recurrent-modern/gru.html).\n",
    "- Experimentar con diferentes hiperparámetros, como el tamaño de la capa oculta, la tasa de aprendizaje, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Taller_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
