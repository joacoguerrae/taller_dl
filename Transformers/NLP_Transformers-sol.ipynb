{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers: Arquitectura de Atención para Traducción Automática\n",
    "\n",
    "En esta notebook, exploraremos la implementación de un modelo **Transformer** desde cero utilizando PyTorch. Esta arquitectura revolucionaria, introducida en el paper \"Attention is All You Need\" (Vaswani et al., 2017), marcó un antes y un después en el campo del Procesamiento de Lenguaje Natural (NLP), siendo la base de modelos modernos como BERT y GPT.\n",
    "\n",
    "## Introducción\n",
    "\n",
    "### Objetivos\n",
    "\n",
    "1. **Comprender la arquitectura Transformer** y cómo funciona el mecanismo de atención (self-attention) para procesar secuencias de texto.\n",
    "2. **Implementar desde cero los componentes clave** del Transformer: Positional Encoding, Multi-Head Attention, Encoder, y Decoder.\n",
    "3. **Entrenar un modelo de traducción automática** inglés-español utilizando la arquitectura Transformer completa.\n",
    "4. **Explorar el uso de máscaras** para el entrenamiento autorregresivo y el manejo de padding en secuencias de longitud variable.\n",
    "\n",
    "### Contenido\n",
    "\n",
    "1. Introducción a la arquitectura Transformer y su relevancia en NLP moderno.\n",
    "2. Preparación de datos y construcción de vocabularios para traducción automática.\n",
    "3. Implementación del **Positional Encoding** para capturar información de posición en las secuencias.\n",
    "4. Desarrollo del mecanismo de **Multi-Head Attention** y comprensión del scaled dot-product attention.\n",
    "5. Construcción de las capas del **Encoder** y **Decoder** con sus componentes: self-attention, encoder-decoder attention, y feed-forward networks.\n",
    "6. Implementación de **máscaras** para padding y look-ahead masking en el decoder.\n",
    "7. Entrenamiento del modelo Transformer completo y evaluación en traducción de frases.\n",
    "\n",
    "### Concepto de Transformer\n",
    "\n",
    "La arquitectura Transformer revolucionó el procesamiento de secuencias al **eliminar completamente las redes recurrentes** (RNNs y LSTMs) y basarse únicamente en mecanismos de atención. A diferencia de las arquitecturas secuenciales tradicionales, el Transformer puede procesar todos los elementos de una secuencia en paralelo, lo que mejora significativamente la eficiencia del entrenamiento.\n",
    "\n",
    "**Componentes principales:**\n",
    "\n",
    "- **Encoder**: Procesa la secuencia de entrada mediante capas de self-attention y redes feed-forward, generando representaciones contextualizadas de cada token.\n",
    "- **Decoder**: Genera la secuencia de salida de forma autorregresiva, utilizando tanto self-attention sobre los tokens ya generados como atención cruzada (cross-attention) sobre la salida del encoder.\n",
    "- **Multi-Head Attention**: Permite al modelo atender a diferentes representaciones y posiciones de la secuencia simultáneamente, capturando relaciones complejas entre palabras.\n",
    "- **Positional Encoding**: Como el Transformer no tiene noción inherente del orden de las palabras, se añade información posicional mediante funciones seno y coseno.\n",
    "\n",
    "Esta arquitectura se ha convertido en el estándar de facto para tareas de NLP, siendo la base de los modelos de lenguaje más avanzados de la actualidad.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://d1.awsstatic.com/GENAI-1.151ded5440b4c997bac0642ec669a00acff2cca1.png\" width=\"600px\">\n",
    "</div>\n",
    "\n",
    "### Dataset de Traducción\n",
    "\n",
    "Para esta notebook, utilizaremos el dataset de traducción inglés-español de Tatoeba, que contiene pares de frases en ambos idiomas. El objetivo es entrenar un modelo Transformer que aprenda a traducir frases del inglés al español. El dataset incluye frases cortas y medianas de diversos contextos cotidianos, lo que permite al modelo aprender patrones lingüísticos variados.\n",
    "\n",
    "El dataset está disponible en: [Tatoeba Downloads](https://tatoeba.org/en/downloads)\n",
    "\n",
    "### Referencias\n",
    "\n",
    "- [Attention is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al. (2017)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Jay Alammar <- Encoder + Decoder\n",
    "- [Transformer Explainer](https://poloclub.github.io/transformer-explainer/) - Visualización interactiva <- Decoder only\n",
    "- [LLM Visualization](https://bbycroft.net/llm) - Brendan Bycroft <- Decoder only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la semilla para que los resultados sean reproducibles\n",
    "SEED = 23\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando cuda\n",
      "Usando 4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "elif torch.xpu.is_available():\n",
    "    DEVICE = \"xpu\"  # si no hay GPU, pero hay XPU, usamos XPU\n",
    "\n",
    "print(f\"Usando {DEVICE}\")\n",
    "\n",
    "NUM_WORKERS = 0  # Win y MacOS pueden tener problemas con múltiples workers\n",
    "if sys.platform == \"linux\":\n",
    "    NUM_WORKERS = 4  # numero de workers para cargar los datos (depende de cada caso)\n",
    "\n",
    "print(f\"Usando {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar los Datos & Preprocesamiento\n",
    "\n",
    "Vamos a leer el dataset de traducción y realizar un preprocesamiento básico para limpiar y normalizar los textos antes de alimentarlos al modelo. Por ejemplo, vamos a convertir los textos a minúsculas, eliminar algunos caracteres especiales y filtrar las oraciones más largas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convertimos a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Insertamos espacios alrededor de los símbolos de puntuación que queremos conservar\n",
    "    text = re.sub(r\"([¿?¡!,])\", r\" \\1 \", text)\n",
    "\n",
    "    # Eliminamos todo lo que no sea letras, números, o los símbolos que queremos conservar\n",
    "    text = re.sub(r\"[^a-zA-Z0-9áéíóúüñ¿?¡!,]+\", \" \", text)\n",
    "\n",
    "    # Remover espacios extras\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = str(Path(\"data\") / \"English-Spanish.tsv\")\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 15  # Máxima longitud de las frases que vamos a considerar\n",
    "\n",
    "\n",
    "def load_data(source_file, max_words=5):\n",
    "    with open(source_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Separamos las frases en dos listas\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "\n",
    "    for line in lines:\n",
    "        elements = line.split(\"\\t\")\n",
    "\n",
    "        input_text = elements[1]\n",
    "        target_text = elements[3]\n",
    "\n",
    "        input_text_clean = clean_text(input_text)\n",
    "        target_text_clean = clean_text(target_text)\n",
    "\n",
    "        # Filtramos frases de hasta max_words palabras\n",
    "        if (\n",
    "            len(input_text_clean.split()) <= max_words\n",
    "            and len(target_text_clean.split()) <= max_words\n",
    "        ):\n",
    "            input_texts.append(input_text_clean)\n",
    "            target_texts.append(target_text_clean)\n",
    "\n",
    "    return input_texts, target_texts\n",
    "\n",
    "\n",
    "src_texts, trg_texts = load_data(DATA_PATH, MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 264266\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples: {len(src_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: as usual , the physics teacher was late for class\n",
      "Target: como de costumbre , el profesor de física llegó tarde a clase\n",
      "\n",
      "Input: nobody seems to know where jean is\n",
      "Target: nadie parece saber dónde está jean\n",
      "\n",
      "Input: how many pupils are there in your school ?\n",
      "Target: ¿ cuántos alumnos hay en tu escuela ?\n",
      "\n",
      "Input: he died at the age of 54\n",
      "Target: murió a la edad de 54 años\n",
      "\n",
      "Input: i have a lot of things to do this morning\n",
      "Target: tengo muchas cosas que hacer esta mañana\n",
      "\n",
      "Input: it s about to rain\n",
      "Target: está a punto de llover\n",
      "\n",
      "Input: i won t die\n",
      "Target: no moriré\n",
      "\n",
      "Input: they are about the same age\n",
      "Target: ellos tienen más o menos la misma edad\n",
      "\n",
      "Input: tom gave me a pen\n",
      "Target: tom me dio un bolígrafo\n",
      "\n",
      "Input: the tea we had there was excellent\n",
      "Target: el té que tomamos allí era excelente\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_idx = np.random.randint(0, len(src_texts), 10)\n",
    "for idx in random_idx:\n",
    "    print(f\"Input: {src_texts[idx]}\")\n",
    "    print(f\"Target: {trg_texts[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de los Vocabularios\n",
    "\n",
    "Es importante construir un vocabulario para cada idioma en el dataset, ya que cada vocabulario tiene que ser capaz de mapear palabras a índices enteros y viceversa.\n",
    "\n",
    "> Nota: para reducir el tiempo de entrenamiento, vamos a limitar el tamaño del vocabulario a las palabras más comunes en cada idioma, con el argumento `FREQ_THRESHOLD` controlamos la cantidad de palabras que se incluirán en el vocabulario.\n",
    "\n",
    "Tenemos además que agregar token especiales:\n",
    "\n",
    "- `SOS` (Start of Sentence): Indica el inicio de una oración.\n",
    "- `EOS` (End of Sentence): Indica el final de una oración.\n",
    "- `UNK` (Unknown): Indica una palabra desconocida que no está en el vocabulario.\n",
    "- `PAD` (Padding): Se utiliza para rellenar secuencias a la misma longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 25033\n",
      "Spanish vocab size: 45139\n"
     ]
    }
   ],
   "source": [
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "FREQ_THRESHOLD = 1  # Frecuencia mínima para considerar una palabra en el vocabulario\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        # mapea palabras a índices\n",
    "        self.word2index = {}\n",
    "        # mapea índices a palabras\n",
    "        self.index2word = {}\n",
    "        # contador de palabras\n",
    "        self.word_count = Counter()\n",
    "        self.index = 0\n",
    "\n",
    "        # Tokens especiales\n",
    "        self.add_special_tokens()\n",
    "\n",
    "    def add_special_tokens(self):\n",
    "        self.add_word(PAD_TOKEN)\n",
    "        self.add_word(SOS_TOKEN)\n",
    "        self.add_word(EOS_TOKEN)\n",
    "        self.add_word(UNK_TOKEN)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.index\n",
    "            self.index2word[self.index] = word\n",
    "            self.index += 1\n",
    "\n",
    "    def build_vocab(self, sentences, min_freq=1):\n",
    "        word_counter = Counter()\n",
    "        for sentence in sentences:\n",
    "            for word in sentence.split():\n",
    "                word_counter[word] += 1\n",
    "\n",
    "        # Filtrar palabras que no alcanzan la frecuencia mínima\n",
    "        words = [word for word, count in word_counter.items() if count >= min_freq]\n",
    "\n",
    "        # Agregar palabras filtradas al vocabulario\n",
    "        for word in words:\n",
    "            self.add_word(word)\n",
    "            self.word_count[word] = word_counter[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2index)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            return self.index2word.get(key, UNK_TOKEN)\n",
    "        if isinstance(key, str):\n",
    "            return self.word2index.get(key, self.word2index[UNK_TOKEN])\n",
    "\n",
    "\n",
    "# Construimos los vocabularios\n",
    "SRC_VOCAB = Vocab()\n",
    "TRG_VOCAB = Vocab()\n",
    "\n",
    "SRC_VOCAB.build_vocab(src_texts, min_freq=FREQ_THRESHOLD)\n",
    "TRG_VOCAB.build_vocab(trg_texts, min_freq=FREQ_THRESHOLD)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(SRC_VOCAB)\n",
    "TRG_VOCAB_SIZE = len(TRG_VOCAB)\n",
    "\n",
    "print(f\"English vocab size: {SRC_VOCAB_SIZE}\")\n",
    "print(f\"Spanish vocab size: {TRG_VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos ejemplos de uso de los vocabularios:\n",
    "\n",
    "- `SRC_VOCAB['hello']`: Devuelve el índice de la palabra \"hello\" en el vocabulario de origen.\n",
    "- `TGT_VOCAB['hola']`: Devuelve el índice de la palabra \"hola\" en el vocabulario de destino.\n",
    "- `TRG_VOCAB['palabra_no_existente']`: Devuelve el índice de la palabra desconocida (`<UNK>`) en el vocabulario de destino.\n",
    "- `TRG_VOCAB[10]`: Devuelve la palabra en el índice 10 del vocabulario de destino.\n",
    "- `TRG_VOCAB[3]`: Devuelve la palabra en el índice 3 del vocabulario de destino.\n",
    "- `SRC_VOCAB[PAD_TOKEN]`: Devuelve el índice del token de padding en el vocabulario de origen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979\n",
      "1308\n",
      "3\n",
      "irme\n",
      "<UNK>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(SRC_VOCAB[\"hello\"])\n",
    "print(TRG_VOCAB[\"hola\"])\n",
    "print(TRG_VOCAB[\"palabra_no_existente\"])\n",
    "print(TRG_VOCAB[10])\n",
    "print(TRG_VOCAB[3])\n",
    "print(TRG_VOCAB[PAD_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[979, 59]\n",
      "[1308, 63, 26196]\n"
     ]
    }
   ],
   "source": [
    "# Función para codificar una frase\n",
    "def encode_sentence(sentence, vocab):\n",
    "    return [vocab[word] for word in sentence.split()]\n",
    "\n",
    "\n",
    "print(encode_sentence(\"hello world\", SRC_VOCAB))\n",
    "print(encode_sentence(\"hola mundo extraterrestre\", TRG_VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irme a dormir\n"
     ]
    }
   ],
   "source": [
    "def decode_sentence(indices, vocab):\n",
    "    return \" \".join(\n",
    "        [\n",
    "            vocab[idx]\n",
    "            for idx in indices\n",
    "            if idx != vocab[PAD_TOKEN]\n",
    "            and idx != vocab[EOS_TOKEN]\n",
    "            and idx != vocab[SOS_TOKEN]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "print(decode_sentence([10, 11, 12], TRG_VOCAB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset de Traducción\n",
    "\n",
    "Trabajaremos con un pequeño dataset de traducción **inglés-español**, compuesto por pares de frases simples. El objetivo es que el modelo aprenda a traducir una frase en inglés a su equivalente en español.\n",
    "\n",
    "#### Ejemplos de Pares:\n",
    "\n",
    "- **Inglés**: \"hello\" → **Español**: \"hola\"\n",
    "- **Inglés**: \"how are you?\" → **Español**: \"¿cómo estás?\"\n",
    "\n",
    "### Preparación del Dataset\n",
    "\n",
    "Cada frase será tokenizada y convertida a índices numéricos de sus respectivos vocabularios. En las secuencias objetivo, añadimos los tokens especiales `<SOS>` y `<EOS>` para marcar el inicio y el fin de cada traducción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences):\n",
    "        super(TranslationDataset, self).__init__()\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sentence = self.source_sentences[idx]\n",
    "        target_sentence = self.target_sentences[idx]\n",
    "        encoded_source_sentence = encode_sentence(source_sentence, SRC_VOCAB)\n",
    "        encoded_target_sentence = encode_sentence(target_sentence, TRG_VOCAB)\n",
    "\n",
    "        # Añadimos tokens especiales <SOS> y <EOS>\n",
    "        encoded_target_sentence = (\n",
    "            [TRG_VOCAB[SOS_TOKEN]] + encoded_target_sentence + [TRG_VOCAB[EOS_TOKEN]]\n",
    "        )\n",
    "\n",
    "        x = torch.tensor(encoded_source_sentence, dtype=torch.long)\n",
    "        y = torch.tensor(encoded_target_sentence, dtype=torch.long)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Crear el dataset y el dataloader\n",
    "train_dataset = TranslationDataset(src_texts, trg_texts)\n",
    "val_len = int(0.10 * len(train_dataset))\n",
    "train_len = len(train_dataset) - val_len\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que las frases tienen longitudes variables, utilizaremos padding para asegurarnos de que todas las secuencias en un batch tengan la misma longitud.\n",
    "\n",
    "La función `collate_fn` es un argumento opcional que se pasa al DataLoader de PyTorch para personalizar el procesamiento de los datos. En este caso, se utiliza para rellenar y agrupar las secuencias de entrada y salida en lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sources = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    # Padding de las secuencias\n",
    "    sources_padded = pad_sequence(\n",
    "        sources,\n",
    "        padding_value=SRC_VOCAB[PAD_TOKEN],\n",
    "        batch_first=True,\n",
    "        padding_side=\"right\",\n",
    "    )\n",
    "    targets_padded = pad_sequence(\n",
    "        targets,\n",
    "        padding_value=TRG_VOCAB[PAD_TOKEN],\n",
    "        batch_first=True,\n",
    "        padding_side=\"right\",\n",
    "    )\n",
    "\n",
    "    return sources_padded, targets_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128  # 128 o menos para clase\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo\n",
    "\n",
    "> **Encoder**: The encoder is composed of a stack of $N = 6$ identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is $LayerNorm(x + Sublayer(x))$, where $Sublayer(x)$ is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model} = 512$.\n",
    ">\n",
    "> **Decoder**: The decoder is also composed of a stack of $N = 6$ identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position,ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.\n",
    "\n",
    "Paper original: [Attention is All You Need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"https://d1.awsstatic.com/GENAI-1.151ded5440b4c997bac0642ec669a00acff2cca1.png\" width=\"400px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparametros para el modelo\n",
    "Modificar para que el modelo sea \"ejecutable\" dentro de un tiempo razonable de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del modelo\n",
    "SRC_PAD_IDX = SRC_VOCAB[PAD_TOKEN]\n",
    "TRG_PAD_IDX = TRG_VOCAB[PAD_TOKEN]\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 6  # clase usar 4 o 2\n",
    "FORWARD_EXPANSION = 4\n",
    "HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "MAX_SENTENCE_LENGTH_MODEL = (\n",
    "    MAX_SENTENCE_LENGTH + 2\n",
    ")  # tenemos en cuenta los tokens SOS y EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://kazemnejad.com/img/transformer_architecture_positional_encoding/model_arc.jpg\" width=\"1200\"/>\n",
    "</div>\n",
    "\n",
    "> Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed. In this work, we use sine and cosine functions of different frequencies:\n",
    "> $$\n",
    "> \\begin{aligned}\n",
    "> \\text{PE}_{(pos,2i)} &= \\sin(pos / 10000^{2i/d_{\\text{model}}}) \\\\\n",
    "> \\text{PE}_{(pos,2i+1)} &= \\cos(pos / 10000^{2i/d_{\\text{model}}})\n",
    "> \\end{aligned}\n",
    "> $$\n",
    "> where $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.\n",
    "\n",
    "> Nota: $d_{model}$ = embedding dimension\n",
    "\n",
    "Esta **forma de codificación posicional** permite:\n",
    "- Dada una posición, obtener una representación única para esa posición (depende de la frecuencia y dimensión).\n",
    "- La distancia entre dos posiciones es consistente sin importar la longitud de la secuencia.\n",
    "- El modelo puede generalizar a secuencias más largas sin necesidad de reentrenamiento.\n",
    "- Su calculo es determinista.\n",
    "\n",
    "**¿Por qué usar funciones seno y coseno?** Permite al model capturar relaciones de distancia entre posiciones mediante combinaciones lineales. Ver [Linear Relationships in the Transformer’s Positional Encoding](https://blog.timodenk.com/linear-relationships-in-the-transformers-positional-encoding/)\n",
    "\n",
    "**¿Por qué 10000?** Es un hiperparámetro que define la escala de las frecuencias utilizadas en las funciones seno y coseno. Si fuese muy pequeño, dos posiciones diferentes podrían tener representaciones muy similares, si fuese muy grande, las diferencias entre posiciones cercanas podrían volverse insignificantes. \"The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.\" -> la primera dimensión (i=0) tendrá una frecuencia alta (ciclos rápidos), cada aproximadamente 6.57 ($2\\pi$), mientras que las últimas dimensiones tendrán frecuencias mucho más bajas (ciclos lentos), permitiendo capturar patrones a largo plazo.\n",
    "\n",
    "**¿Por qué se suman y no concatenan?** \n",
    "- La suma mantiene la dimensionalidad constante (baja la complejidad computacional).\n",
    "- Empiricamente, sumar embeddings y codificaciones posicionales ha demostrado ser efectivo. Ver [Rethinking Positional Encoding in Language Pre-training](https://arxiv.org/abs/2006.15595)\n",
    "\n",
    "Links útiles:\n",
    "- [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "- [\\[video\\] ¿Por qué estas REDES NEURONALES son tan POTENTES? ](https://www.youtube.com/watch?v=xi94v_jl26U)\n",
    "- [\\[video\\] How do Transformer Models keep track of the order of words? Positional Encoding](https://www.youtube.com/watch?v=IHu3QehUmrQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "PositionalEncoding                       --\n",
       "=================================================================\n",
       "Total params: 0\n",
       "Trainable params: 0\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        assert d_model % 2 == 0, \"d_model debe ser par para usar sinusoides\"\n",
    "\n",
    "        self.max_len = max_len\n",
    "\n",
    "        pe = torch.zeros((max_len, d_model))\n",
    "        # pe: [max_len, d_model]\n",
    "        pos = torch.arange(0, max_len, 1)\n",
    "        # pos: [max_len]\n",
    "        pos = pos.unsqueeze(1)\n",
    "        # pos: [max_len, 1]\n",
    "\n",
    "        two_i = torch.arange(0, d_model, 2)  # [0, 2, 4, ... d_model]\n",
    "        # two_i: [d_model / 2]\n",
    "        div = 10_000 ** (two_i / d_model)\n",
    "        # div [d_model / 2]\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(pos / div)\n",
    "        pe[:, 1::2] = torch.cos(pos / div)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # pe[1, max_len, d_model]\n",
    "\n",
    "        # Registrar 'pe' como buffer para que no se actualice durante el entrenamiento : https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        # pe: [1, max_len, d_model]\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "summary(PositionalEncoding(D_MODEL, MAX_SENTENCE_LENGTH_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No tiene párametros entrenables !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "[[ 0.          1.          0.          1.        ]\n",
    " [ 0.84147096  0.54030234  0.00999983  0.99995   ]\n",
    " [ 0.9092974  -0.41614684  0.01999867  0.9998    ]\n",
    " [ 0.14112    -0.9899925   0.0299955   0.99955004]\n",
    " [-0.7568025  -0.6536436   0.03998933  0.9992001 ]\n",
    " [-0.9589243   0.2836622   0.04997917  0.99875027]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.84147096  0.54030234  0.00999983  0.99995   ]\n",
      " [ 0.9092974  -0.41614684  0.01999867  0.9998    ]\n",
      " [ 0.14112    -0.9899925   0.0299955   0.99955004]\n",
      " [-0.7568025  -0.6536436   0.03998933  0.9992001 ]\n",
      " [-0.9589243   0.2836622   0.04997917  0.99875027]]\n"
     ]
    }
   ],
   "source": [
    "# Parámetros\n",
    "d_model = 4\n",
    "max_len = 6\n",
    "\n",
    "# Crear instancia de PositionalEncoding\n",
    "pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "# Imprimir los valores obtenidos\n",
    "positional_encoded_values = pos_encoding.pe.squeeze().numpy()\n",
    "# Mostrar los valores numéricos obtenidos\n",
    "print(positional_encoded_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0184, 0.6415, 0.3315, 0.1464],\n",
      "         [0.9324, 0.0599, 0.1288, 0.0022],\n",
      "         [0.1819, 0.0601, 0.0801, 0.8526],\n",
      "         [0.6830, 0.9237, 0.5524, 0.6168],\n",
      "         [0.9921, 0.7302, 0.1769, 0.9967]]]) \n",
      "\n",
      "tensor([[[ 0.0184,  1.6415,  0.3315,  1.1464],\n",
      "         [ 1.7739,  0.6002,  0.1388,  1.0021],\n",
      "         [ 1.0912, -0.3560,  0.1001,  1.8524],\n",
      "         [ 0.8241, -0.0662,  0.5824,  1.6163],\n",
      "         [ 0.2353,  0.0765,  0.2169,  1.9959]]])\n"
     ]
    }
   ],
   "source": [
    "rand_embed = torch.rand(\n",
    "    1, 5, d_model\n",
    ")  # generamos un batch con una sola secuencia de 5 tokens\n",
    "print(rand_embed, \"\\n\")  # embedding original\n",
    "print(pos_encoding(rand_embed))  # embedding + pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "> We call our particular attention \"Scaled Dot-Product Attention\". The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$ . We compute the matrix of outputs as:\n",
    "> \n",
    "> $$\n",
    "> \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "> $$\n",
    ">\n",
    "> ...\n",
    ">\n",
    "> We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$.\n",
    ">\n",
    "> ...\n",
    "> \n",
    "> Instead of performing a single attention function with $d_{\\text{model}}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values:\n",
    "> \n",
    "> $$\n",
    "> \\begin{aligned}\n",
    "> \\text{MultiHead}(Q, K, V) &= \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O \\\\\n",
    "> \\text{where head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "> \\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links útiles:\n",
    "- [\\[AI by hand\\] 11. Self Attention](https://aibyhand.substack.com/p/11-can-you-calculate-self-attention)\n",
    "- [\\[video\\] ¿Qué es un TRANSFORMER?](https://www.youtube.com/watch?v=aL-EmKuB078)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://yjucho1.github.io/assets/img/2018-10-13/transformer.png\" width=\"1200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Transformer uses multi-head attention in three different ways:\n",
    "> - In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.\n",
    "> - The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
    "> - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MultiHeadAttention                       --\n",
       "├─Linear: 1-1                            65,792\n",
       "├─Linear: 1-2                            65,792\n",
       "├─Linear: 1-3                            65,792\n",
       "├─Linear: 1-4                            65,792\n",
       "=================================================================\n",
       "Total params: 263,168\n",
       "Trainable params: 263,168\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert (\n",
    "            self.d_k * heads == d_model\n",
    "        ), \"el d_model debe ser divisible por los heads\"\n",
    "\n",
    "        self.keys = nn.Linear(d_model, d_model)\n",
    "        self.values = nn.Linear(d_model, d_model)\n",
    "        self.queries = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        # values = [batch_size, values_len, d_model]\n",
    "        # keys = [batch_size, keys_len, d_model]\n",
    "        # queries = [batch_size, queries_len, d_model]\n",
    "        # values_len = keys_len\n",
    "        batch_size = values.size(0)\n",
    "        values_len, keys_len, queries_len = (\n",
    "            values.size(1),\n",
    "            keys.size(1),\n",
    "            queries.size(1),\n",
    "        )\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "        # values = [batch_size, values_len, d_model]\n",
    "        # keys = [batch_size, keys_len, d_model]\n",
    "        # queries = [batch_size, queries_len, d_model]\n",
    "\n",
    "        # dividir en heads\n",
    "        values = values.reshape(batch_size, values_len, self.heads, self.d_k)\n",
    "        keys = keys.reshape(batch_size, keys_len, self.heads, self.d_k)\n",
    "        queries = queries.reshape(batch_size, queries_len, self.heads, self.d_k)\n",
    "\n",
    "        # calcular el score de atencion\n",
    "        queries = torch.permute(queries, (0, 2, 1, 3))\n",
    "        # queries = [batch_size, heads, queries_len, d_model]\n",
    "        keys = torch.permute(keys, (0, 2, 3, 1))\n",
    "        # keys = [batch_size, heads, d_model, key_len]\n",
    "        attention_score = torch.matmul(queries, keys)\n",
    "        # attention_score:  [batch_size, heads, queries_len, key_len]\n",
    "        attention_score = attention_score / (self.d_k**0.5)\n",
    "\n",
    "        # aplicar mascara si me la pasan\n",
    "        if mask is not None:\n",
    "            attention_score = torch.masked_fill(\n",
    "                attention_score, mask == False, float(\"-inf\")\n",
    "            )\n",
    "\n",
    "        attention_score = torch.softmax(attention_score, dim=-1)\n",
    "        # attention_score:  [batch_size, heads, queries_len, key_len]\n",
    "        values = torch.permute(values, (0, 2, 1, 3))\n",
    "        # values:  [batch_size, heads, values_len, d_model]\n",
    "        # recordar: key_len = values_len\n",
    "        attention = torch.matmul(attention_score, values)\n",
    "        # attention:  [batch_size, heads, queries_len, d_model]\n",
    "\n",
    "        # unir las cabezales\n",
    "        attention = torch.reshape(\n",
    "            torch.permute(attention, (0, 2, 1, 3)),\n",
    "            (batch_size, queries_len, self.d_model),\n",
    "        )\n",
    "        # attention: [batch_size, queries_len, d_model]\n",
    "\n",
    "        output = self.output(attention)\n",
    "        # output: [batch_size, queries_len, d_model]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "summary(MultiHeadAttention(d_model=D_MODEL, heads=HEADS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward\n",
    "\n",
    "> In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
    "> \n",
    "> $$\n",
    "> \\text{FFN}(x) = \\max(0, xW_1 + b_1) W_2 + b_2\n",
    "> $$\n",
    ">\n",
    "> (...) The dimensionality of input and output is $d_{\\text{model}} = 512$, and the inner-layer has dimensionality $d_{\\text{ff}} = 2048$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "FeedForward                              --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       263,168\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Linear: 2-3                       262,400\n",
       "=================================================================\n",
       "Total params: 525,568\n",
       "Trainable params: 525,568\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, expansion_factor):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, expansion_factor * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(expansion_factor * d_model, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "summary(FeedForward(d_model=D_MODEL, expansion_factor=FORWARD_EXPANSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\" width=\"400px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Encoder**: The encoder is composed of a stack of $N = 6$ identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is $LayerNorm(x + Sublayer(x))$, where $Sublayer(x)$ is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model} = 512$.\n",
    ">\n",
    "> ...\n",
    ">\n",
    "> **Residual Dropout**: During training, we apply dropout to the output of each sub-layer before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a dropout rate of $P_{drop} = 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "EncoderLayer                             [128, 17, 256]            --\n",
       "├─MultiHeadAttention: 1-1                [128, 17, 256]            --\n",
       "│    └─Linear: 2-1                       [128, 17, 256]            65,792\n",
       "│    └─Linear: 2-2                       [128, 17, 256]            65,792\n",
       "│    └─Linear: 2-3                       [128, 17, 256]            65,792\n",
       "│    └─Linear: 2-4                       [128, 17, 256]            65,792\n",
       "├─Dropout: 1-2                           [128, 17, 256]            --\n",
       "├─LayerNorm: 1-3                         [128, 17, 256]            512\n",
       "├─FeedForward: 1-4                       [128, 17, 256]            --\n",
       "│    └─Sequential: 2-5                   [128, 17, 256]            --\n",
       "│    │    └─Linear: 3-1                  [128, 17, 1024]           263,168\n",
       "│    │    └─ReLU: 3-2                    [128, 17, 1024]           --\n",
       "│    │    └─Linear: 3-3                  [128, 17, 256]            262,400\n",
       "├─Dropout: 1-5                           [128, 17, 256]            --\n",
       "├─LayerNorm: 1-6                         [128, 17, 256]            512\n",
       "==========================================================================================\n",
       "Total params: 789,760\n",
       "Trainable params: 789,760\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 101.09\n",
       "==========================================================================================\n",
       "Input size (MB): 2.23\n",
       "Forward/backward pass size (MB): 49.02\n",
       "Params size (MB): 3.16\n",
       "Estimated Total Size (MB): 54.41\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout, forward_expansion):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = FeedForward(d_model, forward_expansion)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: [Batch, seq_len, d_model]\n",
    "        # mask: [Batch, 1, 1, seq_len] or None\n",
    "        attention = self.self_attention(x, x, x, mask)\n",
    "        # attention [Batch, seq_len, d_model]\n",
    "        attention = self.dropout(attention)\n",
    "        x = self.norm1(attention + x)\n",
    "        # x: [Batch, seq_len, d_model]\n",
    "        forward = self.feed_forward(x)\n",
    "        # forward: [Batch, seq_len, d_model]\n",
    "        forward = self.dropout(forward)\n",
    "        out = self.norm2(forward + x)\n",
    "        # out: [Batch, seq_len, d_model]\n",
    "        return out\n",
    "\n",
    "\n",
    "rand_input = torch.randn(\n",
    "    (BATCH_SIZE, MAX_SENTENCE_LENGTH_MODEL, D_MODEL), dtype=torch.float\n",
    ")  # generamos un batch de una secuencia de embeddings\n",
    "rand_mask = torch.ones(\n",
    "    (BATCH_SIZE, 1, 1, MAX_SENTENCE_LENGTH_MODEL), dtype=torch.bool\n",
    ")  # mascara de atencion (aqui no enmascaramos nada)\n",
    "\n",
    "summary(\n",
    "    EncoderLayer(\n",
    "        d_model=D_MODEL,\n",
    "        heads=HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        forward_expansion=FORWARD_EXPANSION,\n",
    "    ),\n",
    "    input_data=(rand_input, rand_mask),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  [128, 17, 256]            --\n",
       "├─Embedding: 1-1                         [128, 17, 256]            6,408,448\n",
       "├─PositionalEncoding: 1-2                [128, 17, 256]            --\n",
       "├─Dropout: 1-3                           [128, 17, 256]            --\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "│    └─EncoderLayer: 2-1                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-1      [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-2                 [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-3               [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-4             [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-5                 [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-6               [128, 17, 256]            512\n",
       "│    └─EncoderLayer: 2-2                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-7      [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-8                 [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-9               [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-10            [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-11                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-12              [128, 17, 256]            512\n",
       "│    └─EncoderLayer: 2-3                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-13     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-14                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-15              [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-16            [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-17                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-18              [128, 17, 256]            512\n",
       "│    └─EncoderLayer: 2-4                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-19     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-20                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-21              [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-22            [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-23                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-24              [128, 17, 256]            512\n",
       "│    └─EncoderLayer: 2-5                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-25     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-26                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-27              [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-28            [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-29                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-30              [128, 17, 256]            512\n",
       "│    └─EncoderLayer: 2-6                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-31     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-32                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-33              [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-34            [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-35                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-36              [128, 17, 256]            512\n",
       "==========================================================================================\n",
       "Total params: 11,147,008\n",
       "Trainable params: 11,147,008\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.43\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 298.58\n",
       "Params size (MB): 44.59\n",
       "Estimated Total Size (MB): 343.19\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        d_model,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            src_vocab_size, d_model, padding_idx=SRC_VOCAB[PAD_TOKEN]\n",
    "        )\n",
    "        self.position_embedding = PositionalEncoding(d_model, max_length)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(d_model, heads, dropout, forward_expansion)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embeddings = self.word_embedding(x)\n",
    "        # embeddings: [batch_size, seq_len, d_model]\n",
    "        embeddings = self.position_embedding(embeddings)\n",
    "        # embeddings: [batch_size, seq_len, d_model]\n",
    "        out = self.dropout(embeddings)\n",
    "        # out: [batch_size, seq_len, d_model]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)\n",
    "        # out: [batch_size, seq_len, d_model]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "rand_input = torch.randint(\n",
    "    0, SRC_VOCAB_SIZE, (BATCH_SIZE, MAX_SENTENCE_LENGTH_MODEL)\n",
    ")  # generamos un batch de una secuencia de tokens\n",
    "rand_mask = torch.ones(\n",
    "    (BATCH_SIZE, 1, 1, MAX_SENTENCE_LENGTH_MODEL), dtype=torch.bool\n",
    ")  # mascara de atencion (aqui no enmascaramos nada)\n",
    "\n",
    "summary(\n",
    "    Encoder(\n",
    "        src_vocab_size=SRC_VOCAB_SIZE,\n",
    "        d_model=D_MODEL,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        heads=HEADS,\n",
    "        forward_expansion=FORWARD_EXPANSION,\n",
    "        dropout=DROPOUT,\n",
    "        max_length=MAX_SENTENCE_LENGTH_MODEL,\n",
    "    ),\n",
    "    input_data=(rand_input, rand_mask),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://d1.awsstatic.com/GENAI-1.151ded5440b4c997bac0642ec669a00acff2cca1.png\" width=\"400px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Decoder**: The decoder is also composed of a stack of $N = 6$ identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position,ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DecoderLayer                             [128, 17, 256]            --\n",
       "├─MultiHeadAttention: 1-1                [128, 17, 256]            --\n",
       "│    └─Linear: 2-1                       [128, 17, 256]            65,792\n",
       "│    └─Linear: 2-2                       [128, 17, 256]            65,792\n",
       "│    └─Linear: 2-3                       [128, 17, 256]            65,792\n",
       "│    └─Linear: 2-4                       [128, 17, 256]            65,792\n",
       "├─Dropout: 1-2                           [128, 17, 256]            --\n",
       "├─LayerNorm: 1-3                         [128, 17, 256]            512\n",
       "├─MultiHeadAttention: 1-4                [128, 17, 256]            --\n",
       "│    └─Linear: 2-5                       [128, 17, 256]            65,792\n",
       "│    └─Linear: 2-6                       [128, 17, 256]            65,792\n",
       "│    └─Linear: 2-7                       [128, 17, 256]            65,792\n",
       "│    └─Linear: 2-8                       [128, 17, 256]            65,792\n",
       "├─Dropout: 1-5                           [128, 17, 256]            --\n",
       "├─LayerNorm: 1-6                         [128, 17, 256]            512\n",
       "├─FeedForward: 1-7                       [128, 17, 256]            --\n",
       "│    └─Sequential: 2-9                   [128, 17, 256]            --\n",
       "│    │    └─Linear: 3-1                  [128, 17, 1024]           263,168\n",
       "│    │    └─ReLU: 3-2                    [128, 17, 1024]           --\n",
       "│    │    └─Linear: 3-3                  [128, 17, 256]            262,400\n",
       "├─Dropout: 1-8                           [128, 17, 256]            --\n",
       "├─LayerNorm: 1-9                         [128, 17, 256]            512\n",
       "==========================================================================================\n",
       "Total params: 1,053,440\n",
       "Trainable params: 1,053,440\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 134.84\n",
       "==========================================================================================\n",
       "Input size (MB): 4.50\n",
       "Forward/backward pass size (MB): 71.30\n",
       "Params size (MB): 4.21\n",
       "Estimated Total Size (MB): 80.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, forward_expansion, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.encoder_attention = MultiHeadAttention(d_model, heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = FeedForward(d_model, forward_expansion)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        # x: [Batch, trg_seq_len, d_model]\n",
    "        # enc_out: [Batch, src_seq_len, d_model]\n",
    "        # src_mask: [Batch, 1, 1, src_seq_len] or None\n",
    "        # trg_mask: [Batch, 1, trg_seq_len, trg_seq_len] or None\n",
    "\n",
    "        # Self-Attention con máscara de look-ahead\n",
    "        self_attn = self.self_attention(x, x, x, trg_mask)\n",
    "        # self_attn: [Batch, trg_seq_len, d_model]\n",
    "        self_attn = self.dropout(self_attn)\n",
    "        x = self.norm1(self_attn + x)\n",
    "\n",
    "        # Atención sobre la salida del encoder\n",
    "        enc_attn = self.encoder_attention(enc_out, enc_out, x, src_mask)\n",
    "        # enc_attn: [Batch, trg_seq_len, d_model]\n",
    "\n",
    "        enc_attn = self.dropout(enc_attn)\n",
    "        x = self.norm2(enc_attn + x)\n",
    "\n",
    "        # Red Feed-Forward\n",
    "        forward = self.feed_forward(x)\n",
    "        # forward: [Batch, trg_seq_len, d_model]\n",
    "        forward = self.dropout(forward)\n",
    "        out = self.norm3(forward + x)\n",
    "        # out: [Batch, trg_seq_len, d_model]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "rand_enc_out = torch.randn(\n",
    "    (BATCH_SIZE, MAX_SENTENCE_LENGTH_MODEL, D_MODEL), dtype=torch.float\n",
    ")  # generamos un batch de salida del encoder\n",
    "rand_trg_input = torch.randn(\n",
    "    (BATCH_SIZE, MAX_SENTENCE_LENGTH_MODEL, D_MODEL), dtype=torch.float\n",
    ")  # generamos un batch de una secuencia de embeddings (input del decoder)\n",
    "rand_src_mask = torch.ones(\n",
    "    (BATCH_SIZE, 1, 1, MAX_SENTENCE_LENGTH_MODEL), dtype=torch.bool\n",
    ")  # mascara de atencion del encoder (aqui no enmascaramos nada)\n",
    "rand_trg_mask = torch.ones(\n",
    "    (BATCH_SIZE, 1, MAX_SENTENCE_LENGTH_MODEL, MAX_SENTENCE_LENGTH_MODEL),\n",
    "    dtype=torch.bool,\n",
    ")  # mascara de atencion del decoder (aqui no enmascaramos nada)\n",
    "\n",
    "summary(\n",
    "    DecoderLayer(\n",
    "        d_model=D_MODEL,\n",
    "        heads=HEADS,\n",
    "        forward_expansion=FORWARD_EXPANSION,\n",
    "        dropout=DROPOUT,\n",
    "    ),\n",
    "    input_data=(rand_trg_input, rand_enc_out, rand_src_mask, rand_trg_mask),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Decoder                                  [128, 17, 45139]          --\n",
       "├─Embedding: 1-1                         [128, 17, 256]            11,555,584\n",
       "├─PositionalEncoding: 1-2                [128, 17, 256]            --\n",
       "├─Dropout: 1-3                           [128, 17, 256]            --\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "│    └─DecoderLayer: 2-1                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-1      [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-2                 [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-3               [128, 17, 256]            512\n",
       "│    │    └─MultiHeadAttention: 3-4      [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-5                 [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-6               [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-7             [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-8                 [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-9               [128, 17, 256]            512\n",
       "│    └─DecoderLayer: 2-2                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-10     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-11                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-12              [128, 17, 256]            512\n",
       "│    │    └─MultiHeadAttention: 3-13     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-14                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-15              [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-16            [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-17                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-18              [128, 17, 256]            512\n",
       "│    └─DecoderLayer: 2-3                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-19     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-20                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-21              [128, 17, 256]            512\n",
       "│    │    └─MultiHeadAttention: 3-22     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-23                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-24              [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-25            [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-26                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-27              [128, 17, 256]            512\n",
       "│    └─DecoderLayer: 2-4                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-28     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-29                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-30              [128, 17, 256]            512\n",
       "│    │    └─MultiHeadAttention: 3-31     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-32                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-33              [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-34            [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-35                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-36              [128, 17, 256]            512\n",
       "│    └─DecoderLayer: 2-5                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-37     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-38                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-39              [128, 17, 256]            512\n",
       "│    │    └─MultiHeadAttention: 3-40     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-41                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-42              [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-43            [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-44                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-45              [128, 17, 256]            512\n",
       "│    └─DecoderLayer: 2-6                 [128, 17, 256]            --\n",
       "│    │    └─MultiHeadAttention: 3-46     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-47                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-48              [128, 17, 256]            512\n",
       "│    │    └─MultiHeadAttention: 3-49     [128, 17, 256]            263,168\n",
       "│    │    └─Dropout: 3-50                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-51              [128, 17, 256]            512\n",
       "│    │    └─FeedForward: 3-52            [128, 17, 256]            525,568\n",
       "│    │    └─Dropout: 3-53                [128, 17, 256]            --\n",
       "│    │    └─LayerNorm: 3-54              [128, 17, 256]            512\n",
       "├─Linear: 1-5                            [128, 17, 45139]          11,600,723\n",
       "==========================================================================================\n",
       "Total params: 29,476,947\n",
       "Trainable params: 29,476,947\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 3.77\n",
       "==========================================================================================\n",
       "Input size (MB): 2.29\n",
       "Forward/backward pass size (MB): 1218.06\n",
       "Params size (MB): 117.91\n",
       "Estimated Total Size (MB): 1338.25\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        d_model,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            trg_vocab_size, d_model, padding_idx=TRG_VOCAB[PAD_TOKEN]\n",
    "        )\n",
    "        self.position_embedding = PositionalEncoding(d_model, max_length)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(d_model, heads, forward_expansion, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        # x : [Batch, trg_seq_len]\n",
    "        # enc_out:  [Batch, src_seq_len, d_model]\n",
    "        # src_mask: [Batch, 1, 1, src_seq_len] or None\n",
    "        # trg_mask: [Batch, 1, trg_seq_len, trg_seq_len] or None\n",
    "\n",
    "        # Embeddings de entrada\n",
    "        x = self.word_embedding(x)\n",
    "        # x: [Batch, trg_seq_len, d_model]\n",
    "        x = self.position_embedding(x)\n",
    "        # x: [Batch, trg_seq_len, d_model]\n",
    "        x = self.dropout(x)\n",
    "        # x: [Batch, trg_seq_len, d_model]\n",
    "\n",
    "        # Pasar por las capas del decoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, src_mask, trg_mask)\n",
    "            # x: [Batch, trg_seq_len, d_model]\n",
    "\n",
    "        # Proyección a vocabulario de salida\n",
    "        out = self.fc_out(x)\n",
    "        # out: [[Batch, trg_seq_len, trg_vocab_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "rand_enc_out = torch.randn(\n",
    "    (BATCH_SIZE, MAX_SENTENCE_LENGTH_MODEL, D_MODEL), dtype=torch.float\n",
    ")  # generamos un batch de salida del encoder\n",
    "rand_trg_input = torch.randint(\n",
    "    0, TRG_VOCAB_SIZE, (BATCH_SIZE, MAX_SENTENCE_LENGTH_MODEL)\n",
    ")  # generamos un batch de una secuencia de tokens\n",
    "rand_src_mask = torch.ones(\n",
    "    (BATCH_SIZE, 1, 1, MAX_SENTENCE_LENGTH_MODEL), dtype=torch.bool\n",
    ")  # mascara de atencion del encoder (aqui no enmascaramos nada)\n",
    "rand_trg_mask = torch.ones(\n",
    "    (BATCH_SIZE, 1, MAX_SENTENCE_LENGTH_MODEL, MAX_SENTENCE_LENGTH_MODEL),\n",
    "    dtype=torch.bool,\n",
    ")  # mascara de atencion del decoder (aqui no enmascaramos nada)\n",
    "\n",
    "summary(\n",
    "    Decoder(\n",
    "        trg_vocab_size=TRG_VOCAB_SIZE,\n",
    "        d_model=D_MODEL,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        heads=HEADS,\n",
    "        forward_expansion=FORWARD_EXPANSION,\n",
    "        dropout=DROPOUT,\n",
    "        max_length=MAX_SENTENCE_LENGTH_MODEL,\n",
    "    ),\n",
    "    input_data=(rand_trg_input, rand_enc_out, rand_src_mask, rand_trg_mask),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "#### Encoder Padding Mask\n",
    "\n",
    "En el caso de del encoder, cada token puede atender a todos los tokens de la secuencia de entrada (sin enmascaramiento). Sólo se aplica una máscara para los tokens de padding, de manera que el modelo no preste atención a esos tokens irrelevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_src_mask(src, src_pad_idx):\n",
    "    # src: [batch_size, src_len]\n",
    "    src_mask = (src != src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    # [batch_size, 1, 1, src_len]\n",
    "    # recordemos que esto se hace en el contexto de que la aplicación de la máscara es en la matriz de atención (con el shape [batch_size, heads, queries_len, key_len])\n",
    "    # se hace broadcastin en heads porque no importa para qué cabeza es, la máscara es la misma\n",
    "    # se hace broadcasting en queries_len porque no sabemos cuántas queries habrá en el decoder lo importante es que las keys (src_len) estén enmascaradas (las culumnas)\n",
    "    return src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ True,  True,  True, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "input_test = torch.tensor([[1, 2, 3, 0, 0]], dtype=torch.float32)\n",
    "input_test_mask = create_src_mask(input_test, SRC_VOCAB[PAD_TOKEN])\n",
    "print(input_test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como la mascara puede ser usada para enmascarar los scores de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de la máscara:\n",
      "tensor([[[[0.8890, 0.1382, 0.5053, 0.1419, 0.9124],\n",
      "          [0.4866, 0.5986, 0.6645, 0.8106, 0.3378],\n",
      "          [0.8874, 0.6087, 0.3668, 0.0065, 0.1461],\n",
      "          [0.5273, 0.5827, 0.5564, 0.2997, 0.5261],\n",
      "          [0.5466, 0.0291, 0.2278, 0.8868, 0.6011]]]])\n"
     ]
    }
   ],
   "source": [
    "src_len = input_test.size(1)  # longitud de la secuencia de entrada\n",
    "rand_input = torch.rand(\n",
    "    (1, 1, src_len, src_len), dtype=torch.float\n",
    ")  # generamos scores de atención (random)\n",
    "print(\"Antes de la máscara:\")\n",
    "print(rand_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de la máscara:\n",
      "tensor([[[[0.8890, 0.1382, 0.5053,   -inf,   -inf],\n",
      "          [0.4866, 0.5986, 0.6645,   -inf,   -inf],\n",
      "          [0.8874, 0.6087, 0.3668,   -inf,   -inf],\n",
      "          [0.5273, 0.5827, 0.5564,   -inf,   -inf],\n",
      "          [0.5466, 0.0291, 0.2278,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "masked = torch.masked_fill(\n",
    "    rand_input, input_test_mask == False, float(\"-inf\")\n",
    ")  # aplicamos la máscara\n",
    "print(\"Después de la máscara:\")\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Look-Ahead Masking\n",
    "\n",
    "> We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos ayudamos con la función [torch.tril](https://pytorch.org/docs/stable/generated/torch.tril.html) para obtener la matriz triangular inferior de una matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(\n",
    "    torch.full((5, 5), True), diagonal=0\n",
    ")  # jugando con la diagonal podemos controlar qué parte de la matriz triangular inferior queremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trg_mask(trg, trg_pad_idx):\n",
    "    # trg: [batch_size, trg_len]\n",
    "    trg_pad_mask = (trg != trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
    "\n",
    "    # Crear máscara de look-ahead\n",
    "    trg_len = trg.size(1)\n",
    "    trg_sub_mask = torch.tril(\n",
    "        torch.full((trg_len, trg_len), True, device=trg.device)\n",
    "    )  # tril genera la matriz en CPU por defecto, así que nos aseguramos de que esté en el mismo dispositivo que 'trg'\n",
    "    # trg_sub_mask: [trg_len, trg_len]\n",
    "    trg_mask = trg_pad_mask & trg_sub_mask  # combinamos ambas máscaras\n",
    "    # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "    return trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ True, False, False, False, False],\n",
      "          [ True,  True, False, False, False],\n",
      "          [ True,  True,  True, False, False],\n",
      "          [ True,  True,  True, False, False],\n",
      "          [ True,  True,  True, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "input_test = torch.tensor([[1, 2, 3, 0, 0]], dtype=torch.float32)\n",
    "print(create_trg_mask(input_test, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "Links útiles:\n",
    "- [\\[visualización\\] LLM Visualization](https://bbycroft.net/llm)\n",
    "- [\\[visualización\\] TRANSFORMER EXPLAINER](https://poloclub.github.io/transformer-explainer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://d1.awsstatic.com/GENAI-1.151ded5440b4c997bac0642ec669a00acff2cca1.png\" width=\"400px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Transformer                                   [128, 17, 45139]          --\n",
       "├─Encoder: 1-1                                [128, 17, 256]            --\n",
       "│    └─Embedding: 2-1                         [128, 17, 256]            6,408,448\n",
       "│    └─PositionalEncoding: 2-2                [128, 17, 256]            --\n",
       "│    └─Dropout: 2-3                           [128, 17, 256]            --\n",
       "│    └─ModuleList: 2-4                        --                        --\n",
       "│    │    └─EncoderLayer: 3-1                 [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-1      [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-1             [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-2             [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-3             [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-4             [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-2                 [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-3               [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-4             [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-5         [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-5                 [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-6               [128, 17, 256]            512\n",
       "│    │    └─EncoderLayer: 3-2                 [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-7      [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-6             [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-7             [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-8             [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-9             [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-8                 [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-9               [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-10            [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-10        [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-11                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-12              [128, 17, 256]            512\n",
       "│    │    └─EncoderLayer: 3-3                 [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-13     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-11            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-12            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-13            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-14            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-14                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-15              [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-16            [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-15        [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-17                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-18              [128, 17, 256]            512\n",
       "│    │    └─EncoderLayer: 3-4                 [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-19     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-16            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-17            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-18            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-19            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-20                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-21              [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-22            [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-20        [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-23                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-24              [128, 17, 256]            512\n",
       "│    │    └─EncoderLayer: 3-5                 [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-25     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-21            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-22            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-23            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-24            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-26                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-27              [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-28            [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-25        [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-29                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-30              [128, 17, 256]            512\n",
       "│    │    └─EncoderLayer: 3-6                 [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-31     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-26            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-27            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-28            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-29            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-32                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-33              [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-34            [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-30        [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-35                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-36              [128, 17, 256]            512\n",
       "├─Decoder: 1-2                                [128, 17, 45139]          --\n",
       "│    └─Embedding: 2-5                         [128, 17, 256]            11,555,584\n",
       "│    └─PositionalEncoding: 2-6                [128, 17, 256]            --\n",
       "│    └─Dropout: 2-7                           [128, 17, 256]            --\n",
       "│    └─ModuleList: 2-8                        --                        --\n",
       "│    │    └─DecoderLayer: 3-7                 [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-37     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-31            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-32            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-33            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-34            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-38                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-39              [128, 17, 256]            512\n",
       "│    │    │    └─MultiHeadAttention: 4-40     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-35            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-36            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-37            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-38            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-41                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-42              [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-43            [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-39        [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-44                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-45              [128, 17, 256]            512\n",
       "│    │    └─DecoderLayer: 3-8                 [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-46     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-40            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-41            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-42            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-43            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-47                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-48              [128, 17, 256]            512\n",
       "│    │    │    └─MultiHeadAttention: 4-49     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-44            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-45            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-46            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-47            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-50                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-51              [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-52            [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-48        [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-53                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-54              [128, 17, 256]            512\n",
       "│    │    └─DecoderLayer: 3-9                 [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-55     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-49            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-50            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-51            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-52            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-56                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-57              [128, 17, 256]            512\n",
       "│    │    │    └─MultiHeadAttention: 4-58     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-53            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-54            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-55            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-56            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-59                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-60              [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-61            [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-57        [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-62                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-63              [128, 17, 256]            512\n",
       "│    │    └─DecoderLayer: 3-10                [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-64     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-58            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-59            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-60            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-61            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-65                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-66              [128, 17, 256]            512\n",
       "│    │    │    └─MultiHeadAttention: 4-67     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-62            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-63            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-64            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-65            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-68                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-69              [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-70            [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-66        [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-71                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-72              [128, 17, 256]            512\n",
       "│    │    └─DecoderLayer: 3-11                [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-73     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-67            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-68            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-69            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-70            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-74                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-75              [128, 17, 256]            512\n",
       "│    │    │    └─MultiHeadAttention: 4-76     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-71            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-72            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-73            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-74            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-77                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-78              [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-79            [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-75        [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-80                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-81              [128, 17, 256]            512\n",
       "│    │    └─DecoderLayer: 3-12                [128, 17, 256]            --\n",
       "│    │    │    └─MultiHeadAttention: 4-82     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-76            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-77            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-78            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-79            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-83                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-84              [128, 17, 256]            512\n",
       "│    │    │    └─MultiHeadAttention: 4-85     [128, 17, 256]            --\n",
       "│    │    │    │    └─Linear: 5-80            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-81            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-82            [128, 17, 256]            65,792\n",
       "│    │    │    │    └─Linear: 5-83            [128, 17, 256]            65,792\n",
       "│    │    │    └─Dropout: 4-86                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-87              [128, 17, 256]            512\n",
       "│    │    │    └─FeedForward: 4-88            [128, 17, 256]            --\n",
       "│    │    │    │    └─Sequential: 5-84        [128, 17, 256]            525,568\n",
       "│    │    │    └─Dropout: 4-89                [128, 17, 256]            --\n",
       "│    │    │    └─LayerNorm: 4-90              [128, 17, 256]            512\n",
       "│    └─Linear: 2-9                            [128, 17, 45139]          11,600,723\n",
       "===============================================================================================\n",
       "Total params: 40,623,955\n",
       "Trainable params: 40,623,955\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 5.20\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 1516.64\n",
       "Params size (MB): 162.50\n",
       "Estimated Total Size (MB): 1679.17\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        d_model=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0.1,\n",
    "        max_length=100,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            d_model,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            d_model,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = create_src_mask(src, self.src_pad_idx)\n",
    "        trg_mask = create_trg_mask(trg, self.trg_pad_idx)\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "summary(\n",
    "    Transformer(\n",
    "        src_vocab_size=SRC_VOCAB_SIZE,\n",
    "        trg_vocab_size=TRG_VOCAB_SIZE,\n",
    "        src_pad_idx=SRC_PAD_IDX,\n",
    "        trg_pad_idx=TRG_PAD_IDX,\n",
    "        d_model=D_MODEL,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        forward_expansion=FORWARD_EXPANSION,\n",
    "        heads=HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        max_length=MAX_SENTENCE_LENGTH_MODEL,\n",
    "    ),\n",
    "    input_data=(\n",
    "        torch.randint(0, SRC_VOCAB_SIZE, (BATCH_SIZE, MAX_SENTENCE_LENGTH_MODEL)),\n",
    "        torch.randint(0, TRG_VOCAB_SIZE, (BATCH_SIZE, MAX_SENTENCE_LENGTH_MODEL)),\n",
    "    ),\n",
    "    depth=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0005\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    trg_vocab_size=TRG_VOCAB_SIZE,\n",
    "    src_pad_idx=SRC_PAD_IDX,\n",
    "    trg_pad_idx=TRG_PAD_IDX,\n",
    "    d_model=D_MODEL,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    forward_expansion=FORWARD_EXPANSION,\n",
    "    heads=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    max_length=MAX_SENTENCE_LENGTH_MODEL,\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=TRG_PAD_IDX,\n",
    "    label_smoothing=0.05,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(src, trg, model, criterion):\n",
    "    # src: [BATCH_SIZE, SRC_LEN]\n",
    "    # trg: [BATCH_SIZE, TRG_LEN]\n",
    "\n",
    "    input_trg = trg[:, :-1]  # Entrada al decoder: todos los tokens excepto el último\n",
    "    # input_trg: [batch_size, trg_len - 1]\n",
    "\n",
    "    target_trg = trg[\n",
    "        :, 1:\n",
    "    ]  # Objetivo para la pérdida: todos los tokens excepto el primero\n",
    "    # target_trg: [batch_size, trg_len - 1]\n",
    "\n",
    "    # Pasar por el modelo\n",
    "    output = model(src, input_trg)\n",
    "    # output: [batch_size, trg_len - 1, trg_vocab_size]\n",
    "\n",
    "    # Reestructurar las dimensiones para calcular la pérdida\n",
    "    output = output.reshape(-1, output.size(-1))\n",
    "    # output: [batch_size * (trg_len - 1), trg_vocab_size]\n",
    "\n",
    "    target_trg = target_trg.reshape(-1)\n",
    "    # target_trg: [batch_size * (trg_len - 1)]\n",
    "\n",
    "    return criterion(output, target_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, criterion, valid_loader):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in valid_loader:\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "\n",
    "            loss = process_batch(src, trg, model, criterion)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, criterion, train_loader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for src, trg in train_loader:\n",
    "        src = src.to(DEVICE)\n",
    "        # src: [BATCH_SIZE, SRC_LEN]\n",
    "        trg = trg.to(DEVICE)\n",
    "        # trg: [BATCH_SIZE, TRG_LEN]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = process_batch(src, trg, model, criterion)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, optimizer, criterion, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, criterion, train_loader, optimizer)\n",
    "        val_loss = evaluate_epoch(model, criterion, valid_loader)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 3.6644 Val Loss: 2.5896\n",
      "Epoch 2 Train Loss: 2.3532 Val Loss: 2.2277\n",
      "Epoch 3 Train Loss: 2.0148 Val Loss: 2.1012\n",
      "Epoch 4 Train Loss: 1.8351 Val Loss: 2.0485\n",
      "Epoch 5 Train Loss: 1.7139 Val Loss: 1.9969\n",
      "Epoch 6 Train Loss: 1.6249 Val Loss: 1.9784\n",
      "Epoch 7 Train Loss: 1.5591 Val Loss: 1.9629\n",
      "Epoch 8 Train Loss: 1.5055 Val Loss: 1.9405\n",
      "Epoch 9 Train Loss: 1.4631 Val Loss: 1.9419\n",
      "Epoch 10 Train Loss: 1.4282 Val Loss: 1.9431\n",
      "Epoch 11 Train Loss: 1.3991 Val Loss: 1.9447\n",
      "Epoch 12 Train Loss: 1.3750 Val Loss: 1.9353\n",
      "Epoch 13 Train Loss: 1.3512 Val Loss: 1.9361\n",
      "Epoch 14 Train Loss: 1.3312 Val Loss: 1.9338\n",
      "Epoch 15 Train Loss: 1.3137 Val Loss: 1.9390\n",
      "Epoch 16 Train Loss: 1.2961 Val Loss: 1.9359\n",
      "Epoch 17 Train Loss: 1.2805 Val Loss: 1.9453\n",
      "Epoch 18 Train Loss: 1.2667 Val Loss: 1.9413\n",
      "Epoch 19 Train Loss: 1.2543 Val Loss: 1.9493\n",
      "Epoch 20 Train Loss: 1.2414 Val Loss: 1.9477\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10  # 5 para la clase\n",
    "\n",
    "train(model, train_loader, valid_loader, optimizer, criterion, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, max_len):\n",
    "    # Configuramos el modo de evaluación\n",
    "    model.eval()\n",
    "\n",
    "    # Limpiamos la oración de entrada\n",
    "    sentence = clean_text(sentence)\n",
    "\n",
    "    # Preprocesamos la oración de entrada (src_sentence) a tensor\n",
    "    sentence_indexes = encode_sentence(sentence, SRC_VOCAB)\n",
    "    # Convertimos a tensor y añadimos una dimensión extra para el batch\n",
    "    sentence_tensor = torch.tensor(sentence_indexes).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Creamos una máscara para la entrada (evitamos atender al padding)\n",
    "    src_mask = create_src_mask(sentence_tensor, SRC_VOCAB[PAD_TOKEN])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pasamos por el encoder\n",
    "        enc_src = model.encoder(sentence_tensor, src_mask)\n",
    "\n",
    "        # El primer token del decoder es <SOS>\n",
    "        input_tokens = torch.tensor([TRG_VOCAB[SOS_TOKEN]]).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Almacenamos los tokens generados por el decoder\n",
    "        generated_tokens = []\n",
    "\n",
    "        # Decodificación paso a paso\n",
    "        for _ in range(max_len):\n",
    "            trg_mask = create_trg_mask(input_tokens, TRG_VOCAB[PAD_TOKEN])\n",
    "\n",
    "            # Pasamos el token actual por el decoder\n",
    "            output = model.decoder(input_tokens, enc_src, src_mask, trg_mask)\n",
    "\n",
    "            # Solo nos interesa el último token generado\n",
    "            output = output[:, -1, :]\n",
    "\n",
    "            # Obtener el token con mayor probabilidad\n",
    "            top1 = output.argmax(1).item()\n",
    "\n",
    "            # Si el token predicho es <EOS>, detenemos la decodificación\n",
    "            if top1 == TRG_VOCAB[EOS_TOKEN]:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(top1)\n",
    "\n",
    "            # El próximo token de entrada es el que acaba de predecir el modelo\n",
    "            input_tokens = torch.cat(\n",
    "                [input_tokens, torch.tensor([[top1]]).to(DEVICE)], dim=1\n",
    "            )\n",
    "\n",
    "        # Convertimos los índices predichos en palabras usando el vocabulario\n",
    "        predicted_sentence = decode_sentence(generated_tokens, TRG_VOCAB)\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I am hungry\n",
      "Translation: tengo hambre\n",
      "\n",
      "Input: I am tired\n",
      "Translation: estoy cansado\n",
      "\n",
      "Input: I am happy\n",
      "Translation: estoy feliz\n",
      "\n",
      "Input: I'm sad\n",
      "Translation: estoy triste\n",
      "\n",
      "Input: I am angry\n",
      "Translation: estoy enojada\n",
      "\n",
      "Input: every time I study, I get sleepy\n",
      "Translation: cada vez que estudio me da sueño\n",
      "\n",
      "Input: I am going to the gym\n",
      "Translation: voy al gimnasio\n",
      "\n",
      "Input: I am going to the beach\n",
      "Translation: voy a la playa\n",
      "\n",
      "Input: I am going to the supermarket\n",
      "Translation: voy al supermercado\n",
      "\n",
      "Input: I'm going to the movies\n",
      "Translation: voy al cine\n",
      "\n",
      "Input: I don't know what to do\n",
      "Translation: no sé qué hacer\n",
      "\n",
      "Input: I love deep learning\n",
      "Translation: me encanta aprender profundo\n",
      "\n",
      "Input: I can't open the door\n",
      "Translation: no puedo abrir la puerta\n",
      "\n",
      "Input: you can go if you want to\n",
      "Translation: puedes ir si quieres\n",
      "\n",
      "Input: i'm going to the party\n",
      "Translation: voy a la fiesta\n",
      "\n",
      "Input: where does all this come from ?\n",
      "Translation: ¿ de dónde viene todo esto ?\n",
      "\n",
      "Input: I can read your mind\n",
      "Translation: puedo leer tu mente\n",
      "\n",
      "Input: I can't believe it\n",
      "Translation: no me lo puedo creer\n",
      "\n",
      "Input: I can't believe you\n",
      "Translation: no te puedo creer\n",
      "\n",
      "Input: I can't believe this\n",
      "Translation: no puedo creer esto\n",
      "\n",
      "Input: I didn't like it\n",
      "Translation: no me gustó\n",
      "\n",
      "Input: You can do it\n",
      "Translation: puedes hacerlo\n",
      "\n",
      "Input: Do you speak Italian?\n",
      "Translation: ¿ hablas italiano ?\n",
      "\n",
      "Input: Do you want to learn Spanish?\n",
      "Translation: ¿ querés aprender español ?\n",
      "\n",
      "Input: I want to learn French\n",
      "Translation: quiero aprender francés\n",
      "\n",
      "Input: Do you want to go to the movies?\n",
      "Translation: ¿ quieres ir al cine ?\n",
      "\n",
      "Input: this is my favorite song\n",
      "Translation: esta es mi canción favorita\n",
      "\n",
      "Input: I can't wait to see you\n",
      "Translation: no puedo esperar a verte\n",
      "\n",
      "Input: see you later\n",
      "Translation: hasta luego\n",
      "\n",
      "Input: have a nice day\n",
      "Translation: que tengas un buen día\n",
      "\n",
      "Input: we'll talk later\n",
      "Translation: hablaremos más tarde\n",
      "\n",
      "Input: let's grab a coffee sometime\n",
      "Translation: saquémonos un café alguna vez\n",
      "\n",
      "Input: they're coming to the party\n",
      "Translation: ellos están llegando a la fiesta\n",
      "\n",
      "Input: she's my best friend\n",
      "Translation: ella es mi mejor amiga\n",
      "\n",
      "Input: he's a great guy\n",
      "Translation: él es un gran tipo\n",
      "\n",
      "Input: My class is in 30 minutes.\n",
      "Translation: mi clase tiene 30 minutos\n",
      "\n",
      "Input: I have class tomorrow.\n",
      "Translation: tengo clase mañana\n",
      "\n",
      "Input: This is a very long sentence, let's see how the model handles it.\n",
      "Translation: esta es una frase muy larga , vamos a ver cómo se ocupa el modelo\n",
      "\n",
      "Input: Can you help me with my homework?\n",
      "Translation: ¿ me ayudas a hacer los deberes ?\n",
      "\n",
      "Input: What time is it?\n",
      "Translation: ¿ qué hora es ?\n",
      "\n",
      "Input: Where is the nearest restaurant?\n",
      "Translation: ¿ dónde está el restaurante más cercano ?\n",
      "\n",
      "Input: How do I get to the airport?\n",
      "Translation: ¿ cómo llego al aeropuerto ?\n",
      "\n",
      "Input: I would like to make a reservation.\n",
      "Translation: me gustaría hacer una reserva\n",
      "\n",
      "Input: The fish is fresh and delicious.\n",
      "Translation: el pescado es fresco y delicioso\n",
      "\n",
      "Input: I need to buy a new laptop.\n",
      "Translation: necesito comprar un nuevo portátil\n",
      "\n",
      "Input: My favorite color is blue.\n",
      "Translation: mi color favorito es el azul\n",
      "\n",
      "Input: I enjoy hiking on the weekends.\n",
      "Translation: disfruto jugar en los fines de semana\n",
      "\n",
      "Input: The weather is nice today.\n",
      "Translation: hoy hace buen tiempo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I am hungry\",\n",
    "    \"I am tired\",\n",
    "    \"I am happy\",\n",
    "    \"I'm sad\",\n",
    "    \"I am angry\",\n",
    "    \"every time I study, I get sleepy\",\n",
    "    \"I am going to the gym\",\n",
    "    \"I am going to the beach\",\n",
    "    \"I am going to the supermarket\",\n",
    "    \"I'm going to the movies\",\n",
    "    \"I don't know what to do\",\n",
    "    \"I love deep learning\",\n",
    "    \"I can't open the door\",\n",
    "    \"you can go if you want to\",\n",
    "    \"i'm going to the party\",\n",
    "    \"where does all this come from ?\",\n",
    "    \"I can read your mind\",\n",
    "    \"I can't believe it\",\n",
    "    \"I can't believe you\",\n",
    "    \"I can't believe this\",\n",
    "    \"I didn't like it\",\n",
    "    \"You can do it\",\n",
    "    \"Do you speak Italian?\",\n",
    "    \"Do you want to learn Spanish?\",\n",
    "    \"I want to learn French\",\n",
    "    \"Do you want to go to the movies?\",\n",
    "    \"this is my favorite song\",\n",
    "    \"I can't wait to see you\",\n",
    "    \"see you later\",\n",
    "    \"have a nice day\",\n",
    "    \"we'll talk later\",\n",
    "    \"let's grab a coffee sometime\",\n",
    "    \"they're coming to the party\",\n",
    "    \"she's my best friend\",\n",
    "    \"he's a great guy\",\n",
    "    \"My class is in 30 minutes.\",\n",
    "    \"I have class tomorrow.\",\n",
    "    \"This is a very long sentence, let's see how the model handles it.\",\n",
    "    \"Can you help me with my homework?\",\n",
    "    \"What time is it?\",\n",
    "    \"Where is the nearest restaurant?\",\n",
    "    \"How do I get to the airport?\",\n",
    "    \"I would like to make a reservation.\",\n",
    "    \"The fish is fresh and delicious.\",\n",
    "    \"I need to buy a new laptop.\",\n",
    "    \"My favorite color is blue.\",\n",
    "    \"I enjoy hiking on the weekends.\",\n",
    "    \"The weather is nice today.\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f\"Input: {sentence}\")\n",
    "    print(\n",
    "        f\"Translation: {translate_sentence(model, sentence, MAX_SENTENCE_LENGTH_MODEL)}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'la atención es todo lo que necesitas'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model, \"Attention is all you need\", MAX_SENTENCE_LENGTH_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea\n",
    "Modificar la inferencia para que no sea determinista, es decir, que en lugar de tomar siempre el token con mayor probabilidad, tome muestras de la distribución de probabilidad generada por el modelo en cada paso. Incluir parámetros como temperature y top-k sampling para controlar la diversidad de las traducciones generadas. Ver [torch.multinomial](https://docs.pytorch.org/docs/stable/generated/torch.multinomial.html), [torch.topk](https://docs.pytorch.org/docs/stable/generated/torch.topk.html#torch.topk)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taller-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
