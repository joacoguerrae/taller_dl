{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Redes Neuronales Convolucionales (CNN) con FashionMNIST\n",
    "\n",
    "En esta notebook, implementaremos una Red Neuronal Convolucional (CNN) básica y aprenderemos conceptos fundamentales como convoluciones y pooling, utilizando el dataset [FashionMNIST](https://en.wikipedia.org/wiki/Fashion_MNIST). El objetivo principal es entender cómo las CNNs pueden ser aplicadas a problemas de clasificación de imágenes.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "1. **Entender los conceptos básicos de las Redes Neuronales Convolucionales (CNN)**, incluyendo convoluciones, capas de pooling y capas totalmente conectadas.\n",
    "2. **Entrenar una CNN** en PyTorch utilizando el dataset FashionMNIST.\n",
    "3. **Evaluar el rendimiento del modelo** utilizando métricas adecuadas y visualización de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import (\n",
    "    train,\n",
    "    model_calassification_report,\n",
    "    show_tensor_image,\n",
    "    show_tensor_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la semilla para que los resultados sean reproducibles\n",
    "SEED = 34\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "elif torch.xpu.is_available():\n",
    "    DEVICE = \"xpu\"  # si no hay GPU, pero hay XPU, usamos XPU\n",
    "\n",
    "print(f\"Usando {DEVICE}\")\n",
    "\n",
    "NUM_WORKERS = 0 # Win y MacOS pueden tener problemas con múltiples workers\n",
    "if sys.platform == 'linux':\n",
    "    NUM_WORKERS = 4  # numero de workers para cargar los datos (depende de cada caso)\n",
    "\n",
    "print(f\"Usando {NUM_WORKERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 128  # tamaño del batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro a CNNs\n",
    "\n",
    "![Image](assets/cnn.png)\n",
    "\n",
    "\n",
    "Las redes convolucionales (CNNs) son muy utilizadas en el campo de **computer vision**, es decir, en problemas relacionados con imágenes como clasificación, detección de objetos, segmentación, etc. Los dos conceptos clave en las CNNs son las **convoluciones** y el **pooling**. Debido a que los pixeles de una imagen tienen una relación espacial, las CNNs son capaces de capturar patrones locales en la imagen, como bordes, texturas, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convoluciones\n",
    "\n",
    "En el contexto de las CNNs, las convoluciones se aplican a una imagen de entrada y un filtro (kernel) para producir una imagen de salida. La operación de convolución se realiza deslizando el filtro sobre la imagen de entrada, multiplicando sus valores por los valores de los píxeles de la imagen y sumando el resultado.\n",
    "\n",
    "![Image](https://d2l.ai/_images/correlation.svg)\n",
    "\n",
    "Vamos a ver algunos ejemplos de convoluciones y como afectan a las imágenes. Para ellos vamos a utilizar la función [`torch.nn.functional.conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html) de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float32)\n",
    "kernel = torch.tensor([[0, 1], [2, 3]], dtype=torch.float32)\n",
    "\n",
    "# para utilizar F.conv2d, necesitamos que las dimensiones de kernel sean [out_channels, in_channels, kernel_height, kernel_width]\n",
    "# y las dimensiones de input sean [batch_size, in_channels, height, width]\n",
    "\n",
    "input = input.reshape(1, 1, *input.shape)\n",
    "kernel = kernel.reshape(1, 1, *kernel.shape)\n",
    "\n",
    "F.conv2d(input, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo de cómo se aplica una convolución a una imagen real. En este caso, vamos a definir filtros que ayudan a detectar bordes. \n",
    "\n",
    "La función [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html) es una función auxiliar que lee una imagen de un archivo y la convierte en un tensor de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_image(str(Path(\"assets\") / \"PrisonMike.png\"), ImageReadMode.GRAY)\n",
    "\n",
    "show_tensor_image(image, title=\"Original Image\", vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los siguientes filtros:\n",
    "\n",
    "- `identity`: un filtro que no modifica la imagen.\n",
    "- `edge`: detecta bordes.\n",
    "- `sharpen`: resalta los bordes.\n",
    "- `blur`: suaviza la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_n = image.to(torch.float32)\n",
    "image_batch = image_n.unsqueeze(0)\n",
    "\n",
    "print(\"Forma de la imagen:\", image_batch.shape)\n",
    "\n",
    "horizontal_edge = torch.tensor([[[[-1, -1, -1], \n",
    "                                  [ 2,  2,  2], \n",
    "                                  [-1, -1, -1]]]], dtype=torch.float32)\n",
    "\n",
    "vertical_edge = torch.tensor([[[[-1,  2, -1], \n",
    "                                [-1,  2, -1], \n",
    "                                [-1,  2, -1]]]], dtype=torch.float32)\n",
    "\n",
    "diagonal_edge = torch.tensor([[[[2, -1, -1], \n",
    "                                [-1, 2, -1], \n",
    "                                [-1, -1, 2]]]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Apilar los kernels para aplicarlos todos a la vez\n",
    "kernels = torch.cat([horizontal_edge, vertical_edge, diagonal_edge])\n",
    "print(\"Forma de los kernels:\", kernels.shape)\n",
    "\n",
    "# Aplicar la convolución usando los kernels definidos\n",
    "output = F.conv2d(image_batch, kernels)\n",
    "print(\"Forma de la salida:\", output.shape)\n",
    "\n",
    "# Mostrar las imágenes resultantes\n",
    "show_tensor_images(\n",
    "    [output[0, i].unsqueeze(0) for i in range(output.shape[1])],\n",
    "    titles=[\"Horizontal\", \"Vertical\", \"Diagonal\"],\n",
    "    figsize=(20, 5),\n",
    "    vmin=0,\n",
    "    vmax=255,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = torch.tensor([[[[0, 0, 0], \n",
    "                           [0, 1, 0], \n",
    "                           [0, 0, 0]]]], dtype=torch.float32)\n",
    "\n",
    "edge = torch.tensor([[[[-1, -1, -1],\n",
    "                       [-1, 8, -1],\n",
    "                       [-1, -1, -1]]]], dtype=torch.float32)\n",
    "\n",
    "sharpen = torch.tensor([[[[0, -1, 0],\n",
    "                          [-1, 5, -1],\n",
    "                          [0, -1, 0]]]], dtype=torch.float32)\n",
    "\n",
    "box_blur = torch.tensor([[[[1, 1, 1],\n",
    "                           [1, 1, 1],\n",
    "                           [1, 1, 1]]]], dtype=torch.float32) / 9\n",
    "\n",
    "gau_blur = torch.tensor([[[[1, 2, 1],\n",
    "                           [2, 4, 2],\n",
    "                           [1, 2, 1]]]], dtype=torch.float32) / 16\n",
    "\n",
    "\n",
    "# Apilar los kernels para aplicarlos todos a la vez\n",
    "kernels = torch.cat([identity, edge, sharpen, box_blur, gau_blur])\n",
    "print(\"Forma de los kernels:\", kernels.shape)\n",
    "\n",
    "# Aplicar la convolución usando los kernels definidos\n",
    "output = F.conv2d(image_batch, kernels)\n",
    "print(\"Forma de la salida:\", output.shape)\n",
    "\n",
    "# Mostrar las imágenes resultantes\n",
    "show_tensor_images(\n",
    "    [output[0, i].unsqueeze(0) for i in range(output.shape[1])],\n",
    "    titles=[\"Identity\", \"Edge Detection\", \"Sharpen\", \"Box Blur\", \"Gaussian Blur\"],\n",
    "    figsize=(20, 5),\n",
    "    vmin=0,\n",
    "    vmax=255,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos combinar\n",
    "output = F.conv2d(image_batch, edge)\n",
    "output = F.conv2d(output, box_blur)\n",
    "output = F.conv2d(output, gau_blur)\n",
    "\n",
    "show_tensor_image(output, vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La buena noticia es que no necesitamos definir estos filtros manualmente, ya que PyTorch proporciona una función [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) que inicializa los pesos de los filtros de manera aleatoria y los entrena durante el proceso de aprendizaje.\n",
    "\n",
    "Esto quiere decir que la red neuronal aprenderá automáticamente los filtros que son útiles para el problema en cuestión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otros parámetros de la convolución\n",
    "\n",
    "Además de los filtros, las convoluciones tienen otros parámetros importantes:\n",
    "\n",
    "- `stride`: paso de la convolución, es decir, cuántos píxeles se desplaza el filtro en cada paso.\n",
    "- `padding`: relleno de la imagen, es decir, cuántos píxeles se añaden alrededor de la imagen. Muchas veces se utiliza para mantener el tamaño de la imagen de salida igual al de la imagen de entrada.\n",
    "- `bias`: si se incluye un término de sesgo en la convolución. \n",
    "\n",
    "\n",
    "Las dimensiones de salida se pueden calcular con la siguiente fórmula:\n",
    "\n",
    "$$\n",
    "\\text{output\\_size} = \\frac{\\text{input\\_size} - \\text{kernel\\_size} + 2 \\times \\text{padding}}{\\text{stride}} + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = F.conv2d(image_batch, identity, padding=30)  # sumamos 30 pixeles de padding\n",
    "show_tensor_image(output, vmin=0, vmax=255)\n",
    "\n",
    "print(f\"Shape de la imagen original: {image_batch.shape}\")\n",
    "print(f\"Shape de la imagen con padding: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = F.conv2d(image_batch, identity, stride=3, padding=5)\n",
    "show_tensor_image(output)\n",
    "\n",
    "print(f\"Shape de la imagen con stride: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = F.conv2d(image_batch, identity, bias=torch.tensor([-100.0]))\n",
    "show_tensor_image(output, vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "El **Pooling** es una operación clave en las redes neuronales convolucionales (CNNs) que se utiliza para reducir las dimensiones espaciales (ancho y alto) de los mapas de características. Esto se logra resumiendo las características en regiones específicas de la imagen de entrada. El pooling ayuda a disminuir la cantidad de parámetros y el costo computacional, y también proporciona cierta invariancia a las traslaciones y deformaciones en la entrada.\n",
    "\n",
    "> Cabe destacar que el pooling no tiene parámetros entrenables, ya que simplemente aplica una función fija a una región de la imagen.\n",
    "\n",
    "#### Tipos de Pooling\n",
    "\n",
    "Existen varios tipos de pooling, pero los más comunes son:\n",
    "\n",
    "1. **Max Pooling**: Selecciona el valor máximo en cada ventana de pooling. Es útil para resaltar las características más prominentes.\n",
    "2. **Average Pooling**: Calcula el promedio de los valores en cada ventana de pooling. Suaviza las características y es menos agresivo que el max pooling.\n",
    "\n",
    "En PyTorch, podemos aplicar pooling utilizando la función [`torch.nn.functional.max_pool2d`](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html) o [`torch.nn.functional.avg_pool2d`](https://pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool2d.html).\n",
    "\n",
    "#### Dimensiones de salida\n",
    "\n",
    "Las dimensiones de salida del pooling se pueden calcular con la siguiente fórmula:\n",
    "\n",
    "$$\n",
    "\\text{output\\_size} = \\frac{\\text{input\\_size} - \\text{kernel\\_size} + 2 \\times \\text{padding}}{\\text{stride}} + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos un pooling en acción\n",
    "max_pool_out = F.max_pool2d(image_batch, kernel_size=3)\n",
    "avg_pool_out = F.avg_pool2d(image_batch, kernel_size=3)\n",
    "\n",
    "show_tensor_images(\n",
    "    [image_n, max_pool_out, avg_pool_out],\n",
    "    titles=[\"Original\", \"Max Pooling\", \"Avg Pooling\"],\n",
    ")\n",
    "\n",
    "print(f\"Shape de la imagen original: {image_batch.shape}\")\n",
    "print(f\"Shape de la imagen con Max Pooling: {max_pool_out.shape}\")\n",
    "print(f\"Shape de la imagen con Avg Pooling: {avg_pool_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "### Dataset FashionMNIST\n",
    "\n",
    "El dataset FashionMNIST es una alternativa al clásico MNIST. En lugar de contener dígitos escritos a mano, FashionMNIST incluye imágenes de artículos de moda divididos en 10 clases, tales como camisetas, zapatos y bolsos. Cada imagen es de 28x28 píxeles en escala de grises, lo que hace que este dataset sea ideal para probar algoritmos de clasificación de imágenes.\n",
    "\n",
    "Para facilitar el manejo del dataset FashionMNIST, utilizaremos la biblioteca `torchvision.datasets`, la cual proporciona una manera sencilla de cargar y preprocesar estos datos.\n",
    "\n",
    "**Cargando el dataset con torchvision**\n",
    "\n",
    "La clase `torchvision.datasets.FashionMNIST` permite descargar y cargar el dataset FashionMNIST de manera eficiente.\n",
    "\n",
    "```python\n",
    "datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    transform=None,\n",
    "    target_transform=None,\n",
    "    download=True\n",
    ")\n",
    "```\n",
    "\n",
    "Algunos de los parámetros más importantes son:\n",
    "\n",
    "- `root`: Directorio donde se almacenarán los datos.\n",
    "- `train`: Si es `True`, carga el conjunto de entrenamiento; si es `False`, carga el conjunto de prueba.\n",
    "- `transform`: Transformaciones que se aplicarán a los datos.\n",
    "- `target_transform`: Transformaciones que se aplicarán a las etiquetas.\n",
    "- `download`: Si es `True`, descarga el dataset desde internet y lo almacena en `root`\n",
    "\n",
    "Existen otros datasets disponibles en `torchvision.datasets`, como CIFAR10, CIFAR100, MNIST, etc. Puedes consultar la [documentación oficial](https://pytorch.org/vision/stable/datasets.html) para más información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Transformaciones\n",
    "\n",
    "Las transformaciones son operaciones que se aplican a los datos antes de ser alimentados a la red neuronal. En este caso, utilizamos `transforms.Compose` para combinar varias transformaciones. Dos transformaciones comunes son:\n",
    "\n",
    "1. [transforms.ToImage](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToImage.html): Convierte los datos (PIL) a un tensor de imagen.\n",
    "2. [transforms.ToDtype](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToDtype.html): Convierte los datos a un tipo de dato específico.\n",
    "\n",
    "Al combinar estas transformaciones, nos aseguramos de que los datos estén en el formato y rango adecuado para ser procesados por la red neuronal.\n",
    "\n",
    "Estas transformaciones se aplican automáticamente cuando cargamos el dataset utilizando la clase `datasets.FashionMNIST` y se especifican en el parámetro `transform`. Para ver más detalles sobre las transformaciones disponibles en PyTorch, puedes consultar la [documentación oficial](https://pytorch.org/vision/0.19/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py).\n",
    "\n",
    "> Algunas transformaciones puede usarse como data augmentation, es decir, para aumentar la cantidad de datos de entrenamiento. Por ejemplo, rotar, recortar, cambiar el brillo, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "transforms = T.Compose(\n",
    "    [\n",
    "        # TODO\n",
    "    ]\n",
    ")\n",
    "\n",
    "fmnist_train_dataset = datasets.FashionMNIST(\n",
    "    DATA_DIR, download=True, train=True, transform=transforms\n",
    ")\n",
    "\n",
    "fmnist_test_dataset = datasets.FashionMNIST(\n",
    "    DATA_DIR, download=True, train=False, transform=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_classes = fmnist_train_dataset.classes\n",
    "nclasses = len(name_classes)\n",
    "\n",
    "print(f\"Clases: {name_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el dataset en conjuntos de entrenamiento para tener un conjunto de validación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_train_dataset, fmnist_val_dataset = random_split(\n",
    "    fmnist_train_dataset, [0.8, 0.2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    train_loader = DataLoader(\n",
    "        fmnist_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        fmnist_val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        fmnist_test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de CNN\n",
    "\n",
    "Igual que con el modelo FeedForward, para crear un modelo usando convoluciones necesitamos crear una clase, definir los metodos **init** y **forward** y especificar la arcquitectura y comportamiento de los componentes del modelo. \n",
    "\n",
    "En particular vamos a usar:\n",
    "\n",
    "- [Capas convolucionales de 2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) a las que tenemos que especificarles la cantidad de canales de entrada (1 para gris, 3 para color y X para el resultado de un filtro anterior), una cantidad de filtros a usar (out_channels), el tamaño de los mismos (kernel_size) y si aplicamos padding (relleno) o no (esto nos permite hacer convoluciones que no modifiquen el tamaño original de las imagenes). \n",
    "\n",
    "- [Capas de maxpooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d) o [avgpooling](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d) a las que tenemos que decirles el tamaño de la ventana a mirar y el largo del paso que deben tomar (stride).\n",
    "\n",
    "- Finalmente tambien haremos uso de capas lineales y ReLUs como hicimos anteriormente.\n",
    "\n",
    "### LeNet\n",
    "\n",
    "Vamos a implementar [LeNet](https://d2l.ai/chapter_convolutional-neural-networks/lenet.html) que es una de las primeras redes neuronales convolucionales que se utilizó en la práctica. Fue propuesta por Yann LeCun en 1998 para el reconocimiento de dígitos escritos a mano. La arquitectura de LeNet es la siguiente:\n",
    "\n",
    "\n",
    "![Image](./assets/LeNet_architecture.png)\n",
    "\n",
    "\n",
    "> Nota: En la arquitectura original de LeNet la entrada es de 32x32 pixeles, pero en este caso vamos a usar imagenes de 28x28 pixeles.\n",
    "\n",
    "https://www.geeksforgeeks.org/lenet-5-architecture/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "\n",
    "summary(LeNet(1, 10), input_size=(BATCH_SIZE, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_calassification_report(letnet_model, val_loader, DEVICE, nclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. **Implementar una CNN más profunda**: Modificar la arquitectura de la red para agregar más capas convolucionales y/o capas totalmente conectadas. ¿Cómo afecta esto al rendimiento del modelo?\n",
    "2. **Weigth and Bias**: Utilizar la librería [Weights and Biases](https://wandb.ai/site) para correr experimentos y comparar diferentes hipermarametros.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taller-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
