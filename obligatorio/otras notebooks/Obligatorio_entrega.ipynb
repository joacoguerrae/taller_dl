{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuuY8rgLHBVl"
      },
      "source": [
        "# Obligatorio - Taller de Deep Learning\n",
        "\n",
        "**Fecha de entrega:** 3/12/2025  \n",
        "**Puntaje m치ximo:** 50 puntos\n",
        "\n",
        "**Alumno(s):** [Nombre(s) y Apellido(s)]\n",
        "\n",
        "## Obligatorio\n",
        "\n",
        "El objetivo de este obligatorio es evaluar su conocimiento en Deep Learning mediante la implementaci칩n completa de un modelo de segmentaci칩n de im치genes basado en el paper [**\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"**](https://arxiv.org/pdf/1505.04597). Toda la implementaci칩n debe realizarse desde cero utilizando PyTorch, y los estudiantes tendr치n la libertad de ajustar ciertos hiperpar치metros y configuraciones mientras mantengan la esencia del paper original.\n",
        "\n",
        "### **Competencia en Kaggle**\n",
        "\n",
        "Adem치s, como parte de este obligatorio, participar치n en una competencia privada en Kaggle donde se les proporcionar치 un dataset de test oculto (sin target). Deber치n subir sus predicciones a Kaggle y se evaluar치n en funci칩n de la m칠trica **Dice Coefficient (Coeficiente de Dice)**. Esta competencia les permitir치 comparar sus resultados con los de sus compa침eros en un entorno real de evaluaci칩n.\n",
        "\n",
        "### **쯈u칠 es el Dice Coefficient?**\n",
        "El **Dice Coefficient**, tambi칠n conocido como F1-score para segmentaci칩n, es una m칠trica utilizada para evaluar la similitud entre la predicci칩n y la verdad del terreno en tareas de segmentaci칩n. Se define de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\\text{Dice} = \\frac{2 \\cdot |A \\cap B|}{|A| + |B|}\n",
        "$$\n",
        "\n",
        "Donde:\n",
        "- $A$ es el conjunto de p칤xeles predichos como pertenecientes a la clase positiva.\n",
        "- $B$ es el conjunto de p칤xeles verdaderos pertenecientes a la clase positiva.\n",
        "- $|A \\cap B|$ es la intersecci칩n de $A$ y $B$, es decir, los p칤xeles correctamente predichos como positivos.\n",
        "\n",
        "Un valor de Dice de **1** indica una predicci칩n perfecta, mientras que un valor de **0** indica que no hay coincidencia entre la predicci칩n y el valor verdadero. Durante la competencia de Kaggle, deber치n obtener un puntaje de al menos **0.75** en la m칠trica Dice para considerarse aprobados.\n",
        "\n",
        "### **Criterios a Evaluar**\n",
        "\n",
        "1. **An치lisis del Dataset (5 puntos):**\n",
        "   - Exploraci칩n y visualizaci칩n del dataset para comprender su estructura y caracter칤sticas.\n",
        "   - Justificaci칩n de las decisiones tomadas en la preprocesamiento de datos, como normalizaci칩n, aumento de datos (data augmentation), y partici칩n del dataset en conjuntos de entrenamiento, validaci칩n y prueba.\n",
        "\n",
        "2. **Implementaci칩n Correcta del Modelo U-Net (20 puntos):**\n",
        "   - Construcci칩n de la arquitectura U-Net siguiendo la estructura descrita en el paper, permitiendo ajustes como el n칰mero de filtros, funciones de activaci칩n y m칠todos de inicializaci칩n de pesos.\n",
        "   - Se aceptan mejoras como el uso de t칠cnicas adicionales como batch normalization, otras funciones de activaci칩n, etc.\n",
        "\n",
        "3. **Entrenamiento del Modelo (10 puntos):**\n",
        "   - Configuraci칩n adecuada del ciclo de entrenamiento, incluyendo la elecci칩n de la funci칩n de p칠rdida y del optimizador (Adam, SGD, etc.).\n",
        "   - Uso de t칠cnicas de regularizaci칩n para mejorar la generalizaci칩n del modelo, como el dropout, normalizaci칩n de batch y data augmentation.\n",
        "   - Gr치ficas y an치lisis de la evoluci칩n del entrenamiento, mostrando las curvas de p칠rdida y m칠tricas relevantes tanto en el conjunto de entrenamiento como en el de validaci칩n.\n",
        "   - Puede utilizarse experimentaci칩n con hiperpar치metros con Weights & Biases (W&B) para optimizar el rendimiento del modelo. Este punto no es obligatorio, pero se valorar치 positivamente si se justifica su uso y se presentan resultados claros.\n",
        "\n",
        "4. **Evaluaci칩n de Resultados (10 puntos):**\n",
        "   - Evaluaci칩n exhaustiva del modelo utilizando m칠tricas de segmentaci칩n como **Dice Coefficient**.\n",
        "   - An치lisis detallado de los resultados, incluyendo un an치lisis de errores para identificar y discutir casos dif칤ciles.\n",
        "   - Visualizaci칩n de ejemplos representativos de segmentaciones correctas e incorrectas, comparando con las etiquetas manuales proporcionadas en el dataset.\n",
        "\n",
        "5. **Participaci칩n y Resultados en la Competencia Kaggle (5 puntos):**\n",
        "   - Participaci칩n activa en la competencia de Kaggle, con al menos una (1) subida de predicci칩n.\n",
        "   - Puntaje obtenido en la tabla de posiciones de Kaggle, evaluado en base al **Dice Coefficient** en el conjunto de test oculto. Es necesario obtener al menos un valor de **0.75** para esta m칠trica.\n",
        "\n",
        "   Notas:\n",
        "   - **Cualquier decisi칩n debe ser justificada en el notebook.**\n",
        "   - El **Dice Coefficient** es la m칠trica utilizada para evaluar la precisi칩n de los modelos de segmentaci칩n de im치genes en esta competencia.\n",
        "\n",
        "### **Run-Length Encoding (RLE)**\n",
        "\n",
        "Dado que no se suben las im치genes segmentadas directamente a Kaggle, se requiere usar **Run-Length Encoding (RLE)** para comprimir las m치scaras de predicci칩n en una cadena de texto que ser치 evaluada. El **RLE** es una t칠cnica de compresi칩n donde se representan secuencias consecutivas de p칤xeles en formato `start length`, indicando la posici칩n de inicio y la longitud de cada secuencia de p칤xeles positivos.\n",
        "\n",
        "Para calcular el **RLE**, se sigue el siguiente proceso:\n",
        "\n",
        "1. Se aplanan las m치scaras predichas en un solo vector\n",
        "2. Se identifican los p칤xeles con valor positivo (1) y se calculan las secuencias consecutivas.\n",
        "3. Se registra la posici칩n de inicio de cada secuencia y su longitud en formato `start length`.\n",
        "\n",
        "Este formato comprimido se sube a Kaggle en lugar de las im치genes segmentadas.\n",
        "\n",
        "#### **Ejemplo de RLE**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def rle_encode(mask):\n",
        "    pixels = np.array(mask).flatten(order='F')  # Aplanar la m치scara en orden Fortran\n",
        "    pixels = np.concatenate([[0], pixels, [0]])  # A침adir ceros al principio y final\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1  # Encontrar transiciones\n",
        "    runs[1::2] = runs[1::2] - runs[::2]  # Calcular longitudes\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "mask = np.array([[0, 0, 1, 0, 0],\n",
        "                 [0, 1, 1, 1, 0],\n",
        "                 [1, 1, 1, 0, 0],\n",
        "                 [0, 0, 0, 1, 1]])\n",
        "\n",
        "print(rle_encode(mask))\n",
        "```\n",
        "\n",
        "> **Salida:** 3 1 6 2 9 3 14 1 16 1 20 1\n",
        "\n",
        "\n",
        "### **Sobre el Dataset**\n",
        "\n",
        "El dataset proporcionado para esta tarea incluir치 im치genes y m치scaras para la segmentaci칩n de un conjunto espec칤fico de clases. El conjunto de entrenamiento estar치 disponible para su uso durante todo el proceso de desarrollo y pruebas, mientras que el conjunto de validaci칩n se mantendr치 oculto para la evaluaci칩n final en Kaggle.\n",
        "\n",
        "### **Instrucciones de Entrega**\n",
        "\n",
        "- Deber치n entregar un Jupyter Notebook (.ipynb) que contenga todo el c칩digo y las explicaciones necesarias para ejecutar la implementaci칩n, el entrenamiento y la evaluaci칩n del modelo.\n",
        "- El notebook debe incluir secciones bien documentadas explicando las decisiones de dise침o del modelo, los experimentos realizados, y los resultados obtenidos.\n",
        "- El c칩digo debe estar escrito de manera clara.\n",
        "- La entrega debe realizarse a trav칠s de la plataforma de gesti칩n de ORT (gestion.ort.edu.uy) antes de la fecha l칤mite.\n",
        "\n",
        "### **Materiales Adicionales**\n",
        "\n",
        "Para facilitar su trabajo, pueden consultar los siguientes recursos:\n",
        "\n",
        "- [U-Net: Convolutional Networks for Biomedical Image Segmentation (paper original)](https://arxiv.org/abs/1505.04597)\n",
        "- [Documentaci칩n de PyTorch](https://pytorch.org/docs/stable/index.html)\n",
        "- [Tutoriales y recursos adicionales en Kaggle](https://www.kaggle.com/)\n",
        "- [Convoluci칩n Transpuesta](https://d2l.ai/chapter_computer-vision/transposed-conv.html)\n",
        "\n",
        "### **Competencia Kaggle**\n",
        "\n",
        "[Link a la competencia Kaggle](https://www.kaggle.com/competitions/tdl-obligatorio-2025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jGEwily5OuPC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from torchinfo import summary\n",
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "from utils import (\n",
        "    train,\n",
        "    model_calassification_report,\n",
        "    plot_taining\n",
        ")\n",
        "\n",
        "from typing import Literal, List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfnJ62DUHBVn"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-hkc4kVCLWom"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dO_q2z1pMGVd"
      },
      "outputs": [],
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y-zEiKbTMG9b"
      },
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0ajH-zyMJ1E",
        "outputId": "6ac05d29-59bd-4a8c-815e-d8e001f812a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (6.33.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (2.32.5)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (80.9.0)\n",
            "Requirement already satisfied: six>=1.10 in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yY64Fi5tOgKq"
      },
      "outputs": [],
      "source": [
        "img_transform = T.Compose([\n",
        "    T.ToImage(),\n",
        "    T.Resize((572, 572)),\n",
        "    T.Grayscale(num_output_channels=1),\n",
        "    T.ToDtype(torch.float32, scale=True)\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "mask_transform = T.Compose([\n",
        "    T.ToImage(),\n",
        "    T.Resize((572, 572)),\n",
        "    T.Grayscale(num_output_channels=1),        # fuerza 1 canal\n",
        "    T.ToDtype(torch.uint8, scale=False),       # NO escalar, mantener 0-1\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jOihWDSCPnWC"
      },
      "outputs": [],
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, img_transform=None,mask_transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.img_transform = img_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        self.images = sorted(os.listdir(images_dir))\n",
        "        self.masks = sorted(os.listdir(masks_dir))\n",
        "\n",
        "        assert len(self.images) == len(self.masks), \"Cantidad distinta de im치genes y m치scaras\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
        "\n",
        "        img  = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")   # 1 canal\n",
        "\n",
        "        if self.img_transform:\n",
        "            img = self.img_transform(img)\n",
        "\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        # Asegurar que la m치scara sea 0/1\n",
        "        mask = (mask > 0).float()\n",
        "\n",
        "        return img, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6dVSLcAwQpUF"
      },
      "outputs": [],
      "source": [
        "train_ds = SegmentationDataset(\n",
        "    \"train/images\",\n",
        "    \"train/masks\",\n",
        "    img_transform=img_transform,\n",
        "    mask_transform=mask_transform\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zCMXdZFPudqg"
      },
      "outputs": [],
      "source": [
        "val_size = int(len(train_ds) * 0.2)\n",
        "train_size = len(train_ds) - val_size\n",
        "\n",
        "\n",
        "train_ds_split, val_ds_split = random_split(\n",
        "    train_ds,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)  # para reproducibilidad\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_TFIVCgkucdm"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds_split, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_ds_split, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sBEi6V5Q_KT",
        "outputId": "e0736bd9-4f1f-4f25-fe78-e4f8000b5333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2133\n"
          ]
        }
      ],
      "source": [
        "print(len(train_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQMbYPvjRbAP",
        "outputId": "f2fa18bc-cac3-4498-f7e9-9cbac4378201"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Image([[[0.0980, 0.1020, 0.0980,  ..., 0.2000, 0.2000, 0.1961],\n",
              "         [0.1059, 0.1020, 0.1020,  ..., 0.1961, 0.2000, 0.2000],\n",
              "         [0.1059, 0.1059, 0.1020,  ..., 0.2000, 0.2000, 0.2000],\n",
              "         ...,\n",
              "         [0.0824, 0.0824, 0.0824,  ..., 0.0824, 0.0824, 0.0824],\n",
              "         [0.0824, 0.0824, 0.0824,  ..., 0.0824, 0.0824, 0.0824],\n",
              "         [0.0824, 0.0824, 0.0824,  ..., 0.0824, 0.0824, 0.0824]]], ),\n",
              " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gAetR4Q9aXWp"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "\n",
        "  def __init__(self, in_ch: int, out_ch: int,\n",
        "               norm: Literal['bn','gn','none'] = 'none',\n",
        "               groups: int = 8,\n",
        "               dropout: float = 0.0):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=(norm=='none'))\n",
        "    self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=(norm=='none'))\n",
        "    #self.norm1 = self._make_norm(norm, out_ch, groups)\n",
        "    #self.norm2 = self._make_norm(norm, out_ch, groups)\n",
        "    #self.drop = nn.Dropout2d(dropout) if dropout and dropout > 0 else nn.Identity()\n",
        "    self.act = nn.ReLU(inplace=True)\n",
        "\n",
        "  @staticmethod\n",
        "  def _make_norm(kind: str, num_ch: int, groups: int):\n",
        "    if kind == 'bn':\n",
        "      return nn.BatchNorm2d(num_ch)\n",
        "    if kind == 'gn':\n",
        "      g = min(groups, num_ch) if num_ch % groups == 0 else 1\n",
        "      return nn.GroupNorm(g, num_ch)\n",
        "    return nn.Identity()\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.conv1(x)\n",
        "    #x = self.norm1(x)\n",
        "    x = self.act(x)\n",
        "    #x = self.drop(x)\n",
        "    x = self.conv2(x)\n",
        "    #x = self.norm2(x)\n",
        "    x = self.act(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g9BrIn2Ta6bL"
      },
      "outputs": [],
      "source": [
        "class Down(nn.Module):\n",
        "#Downscaling with maxpool then double conv\n",
        "  def __init__(self, in_ch: int, out_ch: int,\n",
        "                 norm: str = 'bn',\n",
        "                 groups: int = 8,\n",
        "                 dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.block = DoubleConv(in_ch, out_ch, norm=norm, groups=groups, dropout=dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(x)\n",
        "      x = self.block(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zniVfasLbDVs"
      },
      "outputs": [],
      "source": [
        "class Up(nn.Module):\n",
        "  \"\"\"Upscaling then double conv\n",
        "  If bilinear: use Upsample, else learned ConvTranspose2d\n",
        "  \"\"\"\n",
        "  def __init__(self, in_ch: int, out_ch: int, bilinear: bool = False, norm: str = 'bn', groups: int = 8, dropout: float = 0.0):\n",
        "    super().__init__()\n",
        "    if bilinear:\n",
        "      self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "      self.reduce = nn.Conv2d(in_ch, in_ch // 2, kernel_size=1)\n",
        "      conv_in = in_ch // 2 + out_ch # after concat with skip (which has out_ch channels)\n",
        "    else:\n",
        "      self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
        "      self.reduce = nn.Identity()\n",
        "      conv_in = in_ch // 2 + out_ch\n",
        "    self.block = DoubleConv(conv_in, out_ch, norm=norm, groups=groups, dropout=dropout)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def _pad_to_match(x: torch.Tensor, ref: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Pad x on the right/bottom to match spatial size of ref.\"\"\"\n",
        "    diff_y = ref.size(2) - x.size(2)\n",
        "    diff_x = ref.size(3) - x.size(3)\n",
        "    if diff_x == 0 and diff_y == 0:\n",
        "      return x\n",
        "    return F.pad(x, [0, diff_x, 0, diff_y])\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.up(x)\n",
        "    x = self.reduce(x)\n",
        "    x = self._pad_to_match(x, skip)\n",
        "    # concat along channel dim\n",
        "    x = torch.cat([skip, x], dim=1)\n",
        "    x = self.block(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "REl749q9bOXJ"
      },
      "outputs": [],
      "source": [
        "class OutConv(nn.Module):\n",
        "  def __init__(self, in_ch: int, out_ch: int):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZZVJm8GqnfO",
        "outputId": "c7a8091a-014b-40e0-d1b3-0b25e42a64cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 4):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvHqKV-bsNkW",
        "outputId": "0b9b5f6c-8ae6-4a74-d9f8-009294080c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "2\n",
            "1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "for i in reversed(range(4)):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CWhELE3qshtq"
      },
      "outputs": [],
      "source": [
        "chs = [64*(2 ** i) for i in range(4+1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlAxy4DcslvK",
        "outputId": "3b087b48-8fde-4139-9580-ea49d769cf9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[64, 128, 256, 512, 1024]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WE-ALFGRtuWX"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=1,\n",
        "                 num_classes=1,\n",
        "                 base_ch=64,\n",
        "                 depth=4,              # n칰mero de downs (pools), igual que el paper\n",
        "                 bilinear=True,\n",
        "                 norm='bn',\n",
        "                 dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ej: base_ch=64, depth=4 -> [64, 128, 256, 512, 1024]\n",
        "        # chs[0]..chs[depth] son los canales en cada nivel de resoluci칩n\n",
        "        chs = [base_ch * (2 ** i) for i in range(depth + 1)]\n",
        "\n",
        "        # Encoder\n",
        "        self.inc = DoubleConv(in_channels, chs[0], norm=norm, dropout=dropout)\n",
        "\n",
        "        self.downs = nn.ModuleList()\n",
        "        # Creamos 'depth' downs con maxpool (como el paper)\n",
        "        for i in range(depth):\n",
        "            self.downs.append(\n",
        "                Down(chs[i], chs[i + 1], norm=norm, dropout=dropout)\n",
        "            )\n",
        "\n",
        "        # Decoder\n",
        "        self.ups = nn.ModuleList()\n",
        "        cur_ch = chs[-1]  # canales del nivel m치s profundo (ej 1024)\n",
        "\n",
        "        # Vamos subiendo desde el fondo hasta el tope\n",
        "        # i = depth-1, depth-2, ..., 0  -> skip channels = chs[i]\n",
        "        for i in reversed(range(depth)):\n",
        "            self.ups.append(\n",
        "                Up(cur_ch, chs[i], bilinear=bilinear, norm=norm, dropout=dropout)\n",
        "            )\n",
        "            cur_ch = chs[i]\n",
        "\n",
        "        self.outc = OutConv(cur_ch, num_classes)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "\n",
        "        # Primer nivel (sin pool)\n",
        "        x = self.inc(x)\n",
        "        skips.append(x)\n",
        "\n",
        "        # Encoder con 'depth' downs (cada uno hace pool + convs)\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        # Ahora x est치 en el nivel m치s profundo, mismo que skips[-1]\n",
        "        # No hay bottleneck extra: el 칰ltimo Down ya es el bloque profundo.\n",
        "\n",
        "        # Decoder: empezamos desde la representaci칩n profunda\n",
        "        x = skips.pop()  # nivel m치s profundo\n",
        "\n",
        "        for up in self.ups:\n",
        "            skip = skips.pop()   # skip correspondiente de encoder\n",
        "            x = up(x, skip)\n",
        "\n",
        "        return self.outc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTMWb0rucXfm",
        "outputId": "98bb00b6-a757-417a-89d3-5b63780e1587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images: torch.Size([16, 1, 572, 572]) torch.float32\n",
            "masks: torch.Size([16, 1, 572, 572]) torch.float32\n"
          ]
        }
      ],
      "source": [
        "images, masks = next(iter(train_loader))\n",
        "print(\"images:\", images.shape, images.dtype)  # [B, 3, 256, 256], float32\n",
        "print(\"masks:\", masks.shape, masks.dtype)    # [B, 1, 256, 256], float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Du-8iKUjdTpS"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zul4Kg_rdVLN"
      },
      "outputs": [],
      "source": [
        "model = UNet(\n",
        "    in_channels=1,\n",
        "    num_classes=1,     # salida 1 canal con logits para binario\n",
        "    base_ch=32,        # pod칠s subir a 64 cuando ande todo\n",
        "    depth=4,\n",
        "    bilinear=False,\n",
        "    norm='bn',\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SO1lPGWgeICU"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIaX_T_seK0s",
        "outputId": "76ae0f07-d8bf-42d1-83da-5f7574049dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "outputs: torch.Size([16, 1, 572, 572])\n",
            "loss: 1.5807158946990967\n",
            "Backward OK 九\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "images, masks = next(iter(train_loader))\n",
        "images = images.to(device)\n",
        "masks  = masks.to(device)\n",
        "\n",
        "outputs = model(images)              # [B, 1, 256, 256]\n",
        "print(\"outputs:\", outputs.shape)\n",
        "\n",
        "loss = criterion(outputs, masks)\n",
        "print(\"loss:\", loss.item())\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "print(\"Backward OK 九\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "sJelBK-DfLHy",
        "outputId": "5c2512d5-d4c1-459d-dbb4-8ba76f493bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001 | Train Loss: 0.65928 | Val Loss: 0.64084 | Val Dice: 0.08147\n",
            "Epoch: 002 | Train Loss: 0.61173 | Val Loss: 0.59476 | Val Dice: 0.51913\n",
            "Epoch: 003 | Train Loss: 0.59089 | Val Loss: 0.59959 | Val Dice: 0.32739\n",
            "Epoch: 004 | Train Loss: 0.58596 | Val Loss: 0.58795 | Val Dice: 0.50311\n",
            "Epoch: 005 | Train Loss: 0.57853 | Val Loss: 0.58062 | Val Dice: 0.53801\n",
            "Epoch: 006 | Train Loss: 0.56908 | Val Loss: 0.57480 | Val Dice: 0.55091\n",
            "Epoch: 007 | Train Loss: 0.56523 | Val Loss: 0.54516 | Val Dice: 0.63381\n",
            "Epoch: 008 | Train Loss: 0.53591 | Val Loss: 0.51984 | Val Dice: 0.65963\n",
            "Epoch: 009 | Train Loss: 0.50687 | Val Loss: 0.47109 | Val Dice: 0.69834\n",
            "Epoch: 010 | Train Loss: 0.46180 | Val Loss: 0.43283 | Val Dice: 0.73813\n",
            "Epoch: 011 | Train Loss: 0.42053 | Val Loss: 0.39354 | Val Dice: 0.74992\n",
            "Epoch: 012 | Train Loss: 0.39654 | Val Loss: 0.36505 | Val Dice: 0.76911\n",
            "Epoch: 013 | Train Loss: 0.37970 | Val Loss: 0.34412 | Val Dice: 0.78574\n",
            "Epoch: 014 | Train Loss: 0.35509 | Val Loss: 0.35779 | Val Dice: 0.76740\n",
            "Epoch: 015 | Train Loss: 0.34107 | Val Loss: 0.35465 | Val Dice: 0.78970\n",
            "Epoch: 016 | Train Loss: 0.32003 | Val Loss: 0.30861 | Val Dice: 0.81369\n",
            "Epoch: 017 | Train Loss: 0.29883 | Val Loss: 0.29580 | Val Dice: 0.82038\n",
            "Epoch: 018 | Train Loss: 0.28704 | Val Loss: 0.31496 | Val Dice: 0.81469\n",
            "Epoch: 019 | Train Loss: 0.27591 | Val Loss: 0.35960 | Val Dice: 0.79307\n",
            "Epoch: 020 | Train Loss: 0.25003 | Val Loss: 0.26970 | Val Dice: 0.83085\n",
            "Epoch: 021 | Train Loss: 0.23530 | Val Loss: 0.29409 | Val Dice: 0.80991\n",
            "Epoch: 022 | Train Loss: 0.20325 | Val Loss: 0.28357 | Val Dice: 0.82807\n",
            "Epoch: 023 | Train Loss: 0.18079 | Val Loss: 0.28873 | Val Dice: 0.83493\n",
            "Epoch: 024 | Train Loss: 0.14866 | Val Loss: 0.31822 | Val Dice: 0.81222\n",
            "Epoch: 025 | Train Loss: 0.12183 | Val Loss: 0.33643 | Val Dice: 0.81947\n",
            "Epoch: 026 | Train Loss: 0.09871 | Val Loss: 0.36685 | Val Dice: 0.81504\n",
            "Epoch: 027 | Train Loss: 0.09823 | Val Loss: 0.37771 | Val Dice: 0.83004\n",
            "Epoch: 028 | Train Loss: 0.07415 | Val Loss: 0.38933 | Val Dice: 0.83157\n",
            "Epoch: 029 | Train Loss: 0.04694 | Val Loss: 0.42450 | Val Dice: 0.82968\n",
            "Epoch: 030 | Train Loss: 0.03849 | Val Loss: 0.46730 | Val Dice: 0.83352\n",
            "Detener entrenamiento en la 칠poca 29, la mejor p칠rdida fue 0.26970\n"
          ]
        }
      ],
      "source": [
        "train_loss, val_loss, val_dice = train(model,optimizer,criterion,train_loader,val_loader,device, epochs=100, patience=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Guardado de modelo entrenado\n",
        "#torch.save(model.state_dict(), 'unet_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "del model  # si ya lo ten칤as creado\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\joaco\\AppData\\Local\\Temp\\ipykernel_22132\\3496321983.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('unet_model.pth',map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (inc): DoubleConv(\n",
              "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (act): ReLU(inplace=True)\n",
              "  )\n",
              "  (downs): ModuleList(\n",
              "    (0): Down(\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (block): DoubleConv(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (act): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Down(\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (block): DoubleConv(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (act): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (2): Down(\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (block): DoubleConv(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (act): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (3): Down(\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (block): DoubleConv(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (act): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ups): ModuleList(\n",
              "    (0): Up(\n",
              "      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (reduce): Identity()\n",
              "      (block): DoubleConv(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (act): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Up(\n",
              "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (reduce): Identity()\n",
              "      (block): DoubleConv(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (act): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (2): Up(\n",
              "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (reduce): Identity()\n",
              "      (block): DoubleConv(\n",
              "        (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (act): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (3): Up(\n",
              "      (up): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (reduce): Identity()\n",
              "      (block): DoubleConv(\n",
              "        (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (act): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (outc): OutConv(\n",
              "    (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Carga del modelo entrenado en otro nombre de variable\n",
        "model = UNet(\n",
        "    in_channels=1,\n",
        "    num_classes=1,     # salida 1 canal con logits para binario\n",
        "    base_ch=32,        # pod칠s subir a 64 cuando ande todo\n",
        "    depth=4,\n",
        "    bilinear=False,\n",
        "    norm='bn',\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load('unet_model.pth',map_location=torch.device('cpu')))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creamos la clase que nos permita cargar test dataset\n",
        "class SegmentationTestDataset(Dataset):\n",
        "    def __init__(self, images_dir, img_transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.img_transform = img_transform\n",
        "\n",
        "        self.images = sorted(os.listdir(images_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "\n",
        "        img  = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.img_transform:\n",
        "            img = self.img_transform(img)\n",
        "\n",
        "        return img, self.images[idx]  # retornamos tambi칠n el nombre del archivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instanciamos el dataset y dataloader de test\n",
        "test_ds = SegmentationTestDataset(\n",
        "    \"test/images\",\n",
        "    img_transform=img_transform\n",
        ")\n",
        "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "TEST_IMAGES_DIR = \"test/images\"  # ajust치 si tu carpeta es distinta\n",
        "\n",
        "def resize_mask_to_original(pred_mask, img_name):\n",
        "    \"\"\"\n",
        "    pred_mask: m치scara [H, W] en 0/1 (572x572)\n",
        "    img_name: nombre del archivo en test/images\n",
        "    \"\"\"\n",
        "    # Abrimos la imagen original para conocer el tama침o real (p.ej. 800x800)\n",
        "    img_path = os.path.join(TEST_IMAGES_DIR, img_name)\n",
        "    with Image.open(img_path) as im:\n",
        "        w, h = im.size  # PIL da (width, height)\n",
        "\n",
        "    # Convertimos la m치scara a imagen 0-255\n",
        "    mask_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n",
        "\n",
        "    # Redimensionamos al tama침o original usando NEAREST (no inventa grises)\n",
        "    mask_img = mask_img.resize((w, h), resample=Image.NEAREST)\n",
        "\n",
        "    # Volvemos a numpy 0/1\n",
        "    mask_resized = (np.array(mask_img) > 0).astype(np.uint8)  # [h, w]\n",
        "    return mask_resized\n",
        "\n",
        "def rle_encode(mask):\n",
        "    \"\"\"\n",
        "    mask: array 2D (H, W) con 0/1\n",
        "    Devuelve string RLE en formato Kaggle, flatten(order='F')\n",
        "    \"\"\"\n",
        "    pixels = mask.flatten(order='F')  # columna a columna\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] = runs[1::2] - runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submission.csv guardado\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "submission = []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_names in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)  # [B, 1, 572, 572]\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        preds = (probs > 0.5).float()\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            img_name = img_names[i]\n",
        "            pred_mask_572 = preds[i, 0].cpu().numpy()  # [572, 572]\n",
        "\n",
        "            # 1) Reescalar al tama침o original (800x800)\n",
        "            pred_mask_orig = resize_mask_to_original(pred_mask_572, img_name)  # [H, W] original\n",
        "\n",
        "            # 2) RLE en formato Kaggle (orden Fortran)\n",
        "            rle_str = rle_encode(pred_mask_orig)\n",
        "\n",
        "            submission.append({\n",
        "                'id': img_name,      # 游녣 revis치 si Kaggle quiere \"xxx.png\" o solo \"xxx\"\n",
        "                'rle_mask': rle_str\n",
        "            })\n",
        "\n",
        "submission_df = pd.DataFrame(submission)\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "print(\"submission.csv guardado\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "iagenerativa",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
