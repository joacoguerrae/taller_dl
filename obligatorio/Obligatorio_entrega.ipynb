{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuuY8rgLHBVl"
      },
      "source": [
        "# Obligatorio - Taller de Deep Learning\n",
        "\n",
        "**Fecha de entrega:** 3/12/2025  \n",
        "**Puntaje máximo:** 50 puntos\n",
        "\n",
        "**Alumno(s):** [Nombre(s) y Apellido(s)]\n",
        "\n",
        "## Obligatorio\n",
        "\n",
        "El objetivo de este obligatorio es evaluar su conocimiento en Deep Learning mediante la implementación completa de un modelo de segmentación de imágenes basado en el paper [**\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"**](https://arxiv.org/pdf/1505.04597). Toda la implementación debe realizarse desde cero utilizando PyTorch, y los estudiantes tendrán la libertad de ajustar ciertos hiperparámetros y configuraciones mientras mantengan la esencia del paper original.\n",
        "\n",
        "### **Competencia en Kaggle**\n",
        "\n",
        "Además, como parte de este obligatorio, participarán en una competencia privada en Kaggle donde se les proporcionará un dataset de test oculto (sin target). Deberán subir sus predicciones a Kaggle y se evaluarán en función de la métrica **Dice Coefficient (Coeficiente de Dice)**. Esta competencia les permitirá comparar sus resultados con los de sus compañeros en un entorno real de evaluación.\n",
        "\n",
        "### **¿Qué es el Dice Coefficient?**\n",
        "El **Dice Coefficient**, también conocido como F1-score para segmentación, es una métrica utilizada para evaluar la similitud entre la predicción y la verdad del terreno en tareas de segmentación. Se define de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\\text{Dice} = \\frac{2 \\cdot |A \\cap B|}{|A| + |B|}\n",
        "$$\n",
        "\n",
        "Donde:\n",
        "- $A$ es el conjunto de píxeles predichos como pertenecientes a la clase positiva.\n",
        "- $B$ es el conjunto de píxeles verdaderos pertenecientes a la clase positiva.\n",
        "- $|A \\cap B|$ es la intersección de $A$ y $B$, es decir, los píxeles correctamente predichos como positivos.\n",
        "\n",
        "Un valor de Dice de **1** indica una predicción perfecta, mientras que un valor de **0** indica que no hay coincidencia entre la predicción y el valor verdadero. Durante la competencia de Kaggle, deberán obtener un puntaje de al menos **0.75** en la métrica Dice para considerarse aprobados.\n",
        "\n",
        "### **Criterios a Evaluar**\n",
        "\n",
        "1. **Análisis del Dataset (5 puntos):**\n",
        "   - Exploración y visualización del dataset para comprender su estructura y características.\n",
        "   - Justificación de las decisiones tomadas en la preprocesamiento de datos, como normalización, aumento de datos (data augmentation), y partición del dataset en conjuntos de entrenamiento, validación y prueba.\n",
        "\n",
        "2. **Implementación Correcta del Modelo U-Net (20 puntos):**\n",
        "   - Construcción de la arquitectura U-Net siguiendo la estructura descrita en el paper, permitiendo ajustes como el número de filtros, funciones de activación y métodos de inicialización de pesos.\n",
        "   - Se aceptan mejoras como el uso de técnicas adicionales como batch normalization, otras funciones de activación, etc.\n",
        "\n",
        "3. **Entrenamiento del Modelo (10 puntos):**\n",
        "   - Configuración adecuada del ciclo de entrenamiento, incluyendo la elección de la función de pérdida y del optimizador (Adam, SGD, etc.).\n",
        "   - Uso de técnicas de regularización para mejorar la generalización del modelo, como el dropout, normalización de batch y data augmentation.\n",
        "   - Gráficas y análisis de la evolución del entrenamiento, mostrando las curvas de pérdida y métricas relevantes tanto en el conjunto de entrenamiento como en el de validación.\n",
        "   - Puede utilizarse experimentación con hiperparámetros con Weights & Biases (W&B) para optimizar el rendimiento del modelo. Este punto no es obligatorio, pero se valorará positivamente si se justifica su uso y se presentan resultados claros.\n",
        "\n",
        "4. **Evaluación de Resultados (10 puntos):**\n",
        "   - Evaluación exhaustiva del modelo utilizando métricas de segmentación como **Dice Coefficient**.\n",
        "   - Análisis detallado de los resultados, incluyendo un análisis de errores para identificar y discutir casos difíciles.\n",
        "   - Visualización de ejemplos representativos de segmentaciones correctas e incorrectas, comparando con las etiquetas manuales proporcionadas en el dataset.\n",
        "\n",
        "5. **Participación y Resultados en la Competencia Kaggle (5 puntos):**\n",
        "   - Participación activa en la competencia de Kaggle, con al menos una (1) subida de predicción.\n",
        "   - Puntaje obtenido en la tabla de posiciones de Kaggle, evaluado en base al **Dice Coefficient** en el conjunto de test oculto. Es necesario obtener al menos un valor de **0.75** para esta métrica.\n",
        "\n",
        "   Notas:\n",
        "   - **Cualquier decisión debe ser justificada en el notebook.**\n",
        "   - El **Dice Coefficient** es la métrica utilizada para evaluar la precisión de los modelos de segmentación de imágenes en esta competencia.\n",
        "\n",
        "### **Run-Length Encoding (RLE)**\n",
        "\n",
        "Dado que no se suben las imágenes segmentadas directamente a Kaggle, se requiere usar **Run-Length Encoding (RLE)** para comprimir las máscaras de predicción en una cadena de texto que será evaluada. El **RLE** es una técnica de compresión donde se representan secuencias consecutivas de píxeles en formato `start length`, indicando la posición de inicio y la longitud de cada secuencia de píxeles positivos.\n",
        "\n",
        "Para calcular el **RLE**, se sigue el siguiente proceso:\n",
        "\n",
        "1. Se aplanan las máscaras predichas en un solo vector\n",
        "2. Se identifican los píxeles con valor positivo (1) y se calculan las secuencias consecutivas.\n",
        "3. Se registra la posición de inicio de cada secuencia y su longitud en formato `start length`.\n",
        "\n",
        "Este formato comprimido se sube a Kaggle en lugar de las imágenes segmentadas.\n",
        "\n",
        "#### **Ejemplo de RLE**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def rle_encode(mask):\n",
        "    pixels = np.array(mask).flatten(order='F')  # Aplanar la máscara en orden Fortran\n",
        "    pixels = np.concatenate([[0], pixels, [0]])  # Añadir ceros al principio y final\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1  # Encontrar transiciones\n",
        "    runs[1::2] = runs[1::2] - runs[::2]  # Calcular longitudes\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "mask = np.array([[0, 0, 1, 0, 0],\n",
        "                 [0, 1, 1, 1, 0],\n",
        "                 [1, 1, 1, 0, 0],\n",
        "                 [0, 0, 0, 1, 1]])\n",
        "\n",
        "print(rle_encode(mask))\n",
        "```\n",
        "\n",
        "> **Salida:** 3 1 6 2 9 3 14 1 16 1 20 1\n",
        "\n",
        "\n",
        "### **Sobre el Dataset**\n",
        "\n",
        "El dataset proporcionado para esta tarea incluirá imágenes y máscaras para la segmentación de un conjunto específico de clases. El conjunto de entrenamiento estará disponible para su uso durante todo el proceso de desarrollo y pruebas, mientras que el conjunto de validación se mantendrá oculto para la evaluación final en Kaggle.\n",
        "\n",
        "### **Instrucciones de Entrega**\n",
        "\n",
        "- Deberán entregar un Jupyter Notebook (.ipynb) que contenga todo el código y las explicaciones necesarias para ejecutar la implementación, el entrenamiento y la evaluación del modelo.\n",
        "- El notebook debe incluir secciones bien documentadas explicando las decisiones de diseño del modelo, los experimentos realizados, y los resultados obtenidos.\n",
        "- El código debe estar escrito de manera clara.\n",
        "- La entrega debe realizarse a través de la plataforma de gestión de ORT (gestion.ort.edu.uy) antes de la fecha límite.\n",
        "\n",
        "### **Materiales Adicionales**\n",
        "\n",
        "Para facilitar su trabajo, pueden consultar los siguientes recursos:\n",
        "\n",
        "- [U-Net: Convolutional Networks for Biomedical Image Segmentation (paper original)](https://arxiv.org/abs/1505.04597)\n",
        "- [Documentación de PyTorch](https://pytorch.org/docs/stable/index.html)\n",
        "- [Tutoriales y recursos adicionales en Kaggle](https://www.kaggle.com/)\n",
        "- [Convolución Transpuesta](https://d2l.ai/chapter_computer-vision/transposed-conv.html)\n",
        "\n",
        "### **Competencia Kaggle**\n",
        "\n",
        "[Link a la competencia Kaggle](https://www.kaggle.com/competitions/tdl-obligatorio-2025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jGEwily5OuPC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from torchinfo import summary\n",
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "from utils import (\n",
        "    train,\n",
        "    model_calassification_report,\n",
        "    plot_taining\n",
        ")\n",
        "\n",
        "from typing import Literal, List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfnJ62DUHBVn"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-hkc4kVCLWom"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dO_q2z1pMGVd"
      },
      "outputs": [],
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y-zEiKbTMG9b"
      },
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0ajH-zyMJ1E",
        "outputId": "6ac05d29-59bd-4a8c-815e-d8e001f812a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (6.33.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (2.32.5)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (80.9.0)\n",
            "Requirement already satisfied: six>=1.10 in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /home/rami/.conda/envs/Taller_DL_fixed/lib/python3.10/site-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yY64Fi5tOgKq"
      },
      "outputs": [],
      "source": [
        "img_transform = T.Compose([\n",
        "    T.ToImage(),\n",
        "    T.Resize((572, 572)),\n",
        "    T.Grayscale(num_output_channels=1),\n",
        "    T.ToDtype(torch.float32, scale=True)\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "mask_transform = T.Compose([\n",
        "    T.ToImage(),\n",
        "    T.Resize((572, 572)),\n",
        "    T.Grayscale(num_output_channels=1),        # fuerza 1 canal\n",
        "    T.ToDtype(torch.uint8, scale=False),       # NO escalar, mantener 0-1\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jOihWDSCPnWC"
      },
      "outputs": [],
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, img_transform=None,mask_transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.img_transform = img_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        self.images = sorted(os.listdir(images_dir))\n",
        "        self.masks = sorted(os.listdir(masks_dir))\n",
        "\n",
        "        assert len(self.images) == len(self.masks), \"Cantidad distinta de imágenes y máscaras\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
        "\n",
        "        img  = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")   # 1 canal\n",
        "\n",
        "        if self.img_transform:\n",
        "            img = self.img_transform(img)\n",
        "\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        # Asegurar que la máscara sea 0/1\n",
        "        mask = (mask > 0).float()\n",
        "\n",
        "        return img, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6dVSLcAwQpUF"
      },
      "outputs": [],
      "source": [
        "train_ds = SegmentationDataset(\n",
        "    \"train/images\",\n",
        "    \"train/masks\",\n",
        "    img_transform=img_transform,\n",
        "    mask_transform=mask_transform\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zCMXdZFPudqg"
      },
      "outputs": [],
      "source": [
        "val_size = int(len(train_ds) * 0.2)\n",
        "train_size = len(train_ds) - val_size\n",
        "\n",
        "\n",
        "train_ds_split, val_ds_split = random_split(\n",
        "    train_ds,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)  # para reproducibilidad\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_TFIVCgkucdm"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds_split, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_ds_split, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sBEi6V5Q_KT",
        "outputId": "e0736bd9-4f1f-4f25-fe78-e4f8000b5333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2133\n"
          ]
        }
      ],
      "source": [
        "print(len(train_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQMbYPvjRbAP",
        "outputId": "f2fa18bc-cac3-4498-f7e9-9cbac4378201"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Image([[[0.0980, 0.1020, 0.1020,  ..., 0.2000, 0.2000, 0.1961],\n",
              "         [0.1059, 0.1020, 0.1020,  ..., 0.1961, 0.2000, 0.2000],\n",
              "         [0.1059, 0.1059, 0.1020,  ..., 0.2000, 0.2000, 0.2000],\n",
              "         ...,\n",
              "         [0.0824, 0.0824, 0.0824,  ..., 0.0824, 0.0824, 0.0824],\n",
              "         [0.0824, 0.0824, 0.0824,  ..., 0.0824, 0.0824, 0.0824],\n",
              "         [0.0824, 0.0824, 0.0824,  ..., 0.0824, 0.0824, 0.0824]]], ),\n",
              " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gAetR4Q9aXWp"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "\n",
        "  def __init__(self, in_ch: int, out_ch: int,\n",
        "               norm: Literal['bn','gn','none'] = 'none',\n",
        "               groups: int = 8,\n",
        "               dropout: float = 0.0):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=(norm=='none'))\n",
        "    self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=(norm=='none'))\n",
        "    #self.norm1 = self._make_norm(norm, out_ch, groups)\n",
        "    #self.norm2 = self._make_norm(norm, out_ch, groups)\n",
        "    #self.drop = nn.Dropout2d(dropout) if dropout and dropout > 0 else nn.Identity()\n",
        "    self.act = nn.ReLU(inplace=True)\n",
        "\n",
        "  @staticmethod\n",
        "  def _make_norm(kind: str, num_ch: int, groups: int):\n",
        "    if kind == 'bn':\n",
        "      return nn.BatchNorm2d(num_ch)\n",
        "    if kind == 'gn':\n",
        "      g = min(groups, num_ch) if num_ch % groups == 0 else 1\n",
        "      return nn.GroupNorm(g, num_ch)\n",
        "    return nn.Identity()\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.conv1(x)\n",
        "    #x = self.norm1(x)\n",
        "    x = self.act(x)\n",
        "    #x = self.drop(x)\n",
        "    x = self.conv2(x)\n",
        "    #x = self.norm2(x)\n",
        "    x = self.act(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g9BrIn2Ta6bL"
      },
      "outputs": [],
      "source": [
        "class Down(nn.Module):\n",
        "#Downscaling with maxpool then double conv\n",
        "  def __init__(self, in_ch: int, out_ch: int,\n",
        "                 norm: str = 'bn',\n",
        "                 groups: int = 8,\n",
        "                 dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.block = DoubleConv(in_ch, out_ch, norm=norm, groups=groups, dropout=dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(x)\n",
        "      x = self.block(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zniVfasLbDVs"
      },
      "outputs": [],
      "source": [
        "class Up(nn.Module):\n",
        "  \"\"\"Upscaling then double conv\n",
        "  If bilinear: use Upsample, else learned ConvTranspose2d\n",
        "  \"\"\"\n",
        "  def __init__(self, in_ch: int, out_ch: int, bilinear: bool = False, norm: str = 'bn', groups: int = 8, dropout: float = 0.0):\n",
        "    super().__init__()\n",
        "    if bilinear:\n",
        "      self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "      self.reduce = nn.Conv2d(in_ch, in_ch // 2, kernel_size=1)\n",
        "      conv_in = in_ch // 2 + out_ch # after concat with skip (which has out_ch channels)\n",
        "    else:\n",
        "      self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
        "      self.reduce = nn.Identity()\n",
        "      conv_in = in_ch // 2 + out_ch\n",
        "    self.block = DoubleConv(conv_in, out_ch, norm=norm, groups=groups, dropout=dropout)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def _pad_to_match(x: torch.Tensor, ref: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Pad x on the right/bottom to match spatial size of ref.\"\"\"\n",
        "    diff_y = ref.size(2) - x.size(2)\n",
        "    diff_x = ref.size(3) - x.size(3)\n",
        "    if diff_x == 0 and diff_y == 0:\n",
        "      return x\n",
        "    return F.pad(x, [0, diff_x, 0, diff_y])\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.up(x)\n",
        "    x = self.reduce(x)\n",
        "    x = self._pad_to_match(x, skip)\n",
        "    # concat along channel dim\n",
        "    x = torch.cat([skip, x], dim=1)\n",
        "    x = self.block(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "REl749q9bOXJ"
      },
      "outputs": [],
      "source": [
        "class OutConv(nn.Module):\n",
        "  def __init__(self, in_ch: int, out_ch: int):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZZVJm8GqnfO",
        "outputId": "c7a8091a-014b-40e0-d1b3-0b25e42a64cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 4):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvHqKV-bsNkW",
        "outputId": "0b9b5f6c-8ae6-4a74-d9f8-009294080c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "2\n",
            "1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "for i in reversed(range(4)):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CWhELE3qshtq"
      },
      "outputs": [],
      "source": [
        "chs = [64*(2 ** i) for i in range(4+1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlAxy4DcslvK",
        "outputId": "3b087b48-8fde-4139-9580-ea49d769cf9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[64, 128, 256, 512, 1024]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WE-ALFGRtuWX"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=1,\n",
        "                 num_classes=1,\n",
        "                 base_ch=64,\n",
        "                 depth=4,              # número de downs (pools), igual que el paper\n",
        "                 bilinear=True,\n",
        "                 norm='bn',\n",
        "                 dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ej: base_ch=64, depth=4 -> [64, 128, 256, 512, 1024]\n",
        "        # chs[0]..chs[depth] son los canales en cada nivel de resolución\n",
        "        chs = [base_ch * (2 ** i) for i in range(depth + 1)]\n",
        "\n",
        "        # Encoder\n",
        "        self.inc = DoubleConv(in_channels, chs[0], norm=norm, dropout=dropout)\n",
        "\n",
        "        self.downs = nn.ModuleList()\n",
        "        # Creamos 'depth' downs con maxpool (como el paper)\n",
        "        for i in range(depth):\n",
        "            self.downs.append(\n",
        "                Down(chs[i], chs[i + 1], norm=norm, dropout=dropout)\n",
        "            )\n",
        "\n",
        "        # Decoder\n",
        "        self.ups = nn.ModuleList()\n",
        "        cur_ch = chs[-1]  # canales del nivel más profundo (ej 1024)\n",
        "\n",
        "        # Vamos subiendo desde el fondo hasta el tope\n",
        "        # i = depth-1, depth-2, ..., 0  -> skip channels = chs[i]\n",
        "        for i in reversed(range(depth)):\n",
        "            self.ups.append(\n",
        "                Up(cur_ch, chs[i], bilinear=bilinear, norm=norm, dropout=dropout)\n",
        "            )\n",
        "            cur_ch = chs[i]\n",
        "\n",
        "        self.outc = OutConv(cur_ch, num_classes)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "\n",
        "        # Primer nivel (sin pool)\n",
        "        x = self.inc(x)\n",
        "        skips.append(x)\n",
        "\n",
        "        # Encoder con 'depth' downs (cada uno hace pool + convs)\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        # Ahora x está en el nivel más profundo, mismo que skips[-1]\n",
        "        # No hay bottleneck extra: el último Down ya es el bloque profundo.\n",
        "\n",
        "        # Decoder: empezamos desde la representación profunda\n",
        "        x = skips.pop()  # nivel más profundo\n",
        "\n",
        "        for up in self.ups:\n",
        "            skip = skips.pop()   # skip correspondiente de encoder\n",
        "            x = up(x, skip)\n",
        "\n",
        "        return self.outc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTMWb0rucXfm",
        "outputId": "98bb00b6-a757-417a-89d3-5b63780e1587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images: torch.Size([16, 1, 572, 572]) torch.float32\n",
            "masks: torch.Size([16, 1, 572, 572]) torch.float32\n"
          ]
        }
      ],
      "source": [
        "images, masks = next(iter(train_loader))\n",
        "print(\"images:\", images.shape, images.dtype)  # [B, 3, 256, 256], float32\n",
        "print(\"masks:\", masks.shape, masks.dtype)    # [B, 1, 256, 256], float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Du-8iKUjdTpS"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zul4Kg_rdVLN"
      },
      "outputs": [],
      "source": [
        "model = UNet(\n",
        "    in_channels=1,\n",
        "    num_classes=1,     # salida 1 canal con logits para binario\n",
        "    base_ch=32,        # podés subir a 64 cuando ande todo\n",
        "    depth=4,\n",
        "    bilinear=False,\n",
        "    norm='bn',\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SO1lPGWgeICU"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIaX_T_seK0s",
        "outputId": "76ae0f07-d8bf-42d1-83da-5f7574049dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "outputs: torch.Size([16, 1, 572, 572])\n",
            "loss: 0.6887622475624084\n",
            "Backward OK ✅\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "images, masks = next(iter(train_loader))\n",
        "images = images.to(device)\n",
        "masks  = masks.to(device)\n",
        "\n",
        "outputs = model(images)              # [B, 1, 256, 256]\n",
        "print(\"outputs:\", outputs.shape)\n",
        "\n",
        "loss = criterion(outputs, masks)\n",
        "print(\"loss:\", loss.item())\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "print(\"Backward OK ✅\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "sJelBK-DfLHy",
        "outputId": "5c2512d5-d4c1-459d-dbb4-8ba76f493bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001 | Train Loss: 0.65928 | Val Loss: 0.64084 | Val Dice: 0.08147\n",
            "Epoch: 002 | Train Loss: 0.61173 | Val Loss: 0.59476 | Val Dice: 0.51913\n",
            "Epoch: 003 | Train Loss: 0.59089 | Val Loss: 0.59959 | Val Dice: 0.32739\n",
            "Epoch: 004 | Train Loss: 0.58596 | Val Loss: 0.58795 | Val Dice: 0.50311\n",
            "Epoch: 005 | Train Loss: 0.57853 | Val Loss: 0.58062 | Val Dice: 0.53801\n",
            "Epoch: 006 | Train Loss: 0.56908 | Val Loss: 0.57480 | Val Dice: 0.55091\n",
            "Epoch: 007 | Train Loss: 0.56523 | Val Loss: 0.54516 | Val Dice: 0.63381\n",
            "Epoch: 008 | Train Loss: 0.53591 | Val Loss: 0.51984 | Val Dice: 0.65963\n",
            "Epoch: 009 | Train Loss: 0.50687 | Val Loss: 0.47109 | Val Dice: 0.69834\n",
            "Epoch: 010 | Train Loss: 0.46180 | Val Loss: 0.43283 | Val Dice: 0.73813\n",
            "Epoch: 011 | Train Loss: 0.42053 | Val Loss: 0.39354 | Val Dice: 0.74992\n",
            "Epoch: 012 | Train Loss: 0.39654 | Val Loss: 0.36505 | Val Dice: 0.76911\n",
            "Epoch: 013 | Train Loss: 0.37970 | Val Loss: 0.34412 | Val Dice: 0.78574\n",
            "Epoch: 014 | Train Loss: 0.35509 | Val Loss: 0.35779 | Val Dice: 0.76740\n",
            "Epoch: 015 | Train Loss: 0.34107 | Val Loss: 0.35465 | Val Dice: 0.78970\n",
            "Epoch: 016 | Train Loss: 0.32003 | Val Loss: 0.30861 | Val Dice: 0.81369\n",
            "Epoch: 017 | Train Loss: 0.29883 | Val Loss: 0.29580 | Val Dice: 0.82038\n",
            "Epoch: 018 | Train Loss: 0.28704 | Val Loss: 0.31496 | Val Dice: 0.81469\n",
            "Epoch: 019 | Train Loss: 0.27591 | Val Loss: 0.35960 | Val Dice: 0.79307\n",
            "Epoch: 020 | Train Loss: 0.25003 | Val Loss: 0.26970 | Val Dice: 0.83085\n",
            "Epoch: 021 | Train Loss: 0.23530 | Val Loss: 0.29409 | Val Dice: 0.80991\n",
            "Epoch: 022 | Train Loss: 0.20325 | Val Loss: 0.28357 | Val Dice: 0.82807\n",
            "Epoch: 023 | Train Loss: 0.18079 | Val Loss: 0.28873 | Val Dice: 0.83493\n",
            "Epoch: 024 | Train Loss: 0.14866 | Val Loss: 0.31822 | Val Dice: 0.81222\n",
            "Epoch: 025 | Train Loss: 0.12183 | Val Loss: 0.33643 | Val Dice: 0.81947\n",
            "Epoch: 026 | Train Loss: 0.09871 | Val Loss: 0.36685 | Val Dice: 0.81504\n",
            "Epoch: 027 | Train Loss: 0.09823 | Val Loss: 0.37771 | Val Dice: 0.83004\n",
            "Epoch: 028 | Train Loss: 0.07415 | Val Loss: 0.38933 | Val Dice: 0.83157\n",
            "Epoch: 029 | Train Loss: 0.04694 | Val Loss: 0.42450 | Val Dice: 0.82968\n",
            "Epoch: 030 | Train Loss: 0.03849 | Val Loss: 0.46730 | Val Dice: 0.83352\n",
            "Detener entrenamiento en la época 29, la mejor pérdida fue 0.26970\n"
          ]
        }
      ],
      "source": [
        "train_loss, val_loss, val_dice = train(model,optimizer,criterion,train_loader,val_loader,device, epochs=100, patience=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Guardado de modelo entrenado\n",
        "torch.save(model.state_dict(), 'unet_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JTnBNCzOekRB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "del model  # si ya lo tenías creado\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Taller_DL_fixed",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
