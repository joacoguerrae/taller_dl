{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Model (Seq2Seq) para Traducción de Idiomas\n",
    "\n",
    "En esta notebook, exploraremos la implementación de un modelo Sequence-to-Sequence (Seq2Seq) utilizando PyTorch. Este tipo de modelos es ampliamente utilizado en tareas de Procesamiento de Lenguaje Natural (NLP) que requieren el manejo de secuencias de texto de longitud variable, como la traducción automática, el resumen de textos y la generación de lenguaje natural.\n",
    "\n",
    "## Introducción\n",
    "\n",
    "### Objetivos\n",
    "\n",
    "1. **Entender el funcionamiento de un modelo Seq2Seq** y cómo se aplica en la traducción automática de idiomas.\n",
    "2. **Implementar el pipeline completo de un modelo Seq2Seq en PyTorch**, desde la preparación de los datos hasta el entrenamiento y evaluación del modelo.\n",
    "3. **Explorar el uso del `teacher forcing`** para mejorar la eficiencia en el entrenamiento de redes neuronales recurrentes.\n",
    "\n",
    "### Contenido\n",
    "\n",
    "1. Introducción a los modelos Sequence-to-Sequence (Seq2Seq) y su relevancia en tareas de traducción automática.\n",
    "2. Preparación de datos textuales para alimentar a un modelo Seq2Seq.\n",
    "3. Implementación de un **encoder-decoder** utilizando LSTM para la traducción de secuencias.\n",
    "4. Aplicación de **teacher forcing** durante el entrenamiento para mejorar el rendimiento.\n",
    "5. Evaluación del modelo y generación de traducciones en el conjunto de prueba.\n",
    "\n",
    "### Concepto de Seq2Seq\n",
    "\n",
    "Un modelo Seq2Seq consiste en dos partes principales: un **encoder** y un **decoder**. El encoder procesa la secuencia de entrada y la comprime en un vector de estado oculto, que luego es utilizado por el decoder para generar la secuencia de salida.\n",
    "\n",
    "- **Encoder**: Procesa la secuencia de entrada palabra por palabra y genera un conjunto de estados ocultos que resumen el contenido de la secuencia.\n",
    "- **Decoder**: Utiliza el estado final del encoder para generar la secuencia de salida, también palabra por palabra.\n",
    "\n",
    "Este enfoque es particularmente útil en tareas de traducción automática, donde queremos convertir una oración en un idioma a una oración equivalente en otro idioma.\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_7.47.32_PM.png\"/>\n",
    "\n",
    "### Dataset de Traducción\n",
    "\n",
    "Para esta notebook, utilizaremos un dataset de traducción inglés-español que contiene pares de frases. El objetivo es traducir frases de inglés a español, simulando un escenario de traducción automática. El dataset se compone de pares de frases cortas en ambos idiomas y servirá como base para entrenar el modelo Seq2Seq.\n",
    "\n",
    "El mismo se encuentra disponible en el siguiente enlace: [Link al dataset](https://tatoeba.org/en/downloads)\n",
    "\n",
    "### Referencias\n",
    "\n",
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) - Sutskever et al. (2014)\n",
    "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078) - Cho et al. (2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la semilla para que los resultados sean reproducibles\n",
    "SEED = 23\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "elif torch.xpu.is_available():\n",
    "    DEVICE = \"xpu\"  # si no hay GPU, pero hay XPU, usamos XPU\n",
    "\n",
    "print(f\"Usando {DEVICE}\")\n",
    "\n",
    "NUM_WORKERS = 0  # Win y MacOS pueden tener problemas con múltiples workers\n",
    "if sys.platform == \"linux\":\n",
    "    NUM_WORKERS = 4  # numero de workers para cargar los datos (depende de cada caso)\n",
    "\n",
    "print(f\"Usando {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar los Datos & Preprocesamiento\n",
    "\n",
    "Vamos a leer el dataset de traducción y realizar un preprocesamiento básico para limpiar y normalizar los textos antes de alimentarlos al modelo Seq2Seq. Por ejemplo, vamos a convertir los textos a minúsculas, eliminar algunos caracteres especiales y filtrar las oraciones más largas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convertimos a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Insertamos espacios alrededor de los símbolos de puntuación que queremos conservar\n",
    "    text = re.sub(r\"([¿?¡!])\", r\" \\1 \", text)\n",
    "\n",
    "    # Eliminamos todo lo que no sea letras, números, o los símbolos que queremos conservar\n",
    "    text = re.sub(r\"[^a-zA-Z0-9áéíóúüñ¿?¡!]+\", \" \", text)\n",
    "\n",
    "    # Remover espacios extras\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = str(Path(\"data\") / \"English-Spanish.tsv\")\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 8  # Máxima longitud de las frases que vamos a considerar\n",
    "\n",
    "\n",
    "def load_data(source_file, max_words=5):\n",
    "    with open(source_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Separamos las frases en dos listas\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "\n",
    "    for line in lines:\n",
    "        elements = line.split(\"\\t\")\n",
    "\n",
    "        input_text = elements[1]\n",
    "        target_text = elements[3]\n",
    "\n",
    "        input_text_clean = clean_text(input_text)\n",
    "        target_text_clean = clean_text(target_text)\n",
    "\n",
    "        # Filtramos frases de hasta max_words palabras\n",
    "        if (\n",
    "            len(input_text_clean.split()) <= max_words\n",
    "            and len(target_text_clean.split()) <= max_words\n",
    "        ):\n",
    "            input_texts.append(input_text_clean)\n",
    "            target_texts.append(target_text_clean)\n",
    "\n",
    "    return input_texts, target_texts\n",
    "\n",
    "\n",
    "src_texts, trg_texts = load_data(DATA_PATH, MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of samples: {len(src_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = np.random.randint(0, len(src_texts), 10)\n",
    "for idx in random_idx:\n",
    "    print(f\"Input: {src_texts[idx]}\")\n",
    "    print(f\"Target: {trg_texts[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de los Vocabularios\n",
    "\n",
    "Es importante construir un vocabulario para cada idioma en el dataset, ya que cada vocabulario tiene que ser capaz de mapear palabras a índices enteros y viceversa.\n",
    "\n",
    "> Nota: para reducir el tiempo de entrenamiento, vamos a limitar el tamaño del vocabulario a las palabras más comunes en cada idioma, con el argumento `FREQ_THRESHOLD` controlamos la cantidad de palabras que se incluirán en el vocabulario.\n",
    "\n",
    "Tenemos además que agregar token especiales:\n",
    "\n",
    "- `SOS` (Start of Sentence): Indica el inicio de una oración.\n",
    "- `EOS` (End of Sentence): Indica el final de una oración.\n",
    "- `UNK` (Unknown): Indica una palabra desconocida que no está en el vocabulario.\n",
    "- `PAD` (Padding): Se utiliza para rellenar secuencias a la misma longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "FREQ_THRESHOLD = 3  # Frecuencia mínima para considerar una palabra en el vocabulario\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        # mapea palabras a índices\n",
    "        self.word2index = {}\n",
    "        # mapea índices a palabras\n",
    "        self.index2word = {}\n",
    "        # autonumeración de índices\n",
    "        self.index = 0\n",
    "\n",
    "        # Tokens especiales\n",
    "        self.add_special_tokens()\n",
    "\n",
    "    def add_special_tokens(self):\n",
    "        self.add_word(PAD_TOKEN)\n",
    "        self.add_word(SOS_TOKEN)\n",
    "        self.add_word(EOS_TOKEN)\n",
    "        self.add_word(UNK_TOKEN)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.index\n",
    "            self.index2word[self.index] = word\n",
    "            self.index += 1\n",
    "\n",
    "    def build_vocab(self, sentences, min_freq=1):\n",
    "        word_counter = Counter()\n",
    "        for sentence in sentences:\n",
    "            for word in sentence.split():\n",
    "                word_counter[word] += 1\n",
    "\n",
    "        # Filtrar palabras que no alcanzan la frecuencia mínima\n",
    "        words = [word for word, count in word_counter.items() if count >= min_freq]\n",
    "\n",
    "        # Agregar palabras filtradas al vocabulario\n",
    "        for word in words:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2index)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            return self.index2word.get(key, UNK_TOKEN)\n",
    "        if isinstance(key, str):\n",
    "            return self.word2index.get(key, self.word2index[UNK_TOKEN])\n",
    "\n",
    "\n",
    "# Construimos los vocabularios\n",
    "SRC_VOCAB = Vocab()\n",
    "TRG_VOCAB = Vocab()\n",
    "\n",
    "SRC_VOCAB.build_vocab(src_texts, min_freq=FREQ_THRESHOLD)\n",
    "TRG_VOCAB.build_vocab(trg_texts, min_freq=FREQ_THRESHOLD)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(SRC_VOCAB)\n",
    "TRG_VOCAB_SIZE = len(TRG_VOCAB)\n",
    "\n",
    "print(f\"English vocab size: {SRC_VOCAB_SIZE}\")\n",
    "print(f\"Spanish vocab size: {TRG_VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos ejemplos de uso de los vocabularios:\n",
    "\n",
    "- `SRC_VOCAB['hello']`: Devuelve el índice de la palabra \"hello\" en el vocabulario de origen.\n",
    "- `TGT_VOCAB['hola']`: Devuelve el índice de la palabra \"hola\" en el vocabulario de destino.\n",
    "- `TRG_VOCAB['palabra_no_existente']`: Devuelve el índice de la palabra desconocida (`<UNK>`) en el vocabulario de destino.\n",
    "- `TRG_VOCAB[10]`: Devuelve la palabra en el índice 10 del vocabulario de destino.\n",
    "- `TRG_VOCAB[3]`: Devuelve la palabra en el índice 3 del vocabulario de destino.\n",
    "- `SRC_VOCAB[PAD_TOKEN]`: Devuelve el índice del token de padding en el vocabulario de origen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SRC_VOCAB[\"hello\"])\n",
    "print(TRG_VOCAB[\"hola\"])\n",
    "print(TRG_VOCAB[\"palabra_no_existente\"])\n",
    "print(TRG_VOCAB[10])\n",
    "print(TRG_VOCAB[3])\n",
    "print(TRG_VOCAB[PAD_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para codificar una frase (strings -> índices)\n",
    "def encode_sentence(sentence, vocab):\n",
    "    return [vocab[word] for word in sentence.split()]\n",
    "\n",
    "print(encode_sentence(\"hello world\", SRC_VOCAB))\n",
    "print(encode_sentence(\"hola mundo extraterrestre\", TRG_VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para decodificar una secuencia de índices (indices -> strings)\n",
    "def decode_sentence(indices, vocab):\n",
    "    if isinstance(indices, torch.Tensor): # en caso que nos pasen un tensor\n",
    "        indices = indices.tolist()\n",
    "    return \" \".join([vocab[idx] for idx in indices if idx != vocab[PAD_TOKEN] and idx != vocab[EOS_TOKEN] and idx != vocab[SOS_TOKEN] and idx != vocab[UNK_TOKEN]])\n",
    "\n",
    "print(\n",
    "    decode_sentence([TRG_VOCAB[SOS_TOKEN],10, 11, 12, TRG_VOCAB[PAD_TOKEN], TRG_VOCAB[PAD_TOKEN], TRG_VOCAB[EOS_TOKEN]], TRG_VOCAB)\n",
    ")\n",
    "\n",
    "print(\n",
    "    decode_sentence(torch.randint(0, len(TRG_VOCAB), (3,)), TRG_VOCAB)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset de Traducción\n",
    "\n",
    "Trabajaremos con un pequeño dataset de traducción **inglés-español**, compuesto por pares de frases simples. El objetivo es que el modelo aprenda a traducir una frase en inglés a su equivalente en español.\n",
    "\n",
    "#### Ejemplos de Pares:\n",
    "\n",
    "- **Inglés**: \"hello\" → **Español**: \"hola\"\n",
    "- **Inglés**: \"how are you?\" → **Español**: \"¿cómo estás?\"\n",
    "\n",
    "### Preparación del Dataset\n",
    "\n",
    "Cada frase será tokenizada y convertida a índices numéricos de sus respectivos vocabularios. En las secuencias objetivo, añadimos los tokens especiales `<SOS>` y `<EOS>` para marcar el inicio y el fin de cada traducción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences):\n",
    "        super(TranslationDataset, self).__init__()\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "# Crear el dataset y el dataloader\n",
    "train_dataset = TranslationDataset(src_texts, trg_texts)\n",
    "val_len = int(0.10 * len(train_dataset))\n",
    "train_len = len(train_dataset) - val_len\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos algunos ejemplos\n",
    "random_idx = np.random.randint(\n",
    "    0, len(train_dataset), 5\n",
    ")  # tomamos 5 ejemplos del dataset train\n",
    "for idx in random_idx:\n",
    "    x, y = train_dataset[idx]\n",
    "    x_sentence = decode_sentence(x, SRC_VOCAB)  # int -> palabras\n",
    "    y_sentence = decode_sentence(y, TRG_VOCAB)  # int -> palabras\n",
    "    print(f\"SRC:           {x_sentence}\")\n",
    "    print(f\"SRC (encoded): {x}\")\n",
    "    print(f\"TRG:           {y_sentence}\")\n",
    "    print(f\"TRG (encoded): {y}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que las RNNs requieren secuencias de longitud fija, vamos a rellenar las secuencias con el token especial `<PAD>` para que todas tengan la misma longitud. Para eso podemos auxiliarnos de la función [pad_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html) de PyTorch.\n",
    "\n",
    "La función `collate_fn` es un argumento opcional que se pasa al DataLoader de PyTorch para personalizar el procesamiento de los datos. En este caso, se utiliza para rellenar y agrupar las secuencias de entrada y salida en lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_1 = torch.tensor([1, 2, 3, 4])\n",
    "seq_2 = torch.tensor([1, 2, 3])\n",
    "seq_3 = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "padded_seqs = pad_sequence([seq_1, seq_2, seq_3], batch_first=True, padding_value=0, padding_side='left')\n",
    "print(padded_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sources, targets = zip(*batch)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "print(f\"Source batch shape: {x.shape}\")\n",
    "print(f\"Target batch shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Seq2Seq\n",
    "\n",
    "El modelo Seq2Seq que vamos a implementar consiste en un **encoder** y un **decoder** basados en capas LSTM. El encoder procesa la secuencia de entrada y genera un vector de contexto que es utilizado por el decoder para generar la secuencia de salida.\n",
    "\n",
    "<img src=\"https://docs.chainer.org/en/v7.8.0/_images/seq2seq.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de hiperparámetros del modelo y entrenamiento\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "LR = 0.001\n",
    "EPOCHS = 20\n",
    "\n",
    "INITIAL_TEACHER_FORCING_RATIO = 1.0\n",
    "TEACHER_FORCING_DECAY = 0.95\n",
    "MIN_TEACHER_FORCING_RATIO = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "El encoder procesa la secuencia de entrada palabra por palabra y genera una representación de la secuencia en forma de un vector de contexto.\n",
    "\n",
    "- Primero, la secuencia de entrada es pasada a través de una capa de embedding para convertir las palabras en vectores densos.\n",
    "- Luego, los embeddings de palabras son pasados a una capa LSTM que procesa la secuencia y genera una representación de la secuencia en forma de un vector de estado oculto.\n",
    "- La salida del encoder es el estado oculto final y la celda oculta final de la capa LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, seq_len]\n",
    "        pass\n",
    "\n",
    "summary(\n",
    "    Encoder(EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT),\n",
    "    input_size=(BATCH_SIZE, MAX_SENTENCE_LENGTH),\n",
    "    dtypes=[torch.long],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "El decoder genera la secuencia de salida palabra por palabra utilizando el vector de contexto generado por el encoder. A diferencia del encoder, el decoder es entrenado para predecir la siguiente palabra en la secuencia de salida.\n",
    "\n",
    "- Se recibe el input (token de entrada) y el estado oculto del encoder (hidden, cell).\n",
    "- El token de entrada es pasado a través de una capa de embedding para convertirlo en un vector denso.\n",
    "- Luego, el embedding de la palabra y el estado oculto anterior son usado para predecir la siguiente palabra en la secuencia de salida.\n",
    "- Se retorna la predicción y el nuevo estado oculto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_token, hidden, cell):\n",
    "        # input_token: [batch_size] -> un solo token por cada oración en el batch\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        # cell: [n_layers, batch_size, hidden_dim]\n",
    "        pass\n",
    "\n",
    "\n",
    "summary(\n",
    "    Decoder(EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT),\n",
    "    input_size=[\n",
    "        (BATCH_SIZE,),\n",
    "        (N_LAYERS, BATCH_SIZE, HIDDEN_DIM),\n",
    "        (N_LAYERS, BATCH_SIZE, HIDDEN_DIM),\n",
    "    ],\n",
    "    dtypes=[torch.long, torch.float, torch.float],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del Modelo\n",
    "\n",
    "Para entrenar el modelo, vamos a definir una función de pérdida y un optimizador. La función de pérdida será la entropía cruzada categórica, ya que estamos tratando con un problema de clasificación de múltiples clases. Además utilizaremos el parametro `ignore_index` del criterio de pérdida para no calcular la pérdida en los tokens de padding.\n",
    "\n",
    "El optimizador necesita los parámetros del Encoder y del Decoder, así como la tasa de aprendizaje.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "decoder = Decoder(EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=TRG_VOCAB[PAD_TOKEN], label_smoothing=0.05\n",
    ")  # ignoramos el padding en el cálculo de la loss\n",
    "optimizer = optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()), lr=LR\n",
    ")  # tomamos los parámetros de ambos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de una Época en Seq2Seq con *Teacher Forcing*\n",
    "\n",
    "**Cálculo de la pérdida para cada token**\n",
    "\n",
    "A diferencia de entrenamientos comunes donde se calcula la pérdida para toda la secuencia de una vez, aquí la pérdida se calcula **token por token** dentro de un bucle. Esto permite que el decoder procese un token a la vez, prediciendo el siguiente en función del estado oculto anterior.\n",
    "\n",
    "**Uso de *Teacher Forcing***\n",
    "\n",
    "Se introduce un parámetro **`teacher_forcing_ratio`**, que controla la probabilidad de usar el token real de la secuencia objetivo como siguiente entrada. Esto difiere de los enfoques comunes donde siempre se usa la predicción del modelo para generar la siguiente entrada.\n",
    "\n",
    "**Actualización de gradientes**\n",
    "La **retropropagación** ocurre después de que se procesan todos los tokens en la secuencia, acumulando las pérdidas de cada token antes de hacer `backward()` y actualizar los parámetros con `optimizer.step()`.\n",
    "\n",
    "**Proceso resumido**\n",
    "\n",
    "1. Inicialización de `encoder` y `decoder` en modo entrenamiento.\n",
    "2. Para cada batch, se:\n",
    "   - Calcula la pérdida token por token.\n",
    "   - Decide si usar el token objetivo real o la predicción anterior.\n",
    "3. Finalmente, se acumula la pérdida total por secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(src, trg, encoder, decoder, criterion, teacher_forcing_ratio):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    encoder, decoder, dataloader, optimizer, criterion, teacher_forcing_ratio\n",
    "):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, valid_loader, criterion):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    n_epochs,\n",
    "    initial_teacher_forcing_ratio=INITIAL_TEACHER_FORCING_RATIO,\n",
    "    teacher_forcing_decay=TEACHER_FORCING_DECAY,\n",
    "    min_teacher_forcing_ratio=MIN_TEACHER_FORCING_RATIO,\n",
    "):\n",
    "    current_teacher_forcing_ratio = initial_teacher_forcing_ratio\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            current_teacher_forcing_ratio,\n",
    "        )\n",
    "        val_loss = evaluate(encoder, decoder, valid_loader, criterion)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} FT: {current_teacher_forcing_ratio:.2f} Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "        current_teacher_forcing_ratio = max(\n",
    "            min_teacher_forcing_ratio,\n",
    "            current_teacher_forcing_ratio * teacher_forcing_decay,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    encoder, decoder, train_loader, valid_loader, optimizer, criterion, n_epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo\n",
    "\n",
    "Vamos a traducir algunas frases de prueba utilizando el modelo entrenado y evaluar la calidad de las traducciones generadas. Para esto, vamos a implementar una función `translate_sentence` que toma una oración en inglés y la traduce al español.\n",
    "\n",
    "Esta vez es el modelo quien decide cuando parar con en el token `<EOS>`, aunque también se puede limitar la longitud máxima de la traducción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(src, encoder, decoder, max_len):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    generated_tokens = []\n",
    "    with torch.no_grad():\n",
    "        pass\n",
    "\n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(encoder, decoder, sentence, max_len):\n",
    "    sentence = clean_text(sentence)\n",
    "    sentence_indexes = encode_sentence(sentence, SRC_VOCAB)\n",
    "    sentence_tensor = torch.tensor(sentence_indexes).unsqueeze(0).to(DEVICE)\n",
    "    generated_tokens = generate_sequence(sentence_tensor, encoder, decoder, max_len)\n",
    "    predicted_sentence = decode_sentence(generated_tokens, TRG_VOCAB)\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I am hungry\",\n",
    "    \"I am tired\",\n",
    "    \"I am happy\",\n",
    "    \"I'm sad\",\n",
    "    \"I am angry\",\n",
    "    \"every time I study, I get sleepy\",\n",
    "    \"I am going to the gym\",\n",
    "    \"I am going to the beach\",\n",
    "    \"I am going to the supermarket\",\n",
    "    \"I'm going to the movies\",\n",
    "    \"I don't know what to do\",\n",
    "    \"I love deep learning\",\n",
    "    \"I can't open the door\",\n",
    "    \"you can go if you want to\",\n",
    "    \"i'm going to the party\",\n",
    "    \"where does all this come from ?\",\n",
    "    \"I can read your mind\",\n",
    "    \"I can't believe it\",\n",
    "    \"I can't believe you\",\n",
    "    \"I can't believe this\",\n",
    "    \"I didn't like it\",\n",
    "    \"You can do it\",\n",
    "    \"Do you speak Italian?\",\n",
    "    \"Do you want to learn Spanish?\",\n",
    "    \"I want to learn French\",\n",
    "    \"Do you want to go to the movies?\",\n",
    "    \"this is my favorite song\",\n",
    "    \"I can't wait to see you\",\n",
    "    \"see you later\",\n",
    "    \"have a nice day\",\n",
    "    \"we'll talk later\",\n",
    "    \"let's grab a coffee sometime\",\n",
    "    \"they're coming to the party\",\n",
    "    \"she's my best friend\",\n",
    "    \"he's a great guy\",\n",
    "    \"My class is in 30 minutes.\",\n",
    "    \"I have class tomorrow.\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f\"Input: {sentence}\")\n",
    "    print(f\"Translation: {translate_sentence(encoder, decoder, sentence, 12)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_sentence(encoder, decoder, \"we need attention\", MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_sentence(encoder, decoder, \"let's go for it!\", MAX_SENTENCE_LENGTH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taller-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
