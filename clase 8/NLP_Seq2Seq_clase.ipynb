{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unxzznxDJug4"
      },
      "source": [
        "# Sequence-to-Sequence Model (Seq2Seq) para Traducción de Idiomas\n",
        "\n",
        "En esta notebook, exploraremos la implementación de un modelo Sequence-to-Sequence (Seq2Seq) utilizando PyTorch. Este tipo de modelos es ampliamente utilizado en tareas de Procesamiento de Lenguaje Natural (NLP) que requieren el manejo de secuencias de texto de longitud variable, como la traducción automática, el resumen de textos y la generación de lenguaje natural.\n",
        "\n",
        "## Introducción\n",
        "\n",
        "### Objetivos\n",
        "\n",
        "1. **Entender el funcionamiento de un modelo Seq2Seq** y cómo se aplica en la traducción automática de idiomas.\n",
        "2. **Implementar el pipeline completo de un modelo Seq2Seq en PyTorch**, desde la preparación de los datos hasta el entrenamiento y evaluación del modelo.\n",
        "3. **Explorar el uso del `teacher forcing`** para mejorar la eficiencia en el entrenamiento de redes neuronales recurrentes.\n",
        "\n",
        "### Contenido\n",
        "\n",
        "1. Introducción a los modelos Sequence-to-Sequence (Seq2Seq) y su relevancia en tareas de traducción automática.\n",
        "2. Preparación de datos textuales para alimentar a un modelo Seq2Seq.\n",
        "3. Implementación de un **encoder-decoder** utilizando LSTM para la traducción de secuencias.\n",
        "4. Aplicación de **teacher forcing** durante el entrenamiento para mejorar el rendimiento.\n",
        "5. Evaluación del modelo y generación de traducciones en el conjunto de prueba.\n",
        "\n",
        "### Concepto de Seq2Seq\n",
        "\n",
        "Un modelo Seq2Seq consiste en dos partes principales: un **encoder** y un **decoder**. El encoder procesa la secuencia de entrada y la comprime en un vector de estado oculto, que luego es utilizado por el decoder para generar la secuencia de salida.\n",
        "\n",
        "- **Encoder**: Procesa la secuencia de entrada palabra por palabra y genera un conjunto de estados ocultos que resumen el contenido de la secuencia.\n",
        "- **Decoder**: Utiliza el estado final del encoder para generar la secuencia de salida, también palabra por palabra.\n",
        "\n",
        "Este enfoque es particularmente útil en tareas de traducción automática, donde queremos convertir una oración en un idioma a una oración equivalente en otro idioma.\n",
        "\n",
        "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_7.47.32_PM.png\"/>\n",
        "\n",
        "### Dataset de Traducción\n",
        "\n",
        "Para esta notebook, utilizaremos un dataset de traducción inglés-español que contiene pares de frases. El objetivo es traducir frases de inglés a español, simulando un escenario de traducción automática. El dataset se compone de pares de frases cortas en ambos idiomas y servirá como base para entrenar el modelo Seq2Seq.\n",
        "\n",
        "El mismo se encuentra disponible en el siguiente enlace: [Link al dataset](https://tatoeba.org/en/downloads)\n",
        "\n",
        "### Referencias\n",
        "\n",
        "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) - Sutskever et al. (2014)\n",
        "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078) - Cho et al. (2014)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7K7j8dlNxBf",
        "outputId": "b3681d7a-260b-49ad-fee1-20746d3e58b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QJyT4ScpJug5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kaPO3-MMJug6"
      },
      "outputs": [],
      "source": [
        "# Fijamos la semilla para que los resultados sean reproducibles\n",
        "SEED = 23\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNSjWZ6GJug6",
        "outputId": "1463b757-0917-4b73-9a12-65d48df25762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando cpu\n",
            "Usando 4\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# definimos el dispositivo que vamos a usar\n",
        "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
        "elif torch.backends.mps.is_available():\n",
        "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
        "elif torch.xpu.is_available():\n",
        "    DEVICE = \"xpu\"  # si no hay GPU, pero hay XPU, usamos XPU\n",
        "\n",
        "print(f\"Usando {DEVICE}\")\n",
        "\n",
        "NUM_WORKERS = 0  # Win y MacOS pueden tener problemas con múltiples workers\n",
        "if sys.platform == \"linux\":\n",
        "    NUM_WORKERS = 4  # numero de workers para cargar los datos (depende de cada caso)\n",
        "\n",
        "print(f\"Usando {NUM_WORKERS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5D7K848Jug6"
      },
      "source": [
        "## Cargar los Datos & Preprocesamiento\n",
        "\n",
        "Vamos a leer el dataset de traducción y realizar un preprocesamiento básico para limpiar y normalizar los textos antes de alimentarlos al modelo Seq2Seq. Por ejemplo, vamos a convertir los textos a minúsculas, eliminar algunos caracteres especiales y filtrar las oraciones más largas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NDtWYLFjJug6"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Convertimos a minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Insertamos espacios alrededor de los símbolos de puntuación que queremos conservar\n",
        "    text = re.sub(r\"([¿?¡!])\", r\" \\1 \", text)\n",
        "\n",
        "    # Eliminamos todo lo que no sea letras, números, o los símbolos que queremos conservar\n",
        "    text = re.sub(r\"[^a-zA-Z0-9áéíóúüñ¿?¡!]+\", \" \", text)\n",
        "\n",
        "    # Remover espacios extras\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6fN3_O6wJug6"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = str(Path(\"data\") / \"English-Spanish.tsv\")\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 8  # Máxima longitud de las frases que vamos a considerar\n",
        "\n",
        "\n",
        "def load_data(source_file, max_words=5):\n",
        "    with open(source_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Separamos las frases en dos listas\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "\n",
        "    for line in lines:\n",
        "        elements = line.split(\"\\t\")\n",
        "\n",
        "        input_text = elements[1]\n",
        "        target_text = elements[3]\n",
        "\n",
        "        input_text_clean = clean_text(input_text)\n",
        "        target_text_clean = clean_text(target_text)\n",
        "\n",
        "        # Filtramos frases de hasta max_words palabras\n",
        "        if (\n",
        "            len(input_text_clean.split()) <= max_words\n",
        "            and len(target_text_clean.split()) <= max_words\n",
        "        ):\n",
        "            input_texts.append(input_text_clean)\n",
        "            target_texts.append(target_text_clean)\n",
        "\n",
        "    return input_texts, target_texts\n",
        "\n",
        "\n",
        "src_texts, trg_texts = load_data(DATA_PATH, MAX_SENTENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UDbv3LxJug7",
        "outputId": "9844e967-924a-4d17-f851-e39ad3b7acb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 193620\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of samples: {len(src_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeQLl6b3Jug7",
        "outputId": "caba1760-55e1-49ef-efd4-8ca11ec60a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: our guests are in a hurry\n",
            "Target: nuestros huéspedes tienen prisa\n",
            "\n",
            "Input: did he do such things ?\n",
            "Target: ¿ seguro que hizo esas cosas ?\n",
            "\n",
            "Input: this room is large enough\n",
            "Target: este cuarto es lo suficientemente grande\n",
            "\n",
            "Input: is tatoeba a pamphlet or what ?\n",
            "Target: ¿ tatoeba es un panfleto o qué ?\n",
            "\n",
            "Input: i would like batteries for this device\n",
            "Target: quisiera pilas para este aparato\n",
            "\n",
            "Input: i want to show you something great\n",
            "Target: quiero mostrarte algo grandioso\n",
            "\n",
            "Input: the bad weather prevented me from going fishing\n",
            "Target: el mal clima me impidió ir a pescar\n",
            "\n",
            "Input: i ve acknowledged my mistake\n",
            "Target: he reconocido mi error\n",
            "\n",
            "Input: you screamed\n",
            "Target: gritaron\n",
            "\n",
            "Input: tom said mary did that on purpose\n",
            "Target: tom dijo que mary lo hizo a propósito\n",
            "\n"
          ]
        }
      ],
      "source": [
        "random_idx = np.random.randint(0, len(src_texts), 10)\n",
        "for idx in random_idx:\n",
        "    print(f\"Input: {src_texts[idx]}\")\n",
        "    print(f\"Target: {trg_texts[idx]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUDVTz7rJug7"
      },
      "source": [
        "## Construcción de los Vocabularios\n",
        "\n",
        "Es importante construir un vocabulario para cada idioma en el dataset, ya que cada vocabulario tiene que ser capaz de mapear palabras a índices enteros y viceversa.\n",
        "\n",
        "> Nota: para reducir el tiempo de entrenamiento, vamos a limitar el tamaño del vocabulario a las palabras más comunes en cada idioma, con el argumento `FREQ_THRESHOLD` controlamos la cantidad de palabras que se incluirán en el vocabulario.\n",
        "\n",
        "Tenemos además que agregar token especiales:\n",
        "\n",
        "- `SOS` (Start of Sentence): Indica el inicio de una oración.\n",
        "- `EOS` (End of Sentence): Indica el final de una oración.\n",
        "- `UNK` (Unknown): Indica una palabra desconocida que no está en el vocabulario.\n",
        "- `PAD` (Padding): Se utiliza para rellenar secuencias a la misma longitud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7qmKWOkJug7",
        "outputId": "7785d7e0-7f95-4872-f6bd-98d3fc35c19b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocab size: 9832\n",
            "Spanish vocab size: 15208\n"
          ]
        }
      ],
      "source": [
        "PAD_TOKEN = \"<PAD>\"\n",
        "SOS_TOKEN = \"<SOS>\"\n",
        "EOS_TOKEN = \"<EOS>\"\n",
        "UNK_TOKEN = \"<UNK>\"\n",
        "FREQ_THRESHOLD = 3  # Frecuencia mínima para considerar una palabra en el vocabulario\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        # mapea palabras a índices\n",
        "        self.word2index = {}\n",
        "        # mapea índices a palabras\n",
        "        self.index2word = {}\n",
        "        # autonumeración de índices\n",
        "        self.index = 0\n",
        "\n",
        "        # Tokens especiales\n",
        "        self.add_special_tokens()\n",
        "\n",
        "    def add_special_tokens(self):\n",
        "        self.add_word(PAD_TOKEN)\n",
        "        self.add_word(SOS_TOKEN)\n",
        "        self.add_word(EOS_TOKEN)\n",
        "        self.add_word(UNK_TOKEN)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.index\n",
        "            self.index2word[self.index] = word\n",
        "            self.index += 1\n",
        "\n",
        "    def build_vocab(self, sentences, min_freq=1):\n",
        "        word_counter = Counter()\n",
        "        for sentence in sentences:\n",
        "            for word in sentence.split():\n",
        "                word_counter[word] += 1\n",
        "\n",
        "        # Filtrar palabras que no alcanzan la frecuencia mínima\n",
        "        words = [word for word, count in word_counter.items() if count >= min_freq]\n",
        "\n",
        "        # Agregar palabras filtradas al vocabulario\n",
        "        for word in words:\n",
        "            self.add_word(word)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2index)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, int):\n",
        "            return self.index2word.get(key, UNK_TOKEN)\n",
        "        if isinstance(key, str):\n",
        "            return self.word2index.get(key, self.word2index[UNK_TOKEN])\n",
        "\n",
        "\n",
        "# Construimos los vocabularios\n",
        "SRC_VOCAB = Vocab()\n",
        "TRG_VOCAB = Vocab()\n",
        "\n",
        "SRC_VOCAB.build_vocab(src_texts, min_freq=FREQ_THRESHOLD)\n",
        "TRG_VOCAB.build_vocab(trg_texts, min_freq=FREQ_THRESHOLD)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(SRC_VOCAB)\n",
        "TRG_VOCAB_SIZE = len(TRG_VOCAB)\n",
        "\n",
        "print(f\"English vocab size: {SRC_VOCAB_SIZE}\")\n",
        "print(f\"Spanish vocab size: {TRG_VOCAB_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwtzQ2lwJug8"
      },
      "source": [
        "Algunos ejemplos de uso de los vocabularios:\n",
        "\n",
        "- `SRC_VOCAB['hello']`: Devuelve el índice de la palabra \"hello\" en el vocabulario de origen.\n",
        "- `TGT_VOCAB['hola']`: Devuelve el índice de la palabra \"hola\" en el vocabulario de destino.\n",
        "- `TRG_VOCAB['palabra_no_existente']`: Devuelve el índice de la palabra desconocida (`<UNK>`) en el vocabulario de destino.\n",
        "- `TRG_VOCAB[10]`: Devuelve la palabra en el índice 10 del vocabulario de destino.\n",
        "- `TRG_VOCAB[3]`: Devuelve la palabra en el índice 3 del vocabulario de destino.\n",
        "- `SRC_VOCAB[PAD_TOKEN]`: Devuelve el índice del token de padding en el vocabulario de origen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzbMHG12Jug8",
        "outputId": "80eed812-5a9f-4272-8654-62b5d0253da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "550\n",
            "695\n",
            "3\n",
            "tiene\n",
            "is\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(SRC_VOCAB[\"hello\"])\n",
        "print(TRG_VOCAB[\"hola\"])\n",
        "print(TRG_VOCAB[\"palabra_no_existente\"])\n",
        "print(TRG_VOCAB[15])\n",
        "print(SRC_VOCAB[14])\n",
        "print(TRG_VOCAB[PAD_TOKEN])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABdgcUHHJug8",
        "outputId": "9017dbd9-7fef-4443-9884-717d3979423f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[550, 51]\n",
            "[695, 53, 3]\n"
          ]
        }
      ],
      "source": [
        "# Función para codificar una frase (strings -> índices)\n",
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab[word] for word in sentence.split()]\n",
        "\n",
        "print(encode_sentence(\"hello world\", SRC_VOCAB))\n",
        "print(encode_sentence(\"hola mundo extraterrestre\", TRG_VOCAB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADDD0Z5ZJug8",
        "outputId": "4e444690-e759-4adc-ea00-8e0f2e26c9d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "irme a dormir\n",
            "desapareció fiable haumea\n"
          ]
        }
      ],
      "source": [
        "# Función para decodificar una secuencia de índices (indices -> strings)\n",
        "def decode_sentence(indices, vocab):\n",
        "    if isinstance(indices, torch.Tensor): # en caso que nos pasen un tensor\n",
        "        indices = indices.tolist()\n",
        "    return \" \".join([vocab[idx] for idx in indices if idx != vocab[PAD_TOKEN] and idx != vocab[EOS_TOKEN] and idx != vocab[SOS_TOKEN] and idx != vocab[UNK_TOKEN]])\n",
        "\n",
        "print(\n",
        "    decode_sentence([TRG_VOCAB[SOS_TOKEN],10, 11, 12, TRG_VOCAB[PAD_TOKEN], TRG_VOCAB[PAD_TOKEN], TRG_VOCAB[EOS_TOKEN]], TRG_VOCAB)\n",
        ")\n",
        "\n",
        "print(\n",
        "    decode_sentence(torch.randint(0, len(TRG_VOCAB), (3,)), TRG_VOCAB)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuqSdX8HJug8"
      },
      "source": [
        "## Dataset de Traducción\n",
        "\n",
        "Trabajaremos con un pequeño dataset de traducción **inglés-español**, compuesto por pares de frases simples. El objetivo es que el modelo aprenda a traducir una frase en inglés a su equivalente en español.\n",
        "\n",
        "#### Ejemplos de Pares:\n",
        "\n",
        "- **Inglés**: \"hello\" → **Español**: \"hola\"\n",
        "- **Inglés**: \"how are you?\" → **Español**: \"¿cómo estás?\"\n",
        "\n",
        "### Preparación del Dataset\n",
        "\n",
        "Cada frase será tokenizada y convertida a índices numéricos de sus respectivos vocabularios. En las secuencias objetivo, añadimos los tokens especiales `<SOS>` y `<EOS>` para marcar el inicio y el fin de cada traducción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ugDnxpuSJug8"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, source_sentences, target_sentences):\n",
        "        super(TranslationDataset, self).__init__()\n",
        "        self.source_sentences = source_sentences\n",
        "        self.target_sentences = target_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.source_sentences[idx] # la frase con palabras src\n",
        "        trg = self.target_sentences[idx] # la frase con palabras trg\n",
        "\n",
        "        src = encode_sentence(src, SRC_VOCAB)\n",
        "        trg = encode_sentence(trg, TRG_VOCAB)\n",
        "\n",
        "        trg = [TRG_VOCAB[SOS_TOKEN]] + trg + [TRG_VOCAB[EOS_TOKEN]]\n",
        "\n",
        "        src = torch.tensor(src, dtype=torch.long)\n",
        "        trg = torch.tensor(trg, dtype=torch.long)\n",
        "\n",
        "        return src, trg\n",
        "\n",
        "# Crear el dataset y el dataloader\n",
        "train_dataset = TranslationDataset(src_texts, trg_texts)\n",
        "val_len = int(0.10 * len(train_dataset))\n",
        "train_len = len(train_dataset) - val_len\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_len, val_len])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKCz0AvPJug8",
        "outputId": "8a47824d-b08b-4d5b-f4a7-b7317a957a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC:           who invented the first automobile ?\n",
            "SRC (encoded): tensor([ 293,  389,   17,  348, 4274,   49])\n",
            "TRG:           ¿ quién inventó el primer automóvil ?\n",
            "TRG (encoded): tensor([   1,   46,  356, 3587,  275, 3306, 5468,   50,    2])\n",
            "\n",
            "SRC:           may his soul rest in peace\n",
            "SRC (encoded): tensor([ 191,  599,  789,  949,   44, 2346])\n",
            "TRG:           que en paz descanse\n",
            "TRG (encoded): tensor([    1,     9,    24,  3269, 10702,     2])\n",
            "\n",
            "SRC:           tom is insolent\n",
            "SRC (encoded): tensor([ 485,   14, 8405])\n",
            "TRG:           tom es insolente\n",
            "TRG (encoded): tensor([    1,   618,    20, 12861,     2])\n",
            "\n",
            "SRC:           dark matter is invisible\n",
            "SRC (encoded): tensor([ 605, 2181,   14, 1457])\n",
            "TRG:           la materia oscura es invisible\n",
            "TRG (encoded): tensor([   1,   18, 4668, 4041,   20, 2041,    2])\n",
            "\n",
            "SRC:           the roof of my house is red\n",
            "SRC (encoded): tensor([  17, 2582,   91,   60, 1307,   14,  427])\n",
            "TRG:           el techo de mi casa es rojo\n",
            "TRG (encoded): tensor([   1,  275, 3660,   79,   59, 1021,   20,  526,    2])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# probamos algunos ejemplos\n",
        "random_idx = np.random.randint(\n",
        "    0, len(train_dataset), 5\n",
        ")  # tomamos 5 ejemplos del dataset train\n",
        "\n",
        "for idx in random_idx:\n",
        "    x, y = train_dataset[idx]\n",
        "    x_sentence = decode_sentence(x, SRC_VOCAB)  # int -> palabras\n",
        "    y_sentence = decode_sentence(y, TRG_VOCAB)  # int -> palabras\n",
        "    print(f\"SRC:           {x_sentence}\")\n",
        "    print(f\"SRC (encoded): {x}\")\n",
        "    print(f\"TRG:           {y_sentence}\")\n",
        "    print(f\"TRG (encoded): {y}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSb9PPCaJug8"
      },
      "source": [
        "Debido a que las RNNs requieren secuencias de longitud fija, vamos a rellenar las secuencias con el token especial `<PAD>` para que todas tengan la misma longitud. Para eso podemos auxiliarnos de la función [pad_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html) de PyTorch.\n",
        "\n",
        "La función `collate_fn` es un argumento opcional que se pasa al DataLoader de PyTorch para personalizar el procesamiento de los datos. En este caso, se utiliza para rellenar y agrupar las secuencias de entrada y salida en lotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AvsDHg6Jug8",
        "outputId": "f86e8f6c-a5b8-4b91-cf42-52ca96e84f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 1, 2, 3, 4],\n",
            "        [0, 0, 0, 1, 2, 3],\n",
            "        [1, 2, 3, 4, 5, 6]])\n"
          ]
        }
      ],
      "source": [
        "seq_1 = torch.tensor([1, 2, 3, 4])\n",
        "seq_2 = torch.tensor([1, 2, 3])\n",
        "seq_3 = torch.tensor([1, 2, 3, 4, 5, 6])\n",
        "padded_seqs = pad_sequence([seq_1, seq_2, seq_3], batch_first=True, padding_value=0, padding_side='left')\n",
        "print(padded_seqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "a2Q4P9aIJug9"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    sources, targets = zip(*batch)\n",
        "\n",
        "    sources = pad_sequence(\n",
        "        sources, batch_first=True, padding_value=SRC_VOCAB[PAD_TOKEN], padding_side='right'\n",
        "    )\n",
        "\n",
        "    targets = pad_sequence(\n",
        "        targets, batch_first=True, padding_value=TRG_VOCAB[PAD_TOKEN], padding_side='right'\n",
        "    )\n",
        "\n",
        "    return sources, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ijLRsGj2Jug9"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 512\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s6Z3OfmJug9",
        "outputId": "4ad2ab00-47d9-436e-d889-32e5cb85ee20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source batch shape: torch.Size([512, 8])\n",
            "Target batch shape: torch.Size([512, 10])\n"
          ]
        }
      ],
      "source": [
        "x, y = next(iter(train_loader))\n",
        "print(f\"Source batch shape: {x.shape}\")\n",
        "print(f\"Target batch shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prwJjsSjJug9"
      },
      "source": [
        "## Modelo Seq2Seq\n",
        "\n",
        "El modelo Seq2Seq que vamos a implementar consiste en un **encoder** y un **decoder** basados en capas LSTM. El encoder procesa la secuencia de entrada y genera un vector de contexto que es utilizado por el decoder para generar la secuencia de salida.\n",
        "\n",
        "<img src=\"https://docs.chainer.org/en/v7.8.0/_images/seq2seq.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "MDvexqdLJug9"
      },
      "outputs": [],
      "source": [
        "# Definicion de hiperparámetros del modelo y entrenamiento\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 512\n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "\n",
        "LR = 0.001\n",
        "EPOCHS = 20\n",
        "\n",
        "INITIAL_TEACHER_FORCING_RATIO = 1.0\n",
        "TEACHER_FORCING_DECAY = 0.95\n",
        "MIN_TEACHER_FORCING_RATIO = 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T60hyMKuJug9"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "El encoder procesa la secuencia de entrada palabra por palabra y genera una representación de la secuencia en forma de un vector de contexto.\n",
        "\n",
        "- Primero, la secuencia de entrada es pasada a través de una capa de embedding para convertir las palabras en vectores densos.\n",
        "- Luego, los embeddings de palabras son pasados a una capa LSTM que procesa la secuencia y genera una representación de la secuencia en forma de un vector de estado oculto.\n",
        "- La salida del encoder es el estado oculto final y la celda oculta final de la capa LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zlfMxsbJug9",
        "outputId": "f2f05fbc-1e83-4055-adfe-b39cf7e38b8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Encoder                                  [2, 512, 512]             --\n",
              "├─Embedding: 1-1                         [512, 8, 128]             1,258,496\n",
              "├─Dropout: 1-2                           [512, 8, 128]             --\n",
              "├─LSTM: 1-3                              [512, 8, 512]             3,416,064\n",
              "==========================================================================================\n",
              "Total params: 4,674,560\n",
              "Trainable params: 4,674,560\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 14.64\n",
              "==========================================================================================\n",
              "Input size (MB): 0.03\n",
              "Forward/backward pass size (MB): 20.97\n",
              "Params size (MB): 18.70\n",
              "Estimated Total Size (MB): 39.70\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed = nn.Embedding(SRC_VOCAB_SIZE, emb_dim,SRC_VOCAB[PAD_TOKEN])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, n_layers, batch_first = True, dropout=dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch_size, seq_len]\n",
        "        embed = self.dropout(self.embed(src))\n",
        "        # embed: [batch_size, seq_len, emb_dim]\n",
        "        _, (hidden, cell) = self.lstm(embed)\n",
        "        # hidden: [num_layer, batch_size, hidden_dim]\n",
        "        # cell: [num_layer, batch_size, hidden_dim]\n",
        "        return hidden, cell\n",
        "\n",
        "summary(\n",
        "    Encoder(EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT),\n",
        "    input_size=(BATCH_SIZE, MAX_SENTENCE_LENGTH),\n",
        "    dtypes=[torch.long],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcC2J77OJug9"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "El decoder genera la secuencia de salida palabra por palabra utilizando el vector de contexto generado por el encoder. A diferencia del encoder, el decoder es entrenado para predecir la siguiente palabra en la secuencia de salida.\n",
        "\n",
        "- Se recibe el input (token de entrada) y el estado oculto del encoder (hidden, cell).\n",
        "- El token de entrada es pasado a través de una capa de embedding para convertirlo en un vector denso.\n",
        "- Luego, el embedding de la palabra y el estado oculto anterior son usado para predecir la siguiente palabra en la secuencia de salida.\n",
        "- Se retorna la predicción y el nuevo estado oculto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMrjD3bzJug9",
        "outputId": "28d5d491-796f-4d11-94d6-d8bd4d5a3244"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Decoder                                  [512, 15208]              --\n",
              "├─Embedding: 1-1                         [512, 1, 128]             1,946,624\n",
              "├─Dropout: 1-2                           [512, 1, 128]             --\n",
              "├─LSTM: 1-3                              [512, 1, 512]             3,416,064\n",
              "├─Linear: 1-4                            [512, 15208]              7,801,704\n",
              "==========================================================================================\n",
              "Total params: 13,164,392\n",
              "Trainable params: 13,164,392\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 6.74\n",
              "==========================================================================================\n",
              "Input size (MB): 4.20\n",
              "Forward/backward pass size (MB): 64.91\n",
              "Params size (MB): 52.66\n",
              "Estimated Total Size (MB): 121.77\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = nn.Embedding(TRG_VOCAB_SIZE, emb_dim,TRG_VOCAB[PAD_TOKEN])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, n_layers, batch_first = True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, TRG_VOCAB_SIZE)\n",
        "\n",
        "    def forward(self, input_token, hidden, cell):\n",
        "        # input_token: [batch_size] -> un solo token por cada oración en el batch\n",
        "        # hidden: [n_layers, batch_size, hidden_dim]\n",
        "        # cell: [n_layers, batch_size, hidden_dim]\n",
        "        input_token = torch.unsqueeze(input_token, 1)\n",
        "        # input_token: [batch_size, 1]\n",
        "        embed = self.dropout(self.embed(input_token))\n",
        "        # embed: [batch_size, 1, emb_dim]\n",
        "        output, (hidden, cell) = self.lstm(embed, (hidden, cell))\n",
        "        # output: [batch_size, 1, hidden_dim]\n",
        "        output = torch.squeeze(output, 1)\n",
        "        # output: [batch_size, hidden_dim]\n",
        "        output = self.fc(output)\n",
        "        # prediction: [batch_size, trg_vocab_size]\n",
        "        return output, hidden, cell\n",
        "\n",
        "\n",
        "summary(\n",
        "    Decoder(EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT),\n",
        "    input_size=[\n",
        "        (BATCH_SIZE,),\n",
        "        (N_LAYERS, BATCH_SIZE, HIDDEN_DIM),\n",
        "        (N_LAYERS, BATCH_SIZE, HIDDEN_DIM),\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.float, torch.float],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNlqoep7Jug9"
      },
      "source": [
        "## Entrenamiento del Modelo\n",
        "\n",
        "Para entrenar el modelo, vamos a definir una función de pérdida y un optimizador. La función de pérdida será la entropía cruzada categórica, ya que estamos tratando con un problema de clasificación de múltiples clases. Además utilizaremos el parametro `ignore_index` del criterio de pérdida para no calcular la pérdida en los tokens de padding.\n",
        "\n",
        "El optimizador necesita los parámetros del Encoder y del Decoder, así como la tasa de aprendizaje.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "DvMGEmAbJug9"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT).to(DEVICE)\n",
        "decoder = Decoder(EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT).to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    ignore_index=TRG_VOCAB[PAD_TOKEN], label_smoothing=0.05\n",
        ")  # ignoramos el padding en el cálculo de la loss\n",
        "optimizer = optim.Adam(\n",
        "    list(encoder.parameters()) + list(decoder.parameters()), lr=LR\n",
        ")  # tomamos los parámetros de ambos modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4acStPUwJug9"
      },
      "source": [
        "### Entrenamiento de una Época en Seq2Seq con *Teacher Forcing*\n",
        "\n",
        "**Cálculo de la pérdida para cada token**\n",
        "\n",
        "A diferencia de entrenamientos comunes donde se calcula la pérdida para toda la secuencia de una vez, aquí la pérdida se calcula **token por token** dentro de un bucle. Esto permite que el decoder procese un token a la vez, prediciendo el siguiente en función del estado oculto anterior.\n",
        "\n",
        "**Uso de *Teacher Forcing***\n",
        "\n",
        "Se introduce un parámetro **`teacher_forcing_ratio`**, que controla la probabilidad de usar el token real de la secuencia objetivo como siguiente entrada. Esto difiere de los enfoques comunes donde siempre se usa la predicción del modelo para generar la siguiente entrada.\n",
        "\n",
        "**Actualización de gradientes**\n",
        "La **retropropagación** ocurre después de que se procesan todos los tokens en la secuencia, acumulando las pérdidas de cada token antes de hacer `backward()` y actualizar los parámetros con `optimizer.step()`.\n",
        "\n",
        "**Proceso resumido**\n",
        "\n",
        "1. Inicialización de `encoder` y `decoder` en modo entrenamiento.\n",
        "2. Para cada batch, se:\n",
        "   - Calcula la pérdida token por token.\n",
        "   - Decide si usar el token objetivo real o la predicción anterior.\n",
        "3. Finalmente, se acumula la pérdida total por secuencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fywAPvC7Jug9"
      },
      "outputs": [],
      "source": [
        "def process_batch(src, trg, encoder, decoder, criterion, teacher_forcing_ratio):\n",
        "    # src: [batch_size, src_len]\n",
        "    # trg: [batch_size, trg_len]\n",
        "    trg_input = trg[:, :-1]\n",
        "    trg_output = trg[:, 1:]\n",
        "    # trg_input: [batch_size, trg_len -1]\n",
        "    trg_len = trg_input.shape[1]\n",
        "    batch_size = trg_input.shape[0]\n",
        "\n",
        "    token_input = trg_input[:, 0]\n",
        "    # token_input: [batch_size]\n",
        "\n",
        "    hidden, cell = encoder(src)\n",
        "\n",
        "    batch_loss = 0\n",
        "    for i in range(trg_len):\n",
        "        output, hidden, cell = decoder(\n",
        "            token_input, hidden, cell\n",
        "        )\n",
        "\n",
        "        batch_loss += criterion(output, trg_output[:, i])\n",
        "\n",
        "        if torch.rand(1).item() < teacher_forcing_ratio:\n",
        "            token_input = trg_input[:, i]\n",
        "        else:\n",
        "            token_input = torch.argmax(output, dim=1).detach()\n",
        "\n",
        "    return batch_loss / trg_len\n",
        "        # decoder_output: [batch_size, trg_vocab_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "qB-1G1ojJug9"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "    encoder, decoder, dataloader, optimizer, criterion, teacher_forcing_ratio\n",
        "):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in dataloader:\n",
        "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        loss = process_batch(src, trg, encoder, decoder, criterion, teacher_forcing_ratio)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "_YOoX0_PJug-"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, valid_loader, criterion):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in valid_loader:\n",
        "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "            loss = process_batch(src, trg, encoder, decoder, criterion, 0)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "s7d-XGnUJug-"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    encoder,\n",
        "    decoder,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    n_epochs,\n",
        "    initial_teacher_forcing_ratio=INITIAL_TEACHER_FORCING_RATIO,\n",
        "    teacher_forcing_decay=TEACHER_FORCING_DECAY,\n",
        "    min_teacher_forcing_ratio=MIN_TEACHER_FORCING_RATIO,\n",
        "):\n",
        "    current_teacher_forcing_ratio = initial_teacher_forcing_ratio\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss = train_epoch(\n",
        "            encoder,\n",
        "            decoder,\n",
        "            train_loader,\n",
        "            optimizer,\n",
        "            criterion,\n",
        "            current_teacher_forcing_ratio,\n",
        "        )\n",
        "        val_loss = evaluate(encoder, decoder, valid_loader, criterion)\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1} FT: {current_teacher_forcing_ratio:.2f} Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}\"\n",
        "        )\n",
        "        current_teacher_forcing_ratio = max(\n",
        "            min_teacher_forcing_ratio,\n",
        "            current_teacher_forcing_ratio * teacher_forcing_decay,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "w7d636ZgJug-",
        "outputId": "d5853b83-5285-4f89-da03-2726c3a78266"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-66330074.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
            "\u001b[0;32m/tmp/ipython-input-2209175924.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, train_loader, valid_loader, optimizer, criterion, n_epochs, initial_teacher_forcing_ratio, teacher_forcing_decay, min_teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcurrent_teacher_forcing_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_teacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         train_loss = train_epoch(\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-124028987.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(encoder, decoder, dataloader, optimizer, criterion, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-972813356.py\u001b[0m in \u001b[0;36mprocess_batch\u001b[0;34m(src, trg, encoder, decoder, criterion, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1311\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3461\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3462\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3463\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(\n",
        "    encoder, decoder, train_loader, valid_loader, optimizer, criterion, n_epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_W8wkrKJug-"
      },
      "source": [
        "## Evaluación del Modelo\n",
        "\n",
        "Vamos a traducir algunas frases de prueba utilizando el modelo entrenado y evaluar la calidad de las traducciones generadas. Para esto, vamos a implementar una función `translate_sentence` que toma una oración en inglés y la traduce al español.\n",
        "\n",
        "Esta vez es el modelo quien decide cuando parar con en el token `<EOS>`, aunque también se puede limitar la longitud máxima de la traducción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMYWqsLoJug-"
      },
      "outputs": [],
      "source": [
        "def generate_sequence(src, encoder, decoder, max_len):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    generated_tokens = []\n",
        "    with torch.no_grad():\n",
        "        pass\n",
        "\n",
        "    return generated_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxBgibseJug-"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(encoder, decoder, sentence, max_len):\n",
        "    sentence = clean_text(sentence)\n",
        "    sentence_indexes = encode_sentence(sentence, SRC_VOCAB)\n",
        "    sentence_tensor = torch.tensor(sentence_indexes).unsqueeze(0).to(DEVICE)\n",
        "    generated_tokens = generate_sequence(sentence_tensor, encoder, decoder, max_len)\n",
        "    predicted_sentence = decode_sentence(generated_tokens, TRG_VOCAB)\n",
        "    return predicted_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRmJL6ypJug-"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"I am hungry\",\n",
        "    \"I am tired\",\n",
        "    \"I am happy\",\n",
        "    \"I'm sad\",\n",
        "    \"I am angry\",\n",
        "    \"every time I study, I get sleepy\",\n",
        "    \"I am going to the gym\",\n",
        "    \"I am going to the beach\",\n",
        "    \"I am going to the supermarket\",\n",
        "    \"I'm going to the movies\",\n",
        "    \"I don't know what to do\",\n",
        "    \"I love deep learning\",\n",
        "    \"I can't open the door\",\n",
        "    \"you can go if you want to\",\n",
        "    \"i'm going to the party\",\n",
        "    \"where does all this come from ?\",\n",
        "    \"I can read your mind\",\n",
        "    \"I can't believe it\",\n",
        "    \"I can't believe you\",\n",
        "    \"I can't believe this\",\n",
        "    \"I didn't like it\",\n",
        "    \"You can do it\",\n",
        "    \"Do you speak Italian?\",\n",
        "    \"Do you want to learn Spanish?\",\n",
        "    \"I want to learn French\",\n",
        "    \"Do you want to go to the movies?\",\n",
        "    \"this is my favorite song\",\n",
        "    \"I can't wait to see you\",\n",
        "    \"see you later\",\n",
        "    \"have a nice day\",\n",
        "    \"we'll talk later\",\n",
        "    \"let's grab a coffee sometime\",\n",
        "    \"they're coming to the party\",\n",
        "    \"she's my best friend\",\n",
        "    \"he's a great guy\",\n",
        "    \"My class is in 30 minutes.\",\n",
        "    \"I have class tomorrow.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Translation: {translate_sentence(encoder, decoder, sentence, 12)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWn20if2Jug-"
      },
      "outputs": [],
      "source": [
        "translate_sentence(encoder, decoder, \"we need attention\", MAX_SENTENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqlf0aHYJug-"
      },
      "outputs": [],
      "source": [
        "translate_sentence(encoder, decoder, \"let's go for it!\", MAX_SENTENCE_LENGTH)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "taller-dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}