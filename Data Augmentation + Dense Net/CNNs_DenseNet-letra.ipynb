{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10: Data Augmentation y Arquitecturas Avanzadas\n",
    "\n",
    "En esta notebook, exploraremos técnicas de **data augmentation** y el uso de **arquitecturas avanzadas** como DenseNet para mejorar el rendimiento de modelos de clasificación en el dataset CIFAR-10. Abordaremos también los desafíos de las redes CNN tradicionales y cómo las arquitecturas modernas pueden superarlos.\n",
    "\n",
    "## Introducción\n",
    "\n",
    "### Objetivos\n",
    "\n",
    "1. **Implementar y entender técnicas de data augmentation** para aumentar la diversidad del conjunto de entrenamiento y mejorar la generalización de los modelos.\n",
    "2. **Comparar arquitecturas de redes neuronales** tradicionales con arquitecturas más avanzadas como DenseNet.\n",
    "3. **Entrenar y evaluar modelos de clasificación** en PyTorch utilizando el dataset CIFAR-10.\n",
    "4. **Analizar el impacto de las técnicas de data augmentation y las arquitecturas avanzadas** en el rendimiento del modelo utilizando métricas adecuadas.\n",
    "\n",
    "### Contenido\n",
    "\n",
    "1. Configuración de bibliotecas y semillas para reproducibilidad.\n",
    "2. Familiarización con `torchvision.transforms`.\n",
    "2. Carga y exploración del dataset CIFAR-10.\n",
    "3. Aplicación de técnicas de data augmentation.\n",
    "4. Implementación y comparación de modelos CNN tradicionales y DenseNet.\n",
    "5. Evaluación y comparación del rendimiento de los modelos con y sin data augmentation y utilizando diferentes arquitecturas.\n",
    "\n",
    "### Sobre el conjunto de datos\n",
    "\n",
    "El dataset CIFAR-10 es un conjunto de datos muy utilizado en la investigación de la visión por computadora. Contiene 60,000 imágenes de 32x32 píxeles en 10 clases diferentes, con 6,000 imágenes por clase. Este conjunto de datos es ideal para entrenar y evaluar modelos de clasificación, permitiendo explorar la efectividad de diversas técnicas de aumento de datos y arquitecturas avanzadas.\n",
    "\n",
    "El modelo está disponible en el siguiente [link](https://www.cs.toronto.edu/~kriz/cifar.html), pero PyTorch proporciona una interfaz para descargar y cargar el conjunto de datos de forma sencilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from utils import (\n",
    "    train,\n",
    "    plot_training,\n",
    "    model_classification_report,\n",
    "    show_tensor_image,\n",
    "    show_tensor_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la semilla para que los resultados sean reproducibles\n",
    "SEED = 23\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "elif torch.xpu.is_available():\n",
    "    DEVICE = \"xpu\"  # si no hay GPU, pero hay XPU, usamos XPU\n",
    "\n",
    "print(f\"Usando {DEVICE}\")\n",
    "\n",
    "NUM_WORKERS = 0 # Win y MacOS pueden tener problemas con múltiples workers\n",
    "if sys.platform == 'linux':\n",
    "    NUM_WORKERS = 4  # numero de workers para cargar los datos (depende de cada caso)\n",
    "\n",
    "print(f\"Usando {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512  # tamaño del batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "El **data augmentation** es una técnica de **regularización** que consiste en aplicar transformaciones aleatorias a las imágenes de entrenamiento, como rotaciones, traslaciones, reflejos, recortes, etc. Esto aumenta la diversidad del conjunto de entrenamiento y ayuda a mejorar la generalización del modelo.\n",
    "\n",
    "PyTorch proporciona una interfaz sencilla para aplicar data augmentation a través de la clase [`torchvision.transforms`](https://pytorch.org/vision/main/transforms.html). Primero tomemos una imagen de ejemplo y apliquemos algunas transformaciones comunes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = read_image(str(Path(\"assets\") / \"dwight.jpg\"))\n",
    "show_tensor_image(img, title=\"Original Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las transformaciones más comunes incluyen:\n",
    "\n",
    "- [`RandomHorizontalFlip`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomHorizontalFlip.html): Voltea horizontalmente la imagen con una probabilidad dada.\n",
    "- [`RandomCrop`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomCrop.html): Recorta la imagen de forma aleatoria.\n",
    "- [`RandomRotation`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomRotation.html): Rota la imagen de forma aleatoria.\n",
    "- [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ColorJitter.html): Cambia el brillo, contraste, saturación y tono de la imagen de forma aleatoria.\n",
    "- [`RandomResizedCrop`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomResizedCrop.html): Recorta y cambia el tamaño de la imagen de forma aleatoria.\n",
    "- ...\n",
    "\n",
    "Se puede ver la lista completa de transformaciones en la [documentación oficial](https://pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rhf = T.RandomHorizontalFlip(p=1)(img)  # p=1 para que siempre se aplique\n",
    "img_rc = T.RandomCrop(size=(100, 100))(img)  # tomamos un recorte de 100x100\n",
    "img_rr = T.RandomRotation(degrees=45)(\n",
    "    img\n",
    ")  # rotamos la imagen de forma aleatoria entre -45 y 45 grados\n",
    "img_cj = T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)(\n",
    "    img\n",
    ")  # aplicamos un cambio de color\n",
    "img_rrc = T.RandomResizedCrop(size=(200, 200))(\n",
    "    img\n",
    ")  # tomamos un recorte y redimensionamos a 200x200\n",
    "\n",
    "show_tensor_images(\n",
    "    [img, img_rhf, img_rc, img_rr, img_cj, img_rrc],\n",
    "    titles=[\n",
    "        f\"original ({img.shape[-2]}x{img.shape[-1]})\",\n",
    "        f\"HorizontalFlip ({img_rhf.shape[-2]}x{img_rhf.shape[-1]})\",\n",
    "        f\"Crop ({img_rc.shape[-2]}x{img_rc.shape[-1]})\",\n",
    "        f\"Rotation ({img_rr.shape[-2]}x{img_rr.shape[-1]})\",\n",
    "        f\"ColorJitter ({img_cj.shape[-2]}x{img_cj.shape[-1]})\",\n",
    "        f\"ResizedCrop ({img_rrc.shape[-2]}x{img_rrc.shape[-1]})\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tamibén se pueden combinar transformaciones utilizando [`Compose`](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Compose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = T.Compose([T.RandomHorizontalFlip(p=1), T.CenterCrop(size=(25, 100))])\n",
    "\n",
    "img_composed = composed(img)\n",
    "show_tensor_image(img_composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general vamos a aplicar las transformaciones a las imágenes de entrenamiento, pero no a las de validación y test. Por lo tanto, es importante tener dos pipelines de transformaciones, uno para entrenamiento y otro para validación/test.\n",
    "\n",
    "> Es importante considerar que no todas las transformaciones son adecuadas para todas las tareas. Por ejemplo, en el caso del dataset MNIST, no se debería aplicar `RandomHorizontalFlip` ya que los dígitos perderían su significado al voltearse horizontalmente. De manera similar, usar `RandomVerticalFlip` podría causar problemas con dígitos como el 6 y el 9, que se confundirían al invertirse verticalmente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y DataLoader\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "El dataset [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) es un conjunto de datos ampliamente utilizado en el campo del aprendizaje automático y la visión por computadora. Contiene 60,000 imágenes a color divididas en 10 clases, tales como aviones, automóviles, pájaros, gatos y perros. Cada imagen es de 32x32 píxeles, lo que lo hace ideal para probar algoritmos de clasificación de imágenes.\n",
    "\n",
    "Para facilitar el manejo del dataset CIFAR-10, utilizaremos la biblioteca `torchvision.datasets`, la cual proporciona una manera sencilla de cargar y preprocesar estos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "Para facilitar los experimentos y el entrenamiento de modelos vamos a definir una función `get_dataloaders` que nos devolverá los DataLoaders para los conjuntos de entrenamiento, validación y test con las transformaciones deseadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# Wrapper para aplicar transformaciones\n",
    "class TransformDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.subset[idx]\n",
    "        return self.transform(image), label\n",
    "\n",
    "def get_dataloaders(\n",
    "    train_transf, test_transf, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS\n",
    "):\n",
    "    \"\"\"\n",
    "    Función para obtener los dataloaders de entrenamiento, validación y test\n",
    "\n",
    "    Args:\n",
    "    - train_transf: transformaciones para el dataset de entrenaiento\n",
    "    - test_transf: transformaciones para el dataset de test\n",
    "    - batch_size: tamaño del batch\n",
    "    - num_workers: número de workers para cargar los datos\n",
    "    \"\"\"\n",
    "\n",
    "    # descargamos el dataset CIFAR10 (si no lo tenemos ya)\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        DATA_DIR, train=True, download=True, transform=None\n",
    "    )\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        DATA_DIR, train=False, download=True, transform=test_transf\n",
    "    )\n",
    "\n",
    "    # dividimos el dataset de entrenamiento en entrenamiento y validación\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    valid_size = len(train_dataset) - train_size\n",
    "    train_dataset, validation_dataset = random_split(\n",
    "        train_dataset,\n",
    "        [train_size, valid_size],\n",
    "        generator=torch.Generator().manual_seed(SEED),  # fijamos la semilla\n",
    "    )\n",
    "\n",
    "    # aplicamos las transformaciones\n",
    "    train_dataset = TransformDataset(train_dataset, train_transf)\n",
    "    validation_dataset = TransformDataset(validation_dataset, test_transf)\n",
    "\n",
    "    # creamos los dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASES = 10  # número de clases en CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones\n",
    "\n",
    "Vamos a definir 3 pipelines de transformaciones:\n",
    "\n",
    "1. **Transformaciones sin data augmentation**: Estas transformaciones se aplican al conjunto de entrenamiento sin utilizar técnicas de data augmentation. Esto nos proporciona un punto de referencia sobre el rendimiento del modelo con datos sin alterar.\n",
    "2. **Transformaciones con data augmentation**: Estas transformaciones se aplican al conjunto de entrenamiento utilizando técnicas de data augmentation. La data augmentation ayuda a aumentar la variabilidad de los datos de entrenamiento, mejorando la capacidad del modelo para generalizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR10(\n",
    "    DATA_DIR, train=True, download=True, transform=T.Compose([\n",
    "        T.ToImage(),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Stack todas las imágenes en un tensor\n",
    "data = torch.stack([img for img, _ in train_dataset])\n",
    "# Shape: (50000, 3, 32, 32)\n",
    "\n",
    "# Calcula mean y std por canal\n",
    "T_MEAN = data.mean(dim=[0, 2, 3])  # Promedia sobre samples, height, width\n",
    "T_STD = data.std(dim=[0, 2, 3])\n",
    "\n",
    "print(f\"Mean: {T_MEAN}\")\n",
    "print(f\"Std: {T_STD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones con data augmentation\n",
    "train_transforms_aug = T.Compose(\n",
    "    [\n",
    "        # Data augmentation:\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomRotation(degrees=10),\n",
    "        # Transformaciones comunes:\n",
    "        T.ToImage(),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(mean=T_MEAN, std=T_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Transformaciones con data augmentation\n",
    "train_transforms_no_aug = T.Compose(\n",
    "    [\n",
    "        # Transformaciones comunes:\n",
    "        T.ToImage(),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(mean=T_MEAN, std=T_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Transformaciones sin data augmentation\n",
    "test_transform = T.Compose(\n",
    "    [\n",
    "        T.ToImage(),  # tenemos que convertir la imagen PIL a tensor\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(mean=T_MEAN, std=T_STD),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Data Augmentation\n",
    "\n",
    "En este experimento, compararemos el rendimiento de un modelo de clasificación de imágenes en el dataset CIFAR-10 con y sin data augmentation. Utilizaremos una arquitectura de red neuronal convolucional tradicional y evaluaremos el rendimiento en términos de precisión y pérdida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo base: LeNet\n",
    "\n",
    "Tomemos [LeNet](https://d2l.ai/chapter_convolutional-neural-networks/lenet.html) para este experimento. LeNet es una red neuronal convolucional simple y nos permitirá comparar el impacto de la data augmentation en un modelo básico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=6, kernel_size=5)\n",
    "        self.s2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.c3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.s4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.c5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5)\n",
    "        self.f6 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.output = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.c1(x))\n",
    "        x = self.s2(x)\n",
    "        x = F.tanh(self.c3(x))\n",
    "        x = self.s4(x)\n",
    "        x = F.tanh(self.c5(x))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.tanh(self.f6(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "summary(LeNet(3, NUM_CLASES), input_size=(BATCH_SIZE, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definicion de hiperparametros\n",
    "LR = 0.001\n",
    "EPOCHS = 30\n",
    "# definimos\n",
    "CRITERION = nn.CrossEntropyLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento sin data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    train_transforms_no_aug, test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letnet_model = LeNet(3, NUM_CLASES).to(DEVICE)\n",
    "optimizer = optim.Adam(letnet_model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors_ln_nda, val_errors_ln_nda = train(\n",
    "    model=letnet_model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=CRITERION,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    do_early_stopping=True,\n",
    "    patience=3,\n",
    "    epochs=EPOCHS,\n",
    ")\n",
    "\n",
    "plot_training(train_errors_ln_nda, val_errors_ln_nda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification_report(letnet_model, test_loader, DEVICE, NUM_CLASES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento con data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letnet_model = LeNet(3, NUM_CLASES).to(DEVICE)\n",
    "optimizer = optim.Adam(letnet_model.parameters(), lr=LR)\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    train_transforms_aug, test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors_ln_da, val_errors_ln_da = train(\n",
    "    model=letnet_model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=CRITERION,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    do_early_stopping=True,\n",
    "    patience=3,\n",
    "    epochs=EPOCHS,\n",
    ")\n",
    "\n",
    "plot_training(train_errors_ln_da, val_errors_ln_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification_report(letnet_model, test_loader, DEVICE, NUM_CLASES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1. ¿Qué mejoras observas al utilizar data augmentation en este experimento?\n",
    "2. Experimenta con diferentes transformaciones y evalúa su impacto en el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Ej1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Ej2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: DenseNet\n",
    "\n",
    "[DenseNet](https://d2l.ai/chapter_convolutional-modern/densenet.html) es una arquitectura de red neuronal convolucional que aborda los problemas de las redes tradicionales conectando cada capa con todas las capas subsiguientes. Esto permite que la información fluya de manera más eficiente a través de la red y facilita el entrenamiento de redes profundas.\n",
    "\n",
    "Se recomienda leer los articulos:\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) \n",
    "- [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura DenseNet\n",
    "\n",
    "Podemos dividir DenseNet en cuatro partes principales:\n",
    "- **Convolucion inicial**: Una sola capa convolucional seguida de una capa de pooling.\n",
    "- **Bloques densos**: Varios bloques densos, cada uno compuesto por múltiples capas convolucionales conectadas entre sí.\n",
    "- **Transiciones**: Capas convolucionales que reducen la altura y el ancho de las activaciones.\n",
    "- **Capa de salida**: Una capa de pooling global y una capa completamente conectada.\n",
    "\n",
    "![Image](https://miro.medium.com/max/5164/1*_Y7-f9GpV7F93siM1js0cg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloque denso\n",
    "\n",
    "Los [bloques densos](https://d2l.ai/chapter_convolutional-modern/densenet.html#dense-blocks) son la característica principal de DenseNet. Cada bloque denso conecta cada capa con todas las capas subsiguientes en un patrón denso (de ahí el nombre). Esto permite que la información fluya de manera más eficiente a través de la red y facilita el entrenamiento de redes profundas.\n",
    "\n",
    "Los bloques densos están compuestos por capas densa que constan de:\n",
    "- Una capa convolucional.\n",
    "- Un capa de Batch Normalization.\n",
    "- Una función de activación ReLU.\n",
    "\n",
    "Luego en el forward se concatenan la salida con la entrada usando la función [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html).\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-20_at_11.33.17_PM_Mt0HOZL.png\" width=\"500\">\n",
    "\n",
    "La forma de controlar la cantidad de canales en cada capa es mediante el parámetro `growth_rate`, que indica cuántos canales se agregarán en cada capa convolucional dentro de un bloque denso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels  = 6  # número de canales de entrada\n",
    "out_channels = 4 # número de canales de salida\n",
    "dense_layer = DenseLayer(in_channels=in_channels, out_channels=out_channels)\n",
    "summary(dense_layer, input_size=(BATCH_SIZE, in_channels, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels, growth_rate):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 6  # número de canales de entrada\n",
    "num_layer   = 5  # número de capas en el bloque denso\n",
    "growth_rate = 4  # tasa de crecimiento\n",
    "\n",
    "dense_block = DenseBlock(num_layer, in_channels, growth_rate)\n",
    "\n",
    "summary(dense_block, input_size=(BATCH_SIZE, in_channels, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transiciones\n",
    "\n",
    "Los bloques densos pueden generar una gran cantidad de canales, lo que puede aumentar significativamente la complejidad del modelo. Para reducir la cantidad de canales y controlar la complejidad del modelo, DenseNet utiliza [capas de transición](https://d2l.ai/chapter_convolutional-modern/densenet.html#transition-layers) entre los bloques densos. Estas capas consisten en una capa convolucional seguida de una capa de pooling.\n",
    "\n",
    "Por lo cual:\n",
    "- Reducen la cantidad de canales a través de una capa convolucional.\n",
    "- Reducen la altura y el ancho de las activaciones (downsampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 100  # número de canales de entrada\n",
    "out_channels = 25  # número de canales de salida (en general, se reduce a la mitad)\n",
    "img_w = 32  # ancho de la imagen\n",
    "img_h = 32  # alto de la imagen\n",
    "\n",
    "transition_layer = TransitionLayer(in_channels, out_channels)\n",
    "\n",
    "summary(transition_layer, input_size=(BATCH_SIZE, in_channels, img_w, img_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, num_layers = [6, 12, 24, 16], growth_rate=32, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "        self.num_layers = num_layers\n",
    "        self.num_blocks = len(num_layers)\n",
    "\n",
    "        # Capa inicial: se encarga de extraer características iniciales\n",
    "\n",
    "        # Bloques densos y capas de transición en nn.Sequential\n",
    "        \n",
    "        # Capa de clasificación\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "model = DenseNet(num_layers=[6, 12, 24, 16], growth_rate=32, num_classes=10)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 3, 32, 32), depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(\n",
    "    num_layers=[6, 12, 24, 16], growth_rate=12, num_classes=10\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_errors_dn, val_errors_dn = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=CRITERION,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    do_early_stopping=True,\n",
    "    patience=3,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(train_errors_dn, val_errors_dn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification_report(model, test_loader, DEVICE, NUM_CLASES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1. Utilizar otras transformaciones para data augmentation y evaluar su impacto en el rendimiento del modelo.\n",
    "2. Experimentar con diferentes números de bloques densos y growth rates en DenseNet.\n",
    "3. Utilizar Dropout y evaluar su impacto en el rendimiento del modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
