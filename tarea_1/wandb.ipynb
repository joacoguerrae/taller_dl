{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1 - Taller de Deep Learning\n",
    "\n",
    "**Fecha de entrega: 19/10/2025**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2 as T\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import utils\n",
    "import sys\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la ki\n",
    "cb879b8473582418585ac8a9fcb1e1695ebcdf97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WANDB_TEAM_NAME = \"[joacoguerrae-ort]\"\n",
    "WANDB_PROJECT = \"[wandb-labo1-guerra-sanes]\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando cuda\n",
      "Usando 4\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "elif torch.xpu.is_available():\n",
    "    DEVICE = \"xpu\"  # si no hay GPU, pero hay XPU, usamos XPU\n",
    "\n",
    "print(f\"Usando {DEVICE}\")\n",
    "\n",
    "NUM_WORKERS = 0 # Win y MacOS pueden tener problemas con múltiples workers\n",
    "if sys.platform == 'linux':\n",
    "    NUM_WORKERS = 4  # numero de workers para cargar los datos (depende de cada caso)\n",
    "\n",
    "print(f\"Usando {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "\n",
    "\n",
    "imagenette_dataset = datasets.Imagenette(\n",
    "    DATA_DIR, download=False\n",
    ")\n",
    "\n",
    "transforms = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.Grayscale(),\n",
    "    T.ToDtype(torch.float32, scale=True),\n",
    "    T.Resize((224, 224))\n",
    "\n",
    "])\n",
    "\n",
    "imagenette_dataset_train = datasets.Imagenette(\n",
    "    DATA_DIR, download=False,split = 'train', transform=transforms\n",
    ")\n",
    "\n",
    "imagenette_dataset_val = datasets.Imagenette(\n",
    "    DATA_DIR, download=False,split = 'val', transform=transforms\n",
    ")\n",
    "\n",
    "name_classes = imagenette_dataset.classes\n",
    "nclasses = len(name_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenette_dataset_train, imagenette_dataset_test = random_split(\n",
    "    imagenette_dataset_train, [0.8, 0.2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128  # tamaño del batch\n",
    "def get_dataloaders(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    train_loader = DataLoader(\n",
    "        imagenette_dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        imagenette_dataset_val,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        imagenette_dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegularizedCNN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout2d(p=0.1, inplace=False)\n",
      "    (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout2d(p=0.2, inplace=False)\n",
      "    (10): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Dropout2d(p=0.25, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=50176, out_features=512, bias=True)\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Dropout(p=0.5, inplace=False)\n",
      "    (9): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RegularizedCNN(nn.Module):\n",
    "    def __init__(self, input, n_classes, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # 224*224\n",
    "            nn.Conv2d(input[0], 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),  # Batch Normalization\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.1),  # Dropout espacial\n",
    "            \n",
    "            # 112*112\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            # 56*56\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.25),\n",
    "            # 28*28\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 28 * 28, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            \n",
    "            nn.Linear(256, n_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Crear el modelo con regularización\n",
    "modelo_regularizado = RegularizedCNN(train_loader.dataset[0][0].shape, nclasses, dropout_rate=0.5).to(DEVICE)\n",
    "print(modelo_regularizado)\n",
    "\n",
    "# Optimizer con weight decay (L2 regularization)\n",
    "optimizer_reg = optim.Adam(modelo_regularizado.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración mejorada del sweep con más parámetros\n",
    "sweep_config_improved = {\n",
    "    \"name\": \"sweep-imagenette-regularized\",\n",
    "    \"method\": \"bayes\",  # Bayesian optimization es más eficiente que random\n",
    "    \"metric\": {\"name\": \"val_loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1e-2\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"adam\", \"sgd\", \"adamw\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [32, 64, 128, 256]\n",
    "        },\n",
    "        \"dropout_rate\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.2,\n",
    "            \"max\": 0.7\n",
    "        },\n",
    "        \"weight_decay\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-6,\n",
    "            \"max\": 1e-3\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.85,\n",
    "            \"max\": 0.99\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"value\": 30\n",
    "        }\n",
    "    },\n",
    "    \"early_terminate\": {\n",
    "        \"type\": \"hyperband\",\n",
    "        \"min_iter\": 5\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_wandb(config=None):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento que será llamada por wandb.agent para cada configuración del sweep.\n",
    "    \"\"\"\n",
    "    with wandb.init(config=config):\n",
    "        # Obtener la configuración del sweep\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Crear dataloaders con el batch_size del sweep\n",
    "        train_loader_sweep, val_loader_sweep, test_loader_sweep = get_dataloaders(\n",
    "            batch_size=config.batch_size, \n",
    "            num_workers=NUM_WORKERS\n",
    "        )\n",
    "        \n",
    "        # Crear el modelo con los parámetros del sweep\n",
    "        model = RegularizedCNN(\n",
    "            train_loader_sweep.dataset[0][0].shape, \n",
    "            nclasses, \n",
    "            dropout_rate=config.dropout_rate\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Definir el optimizador según la configuración\n",
    "        if config.optimizer == \"adam\":\n",
    "            optimizer = optim.Adam(\n",
    "                model.parameters(), \n",
    "                lr=config.learning_rate, \n",
    "                weight_decay=config.weight_decay\n",
    "            )\n",
    "        elif config.optimizer == \"sgd\":\n",
    "            optimizer = optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=config.learning_rate, \n",
    "                momentum=config.momentum,\n",
    "                weight_decay=config.weight_decay\n",
    "            )\n",
    "        elif config.optimizer == \"adamw\":\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(), \n",
    "                lr=config.learning_rate, \n",
    "                weight_decay=config.weight_decay\n",
    "            )\n",
    "        \n",
    "        # Criterio de pérdida\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Entrenar el modelo\n",
    "        train_errors, val_errors = utils.train(\n",
    "            model, \n",
    "            optimizer=optimizer, \n",
    "            criterion=criterion, \n",
    "            do_early_stopping=True,\n",
    "            train_loader=train_loader_sweep, \n",
    "            val_loader=val_loader_sweep, \n",
    "            device=DEVICE, \n",
    "            epochs=config.epochs,\n",
    "            patience=10\n",
    "        )\n",
    "        \n",
    "        # Evaluar en el conjunto de test al final\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader_sweep:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        test_accuracy = 100. * correct / total\n",
    "        test_loss = test_loss / len(test_loader_sweep)\n",
    "        \n",
    "        # Log de métricas finales\n",
    "        wandb.log({\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"test_loss\": test_loss\n",
    "        })\n",
    "        \n",
    "        print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ruprmlf1\n",
      "Sweep URL: https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1\n",
      "Sweep ID: ruprmlf1\n",
      "Para ejecutar el sweep, usa: wandb.agent('ruprmlf1', function=train_with_wandb, count=20)\n"
     ]
    }
   ],
   "source": [
    "# Crear el sweep\n",
    "sweep_id_improved = wandb.sweep(sweep_config_improved, project=WANDB_PROJECT)\n",
    "\n",
    "print(f\"Sweep ID: {sweep_id_improved}\")\n",
    "print(f\"Para ejecutar el sweep, usa: wandb.agent('{sweep_id_improved}', function=train_with_wandb, count=20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lxpu3r81 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3364040180623441\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.895654925937952e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9441812915072618\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 4.035390083182272e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_202441-lxpu3r81</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/lxpu3r81' target=\"_blank\">happy-sweep-1</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/lxpu3r81' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/lxpu3r81</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train Loss: 2.31497 | Val Loss: 2.13094\n",
      "Epoch: 002 | Train Loss: 2.14486 | Val Loss: 2.03087\n",
      "Epoch: 003 | Train Loss: 2.07387 | Val Loss: 1.98865\n",
      "Epoch: 004 | Train Loss: 1.99345 | Val Loss: 1.92981\n",
      "Epoch: 005 | Train Loss: 1.94848 | Val Loss: 1.89714\n",
      "Epoch: 006 | Train Loss: 1.90319 | Val Loss: 1.87722\n",
      "Epoch: 007 | Train Loss: 1.88110 | Val Loss: 1.85146\n",
      "Epoch: 008 | Train Loss: 1.84587 | Val Loss: 1.81214\n",
      "Epoch: 009 | Train Loss: 1.82611 | Val Loss: 1.80435\n",
      "Epoch: 010 | Train Loss: 1.80061 | Val Loss: 1.77857\n",
      "Epoch: 011 | Train Loss: 1.77879 | Val Loss: 1.76621\n",
      "Epoch: 012 | Train Loss: 1.76876 | Val Loss: 1.75522\n",
      "Epoch: 013 | Train Loss: 1.74998 | Val Loss: 1.74329\n",
      "Epoch: 014 | Train Loss: 1.73451 | Val Loss: 1.72145\n",
      "Epoch: 015 | Train Loss: 1.71563 | Val Loss: 1.72195\n",
      "Epoch: 016 | Train Loss: 1.70501 | Val Loss: 1.71015\n",
      "Epoch: 017 | Train Loss: 1.69057 | Val Loss: 1.70866\n",
      "Epoch: 018 | Train Loss: 1.67286 | Val Loss: 1.68675\n",
      "Epoch: 019 | Train Loss: 1.64962 | Val Loss: 1.68449\n",
      "Epoch: 020 | Train Loss: 1.64647 | Val Loss: 1.66516\n",
      "Epoch: 021 | Train Loss: 1.63121 | Val Loss: 1.66868\n",
      "Epoch: 022 | Train Loss: 1.62271 | Val Loss: 1.65982\n",
      "Epoch: 023 | Train Loss: 1.60503 | Val Loss: 1.64711\n",
      "Epoch: 024 | Train Loss: 1.60324 | Val Loss: 1.64758\n",
      "Epoch: 025 | Train Loss: 1.58806 | Val Loss: 1.62898\n",
      "Epoch: 026 | Train Loss: 1.57595 | Val Loss: 1.62573\n",
      "Epoch: 027 | Train Loss: 1.56542 | Val Loss: 1.63167\n",
      "Epoch: 028 | Train Loss: 1.55641 | Val Loss: 1.63034\n",
      "Epoch: 029 | Train Loss: 1.54502 | Val Loss: 1.61939\n",
      "Epoch: 030 | Train Loss: 1.53229 | Val Loss: 1.61680\n",
      "Test Accuracy: 45.17%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>45.1664</td></tr><tr><td>test_loss</td><td>1.62211</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-sweep-1</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/lxpu3r81' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/lxpu3r81</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_202441-lxpu3r81/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6xsai3nr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2924063115158868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.4554731252598313e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9416585030528744\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1.730447508059195e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_203254-6xsai3nr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/6xsai3nr' target=\"_blank\">iconic-sweep-2</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/6xsai3nr' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/6xsai3nr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train Loss: 2.03194 | Val Loss: 1.86315\n",
      "Epoch: 002 | Train Loss: 1.83643 | Val Loss: 1.77011\n",
      "Epoch: 003 | Train Loss: 1.73517 | Val Loss: 1.71309\n",
      "Epoch: 004 | Train Loss: 1.65914 | Val Loss: 1.66694\n",
      "Epoch: 005 | Train Loss: 1.60373 | Val Loss: 1.64316\n",
      "Epoch: 006 | Train Loss: 1.55292 | Val Loss: 1.60335\n",
      "Epoch: 007 | Train Loss: 1.49945 | Val Loss: 1.57747\n",
      "Epoch: 008 | Train Loss: 1.44319 | Val Loss: 1.58158\n",
      "Epoch: 009 | Train Loss: 1.39027 | Val Loss: 1.52805\n",
      "Epoch: 010 | Train Loss: 1.33288 | Val Loss: 1.50865\n",
      "Epoch: 011 | Train Loss: 1.27876 | Val Loss: 1.49514\n",
      "Epoch: 012 | Train Loss: 1.21456 | Val Loss: 1.47717\n",
      "Epoch: 013 | Train Loss: 1.14955 | Val Loss: 1.45166\n",
      "Epoch: 014 | Train Loss: 1.09358 | Val Loss: 1.43346\n",
      "Epoch: 015 | Train Loss: 1.03017 | Val Loss: 1.43935\n",
      "Epoch: 016 | Train Loss: 0.98483 | Val Loss: 1.43629\n",
      "Epoch: 017 | Train Loss: 0.92003 | Val Loss: 1.41972\n",
      "Epoch: 018 | Train Loss: 0.85659 | Val Loss: 1.40816\n",
      "Epoch: 019 | Train Loss: 0.79634 | Val Loss: 1.43668\n",
      "Epoch: 020 | Train Loss: 0.75546 | Val Loss: 1.41235\n",
      "Epoch: 021 | Train Loss: 0.70087 | Val Loss: 1.43439\n",
      "Epoch: 022 | Train Loss: 0.64449 | Val Loss: 1.44608\n",
      "Epoch: 023 | Train Loss: 0.58586 | Val Loss: 1.44421\n",
      "Epoch: 024 | Train Loss: 0.54581 | Val Loss: 1.45522\n",
      "Epoch: 025 | Train Loss: 0.50868 | Val Loss: 1.46342\n",
      "Epoch: 026 | Train Loss: 0.46914 | Val Loss: 1.46119\n",
      "Epoch: 027 | Train Loss: 0.43915 | Val Loss: 1.47445\n",
      "Epoch: 028 | Train Loss: 0.39776 | Val Loss: 1.54600\n",
      "Detener entrenamiento en la época 27, la mejor pérdida fue 1.40816\n",
      "Test Accuracy: 50.82%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>50.81881</td></tr><tr><td>test_loss</td><td>1.52343</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">iconic-sweep-2</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/6xsai3nr' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/6xsai3nr</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_203254-6xsai3nr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sxjmcs5l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.4840139322498795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.14044460669411e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.8905482421013443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0006180221751276399\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204107-sxjmcs5l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/sxjmcs5l' target=\"_blank\">blooming-sweep-3</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/sxjmcs5l' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/sxjmcs5l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 47, in train_with_wandb\n",
      "    train_errors, val_errors = utils.train(\n",
      "                               ^^^^^^^^^^^^\n",
      "  File \"/home/rami/Documents/ORT/taller_dl/tarea_1/utils.py\", line 109, in train\n",
      "    output = model(x)  # forward pass (prediccion)\n",
      "             ^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_35337/2780542272.py\", line 44, in forward\n",
      "    x = self.conv(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/functional.py\", line 2812, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 73.44 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-sweep-3</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/sxjmcs5l' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/sxjmcs5l</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204107-sxjmcs5l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 47, in train_with_wandb\n",
      "    train_errors, val_errors = utils.train(\n",
      "                               ^^^^^^^^^^^^\n",
      "  File \"/home/rami/Documents/ORT/taller_dl/tarea_1/utils.py\", line 109, in train\n",
      "    output = model(x)  # forward pass (prediccion)\n",
      "             ^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_35337/2780542272.py\", line 44, in forward\n",
      "    x = self.conv(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/functional.py\", line 2812, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 73.44 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run sxjmcs5l errored: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 73.44 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: li5ek3hq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3631255603469864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00016643977069202766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.958614479457634\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00014539565355512244\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204117-li5ek3hq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/li5ek3hq' target=\"_blank\">lucky-sweep-4</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/li5ek3hq' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/li5ek3hq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 86.56 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lucky-sweep-4</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/li5ek3hq' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/li5ek3hq</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204117-li5ek3hq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 86.56 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run li5ek3hq errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 86.56 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tsrl2xwo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.35599937373182355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0010784807662166617\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9866285715737964\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0006197772890922796\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204123-tsrl2xwo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/tsrl2xwo' target=\"_blank\">desert-sweep-5</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/tsrl2xwo' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/tsrl2xwo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 95.50 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 3.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">desert-sweep-5</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/tsrl2xwo' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/tsrl2xwo</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204123-tsrl2xwo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 95.50 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 3.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run tsrl2xwo errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 95.50 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 3.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bl0wr8fg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.4464301304571894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.808684286773261e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.955641963155214\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.000682175128924977\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204128-bl0wr8fg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/bl0wr8fg' target=\"_blank\">fallen-sweep-6</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/bl0wr8fg' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/bl0wr8fg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 115.25 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fallen-sweep-6</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/bl0wr8fg' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/bl0wr8fg</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204128-bl0wr8fg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 115.25 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run bl0wr8fg errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 115.25 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ae0ea6yu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.530156411785668\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004974985549309319\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9829455564447744\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1.55862439724054e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204133-ae0ea6yu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/ae0ea6yu' target=\"_blank\">dandy-sweep-7</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/ae0ea6yu' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/ae0ea6yu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 97.38 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dandy-sweep-7</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/ae0ea6yu' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/ae0ea6yu</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204133-ae0ea6yu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 97.38 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ae0ea6yu errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 97.38 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nmdsuriz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.4011605561861814\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001222348305053245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.926483477582984\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00010373191802237866\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204147-nmdsuriz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/nmdsuriz' target=\"_blank\">peachy-sweep-8</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/nmdsuriz' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/nmdsuriz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 127.00 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peachy-sweep-8</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/nmdsuriz' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/nmdsuriz</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204147-nmdsuriz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 127.00 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run nmdsuriz errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 127.00 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6b2qp3ae with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5817969476720458\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.34083928297296e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.917580228801368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00015568691086427893\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204153-6b2qp3ae</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/6b2qp3ae' target=\"_blank\">dulcet-sweep-9</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/6b2qp3ae' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/6b2qp3ae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 81.56 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dulcet-sweep-9</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/6b2qp3ae' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/6b2qp3ae</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204153-6b2qp3ae/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 81.56 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 6b2qp3ae errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 81.56 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gtt3719e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.6433455004178749\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00010752557766570928\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.8779330827393935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 2.9547314671749608e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204159-gtt3719e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/gtt3719e' target=\"_blank\">dauntless-sweep-10</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/gtt3719e' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/gtt3719e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 122.75 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dauntless-sweep-10</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/gtt3719e' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/gtt3719e</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204159-gtt3719e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 122.75 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run gtt3719e errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 122.75 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: giwt7734 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.24979925062292144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00016568628617373663\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.8837362031707213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 4.176280378335339e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204204-giwt7734</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/giwt7734' target=\"_blank\">eager-sweep-11</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/giwt7734' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/giwt7734</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 107.94 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sweep-11</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/giwt7734' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/giwt7734</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204204-giwt7734/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 107.94 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run giwt7734 errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 107.94 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k6nilhc5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.41981904144913185\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001380921438605582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9736874774559316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 5.1616906201247405e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204209-k6nilhc5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/k6nilhc5' target=\"_blank\">logical-sweep-12</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/k6nilhc5' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/k6nilhc5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 105.31 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-sweep-12</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/k6nilhc5' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/k6nilhc5</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204209-k6nilhc5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 105.31 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run k6nilhc5 errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 105.31 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jhwp2pqn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5324128164696064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0004446500888012665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9116527025246038\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0008671385037619237\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204215-jhwp2pqn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/jhwp2pqn' target=\"_blank\">sweet-sweep-13</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/jhwp2pqn' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/jhwp2pqn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 86.62 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweet-sweep-13</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/jhwp2pqn' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/jhwp2pqn</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204215-jhwp2pqn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 86.62 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jhwp2pqn errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 86.62 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c55gdhpt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.4854325454231369\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.680161716012681e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.943332816476828\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00015926286466800706\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204220-c55gdhpt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/c55gdhpt' target=\"_blank\">pious-sweep-14</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/c55gdhpt' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/c55gdhpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 88.06 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pious-sweep-14</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/c55gdhpt' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/c55gdhpt</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204220-c55gdhpt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 88.06 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run c55gdhpt errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 88.06 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 2.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 112l5hy0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.31410063493699053\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0015129232334257128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9073114904437783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1.486902885741746e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204226-112l5hy0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/112l5hy0' target=\"_blank\">grateful-sweep-15</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/112l5hy0' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/112l5hy0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 134.75 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-sweep-15</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/112l5hy0' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/112l5hy0</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204226-112l5hy0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 134.75 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 112l5hy0 errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 134.75 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uxktomtj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.43247422153576953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.620194357415125e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9258772713347582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1.9120959037607267e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204231-uxktomtj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/uxktomtj' target=\"_blank\">denim-sweep-16</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/uxktomtj' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/uxktomtj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 140.69 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-sweep-16</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/uxktomtj' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/uxktomtj</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204231-uxktomtj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 140.69 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run uxktomtj errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 140.69 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ataegh74 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.31601480213532973\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.0167095620354809e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.8578737387289618\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1.871558723089128e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204237-ataegh74</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/ataegh74' target=\"_blank\">easy-sweep-17</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/ataegh74' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/ataegh74</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 79.81 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-sweep-17</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/ataegh74' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/ataegh74</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204237-ataegh74/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 79.81 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ataegh74 errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 79.81 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b4e4xy29 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.35655883705950087\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.86041357311851e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.944342629876386\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 2.1200477341013142e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204242-b4e4xy29</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/b4e4xy29' target=\"_blank\">fancy-sweep-18</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/b4e4xy29' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/b4e4xy29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 109.00 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-sweep-18</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/b4e4xy29' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/b4e4xy29</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204242-b4e4xy29/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 109.00 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run b4e4xy29 errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 109.00 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b0w8wsm6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5034268603545323\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4.9466727308945705e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9186748322093916\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 3.368416441303122e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204248-b0w8wsm6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/b0w8wsm6' target=\"_blank\">wise-sweep-19</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/b0w8wsm6' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/b0w8wsm6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 130.94 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-sweep-19</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/b0w8wsm6' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/b0w8wsm6</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204248-b0w8wsm6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 130.94 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run b0w8wsm6 errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 130.94 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sb20yasz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3726280551164563\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0017470757676060953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9559390465435482\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00033137529901756263\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rami/Documents/ORT/taller_dl/tarea_1/wandb/run-20251004_204253-sb20yasz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/sb20yasz' target=\"_blank\">classic-sweep-20</a></strong> to <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/sweeps/ruprmlf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/sb20yasz' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/sb20yasz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 104.25 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">classic-sweep-20</strong> at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/sb20yasz' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D/runs/sb20yasz</a><br> View project at: <a href='https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D' target=\"_blank\">https://wandb.ai/joacoguerrae-ort/%5Bwandb-labo1-guerra-sanes%5D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251004_204253-sb20yasz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 297, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_35337/2715244269.py\", line 20, in train_with_wandb\n",
      "    ).to(DEVICE)\n",
      "      ^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/rami/.conda/envs/Taller_DL/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 104.25 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run sb20yasz errored: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 104.25 MiB is free. Process 35043 has 4.90 GiB memory in use. Including non-PyTorch memory, this process has 6.28 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 1.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "wandb.agent('ruprmlf1', function=train_with_wandb, count=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Taller_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
